diff -uNr Spark-GATK/bin/gatk-spark-submit GATK-Spark-Clean/bin/gatk-spark-submit
--- Spark-GATK/bin/gatk-spark-submit	2016-04-28 19:54:37.795513752 +0800
+++ GATK-Spark-Clean/bin/gatk-spark-submit	2016-05-06 09:27:53.897365912 +0800
@@ -29,7 +29,7 @@
 
 # Get list of required jars for ADAM
 LIB_DIR="${ROOT_DIR}/lib"
-CLASSPATH="$BASEDIR"/etc:"$GATK_SPARK_JAR":"$LIB_DIR"/adam-core_2.10-0.17.0.jar:"$LIB_DIR"/htsjdk-1.140.jar:"$LIB_DIR"/bdg-formats-0.4.0.jar:"$LIB_DIR"/ActiveRegionMapper.jar:"$LIB_DIR"/BaseCalculator.jar:"$LIB_DIR"/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:"$LIB_DIR"/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:"$LIB_DIR"/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:"$LIB_DIR"/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:"$LIB_DIR"/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:"$LIB_DIR"/org/tukaani/xz/1.0/xz-1.0.jar:"$LIB_DIR"/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:"$LIB_DIR"/log4j/log4j/1.2.17/log4j-1.2.17.jar:"$LIB_DIR"/org/xerial/snappy/snappy-java/1.1.1.6/snappy-java-1.1.1.6.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-io_2.10/0.2.1/utils-io_2.10-0.2.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-misc_2.10/0.2.1/utils-misc_2.10-0.2.1.jar:"$LIB_DIR"/com/twitter/parquet-avro/1.6.0rc4/parquet-avro-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-column/1.6.0rc4/parquet-column-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-common/1.6.0rc4/parquet-common-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-encoding/1.6.0rc4/parquet-encoding-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-generator/1.6.0rc4/parquet-generator-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-hadoop/1.6.0rc4/parquet-hadoop-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-jackson/1.6.0rc4/parquet-jackson-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-format/2.2.0-rc1/parquet-format-2.2.0-rc1.jar:"$LIB_DIR"/org/apache/httpcomponents/httpclient/4.3.2/httpclient-4.3.2.jar:"$LIB_DIR"/org/apache/httpcomponents/httpcore/4.3.1/httpcore-4.3.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-cli_2.10/0.2.1/utils-cli_2.10-0.2.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-metrics_2.10/0.2.1/utils-metrics_2.10-0.2.1.jar:"$LIB_DIR"/com/netflix/servo/servo-core/0.5.5/servo-core-0.5.5.jar:"$LIB_DIR"/com/google/code/findbugs/annotations/2.0.0/annotations-2.0.0.jar:"$LIB_DIR"/org/scoverage/scalac-scoverage-plugin_2.10/0.99.2/scalac-scoverage-plugin_2.10-0.99.2.jar:"$LIB_DIR"/commons-io/commons-io/1.3.2/commons-io-1.3.2.jar:"$LIB_DIR"/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:"$LIB_DIR"/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:"$LIB_DIR"/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:"$LIB_DIR"/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:"$LIB_DIR"/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:"$LIB_DIR"/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:"$LIB_DIR"/org/ow2/asm/asm/4.0/asm-4.0.jar:"$LIB_DIR"/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:"$LIB_DIR"/org/objenesis/objenesis/1.2/objenesis-1.2.jar:"$LIB_DIR"/it/unimi/dsi/fastutil/6.4.4/fastutil-6.4.4.jar:"$LIB_DIR"/com/twitter/parquet-scala_2.10/1.6.0rc4/parquet-scala_2.10-1.6.0rc4.jar:"$LIB_DIR"/org/seqdoop/hadoop-bam/7.0.0/hadoop-bam-7.0.0.jar:"$LIB_DIR"/org/apache/commons/commons-jexl/2.1.1/commons-jexl-2.1.1.jar:"$LIB_DIR"/com/google/guava/guava/14.0.1/guava-14.0.1.jar:"$LIB_DIR"/org/bdgenomics/adam/adam-apis_2.10/0.17.0/adam-apis_2.10-0.17.0.jar:"$LIB_DIR"/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar:"$LIB_DIR"/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:"$LIB_DIR"/args4j/args4j/2.0.23/args4j-2.0.23.jar:"$LIB_DIR"/org/apache/commons/collections/commons-collections-3.2.1.jar:"$LIB_DIR"/org/apache/commons/lang/commons-lang-2.5.jar:"$LIB_DIR"/net/sf/json/json-lib-2.4-jdk15.jar:"$LIB_DIR"/org/jgrapht/jgrapht-0.8.3.jar:"$LIB_DIR"/org/apache/commons/math/commons-math-2.2.jar:"$LIB_DIR"/org/reflections/reflections-0.9.9-RC1.jar:"$LIB_DIR"/javassist/javassist-3.12.1.GA.jar:"$LIB_DIR"/javassist/colt-1.2.0.jar
+CLASSPATH="$BASEDIR"/etc:"$GATK_SPARK_JAR":"$LIB_DIR"/htsjdk-1.140.jar:"$LIB_DIR"/bdg-formats-0.4.0.jar:"$LIB_DIR"/ActiveRegionMapper.jar:"$LIB_DIR"/BaseCalculator.jar:"$LIB_DIR"/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:"$LIB_DIR"/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:"$LIB_DIR"/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:"$LIB_DIR"/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:"$LIB_DIR"/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:"$LIB_DIR"/org/tukaani/xz/1.0/xz-1.0.jar:"$LIB_DIR"/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:"$LIB_DIR"/log4j/log4j/1.2.17/log4j-1.2.17.jar:"$LIB_DIR"/org/xerial/snappy/snappy-java/1.1.1.6/snappy-java-1.1.1.6.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-io_2.10/0.2.1/utils-io_2.10-0.2.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-misc_2.10/0.2.1/utils-misc_2.10-0.2.1.jar:"$LIB_DIR"/com/twitter/parquet-avro/1.6.0rc4/parquet-avro-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-column/1.6.0rc4/parquet-column-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-common/1.6.0rc4/parquet-common-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-encoding/1.6.0rc4/parquet-encoding-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-generator/1.6.0rc4/parquet-generator-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-hadoop/1.6.0rc4/parquet-hadoop-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-jackson/1.6.0rc4/parquet-jackson-1.6.0rc4.jar:"$LIB_DIR"/com/twitter/parquet-format/2.2.0-rc1/parquet-format-2.2.0-rc1.jar:"$LIB_DIR"/org/apache/httpcomponents/httpclient/4.3.2/httpclient-4.3.2.jar:"$LIB_DIR"/org/apache/httpcomponents/httpcore/4.3.1/httpcore-4.3.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-cli_2.10/0.2.1/utils-cli_2.10-0.2.1.jar:"$LIB_DIR"/org/bdgenomics/utils/utils-metrics_2.10/0.2.1/utils-metrics_2.10-0.2.1.jar:"$LIB_DIR"/com/netflix/servo/servo-core/0.5.5/servo-core-0.5.5.jar:"$LIB_DIR"/com/google/code/findbugs/annotations/2.0.0/annotations-2.0.0.jar:"$LIB_DIR"/org/scoverage/scalac-scoverage-plugin_2.10/0.99.2/scalac-scoverage-plugin_2.10-0.99.2.jar:"$LIB_DIR"/commons-io/commons-io/1.3.2/commons-io-1.3.2.jar:"$LIB_DIR"/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:"$LIB_DIR"/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:"$LIB_DIR"/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:"$LIB_DIR"/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:"$LIB_DIR"/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:"$LIB_DIR"/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:"$LIB_DIR"/org/ow2/asm/asm/4.0/asm-4.0.jar:"$LIB_DIR"/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:"$LIB_DIR"/org/objenesis/objenesis/1.2/objenesis-1.2.jar:"$LIB_DIR"/it/unimi/dsi/fastutil/6.4.4/fastutil-6.4.4.jar:"$LIB_DIR"/com/twitter/parquet-scala_2.10/1.6.0rc4/parquet-scala_2.10-1.6.0rc4.jar:"$LIB_DIR"/org/seqdoop/hadoop-bam/7.0.0/hadoop-bam-7.0.0.jar:"$LIB_DIR"/org/apache/commons/commons-jexl/2.1.1/commons-jexl-2.1.1.jar:"$LIB_DIR"/com/google/guava/guava/14.0.1/guava-14.0.1.jar:"$LIB_DIR"/org/bdgenomics/adam/adam-apis_2.10/0.17.0/adam-apis_2.10-0.17.0.jar:"$LIB_DIR"/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar:"$LIB_DIR"/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:"$LIB_DIR"/args4j/args4j/2.0.23/args4j-2.0.23.jar:"$LIB_DIR"/org/apache/commons/collections/commons-collections-3.2.1.jar:"$LIB_DIR"/org/apache/commons/lang/commons-lang-2.5.jar:"$LIB_DIR"/net/sf/json/json-lib-2.4-jdk15.jar:"$LIB_DIR"/org/jgrapht/jgrapht-0.8.3.jar:"$LIB_DIR"/org/apache/commons/math/commons-math-2.2.jar:"$LIB_DIR"/org/reflections/reflections-0.9.9-RC1.jar:"$LIB_DIR"/javassist/javassist-3.12.1.GA.jar:"$LIB_DIR"/javassist/colt-1.2.0.jar
 EXTERNAL_JARS=$(echo "$CLASSPATH" | tr ":" "," | cut -d "," -f 2-)
 
 # append EXTERNAL_JARS to the --jars option, if any
Binary files Spark-GATK/lib/adam-core_2.10-0.17.0.jar and GATK-Spark-Clean/lib/adam-core_2.10-0.17.0.jar differ
diff -uNr Spark-GATK/pom.xml GATK-Spark-Clean/pom.xml
--- Spark-GATK/pom.xml	2016-04-28 19:54:38.023513752 +0800
+++ GATK-Spark-Clean/pom.xml	2016-05-06 09:27:53.940365912 +0800
@@ -75,14 +75,6 @@
     <dependencies>
         <!-- local dependency -->
         <dependency>
-            <groupId>org.bdgenomics.adam</groupId>
-            <artifactId>adam-core_2.10</artifactId>
-            <version>1.0</version>
-            <scope>system</scope>
-            <systemPath>${project.basedir}/lib/adam-core_2.10-0.17.0.jar
-            </systemPath>
-        </dependency>
-        <dependency>
             <groupId>com.github</groupId>
             <artifactId>htsjdk</artifactId>
             <version>1.1.40</version>
diff -uNr Spark-GATK/src/main/java/org/bdgenomics/adam/io/InterleavedFastqInputFormat.java GATK-Spark-Clean/src/main/java/org/bdgenomics/adam/io/InterleavedFastqInputFormat.java
--- Spark-GATK/src/main/java/org/bdgenomics/adam/io/InterleavedFastqInputFormat.java	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/java/org/bdgenomics/adam/io/InterleavedFastqInputFormat.java	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,320 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.bdgenomics.adam.io;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.util.LineReader;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+
+/**
+ * This class is a Hadoop reader for "interleaved fastq" -- that is,
+ * fastq with paired reads in the same file, interleaved, rather than
+ * in two separate files. This makes it much easier to Hadoopily slice
+ * up a single file and feed the slices into an aligner.
+ * The format is the same as fastq, but records are expected to alternate
+ * between /1 and /2. As a precondition, we assume that the interleaved
+ * FASTQ files are always uncompressed; if the files are compressed, they
+ * cannot be split, and thus there is no reason to use the interleaved
+ * format.
+ *
+ * This reader is based on the FastqInputFormat that's part of Hadoop-BAM,
+ * found at https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/FastqInputFormat.java
+ *
+ * @author Jeremy Elson (jelson@microsoft.com)
+ * @date Feb 2014
+ */
+public class InterleavedFastqInputFormat extends FileInputFormat<Void,Text> {
+    
+    public static class InterleavedFastqRecordReader extends RecordReader<Void,Text> {
+        /*
+         * fastq format:
+         * <fastq>  :=  <block>+
+         * <block>  :=  @<seqname>\n<seq>\n\+[<seqname>]\n<qual>\n
+         * <seqname>  :=  [A-Za-z0-9_.:-]+
+         * <seq>  :=  [A-Za-z\n\.~]+
+         * <qual> :=  [!-~\n]+
+         *
+         * LP: this format is broken, no?  You can have multi-line sequence and quality strings,
+         * and the quality encoding includes '@' in its valid character range.  So how should one
+         * distinguish between \n@ as a record delimiter and and \n@ as part of a multi-line
+         * quality string?
+         *
+         * For now I'm going to assume single-line sequences.  This works for our sequencing
+         * application.  We'll see if someone complains in other applications.
+         */
+        
+        // start:  first valid data index
+        private long start;
+        // end:  first index value beyond the slice, i.e. slice is in range [start,end)
+        private long end;
+        // pos: current position in file
+        private long pos;
+        // file:  the file being read
+        private Path file;
+        
+        private LineReader lineReader;
+        private InputStream inputStream;
+        private Text currentValue;
+        private byte[] newline = "\n".getBytes();
+
+        // How long can a read get?
+        private static final int MAX_LINE_LENGTH = 10000;
+
+        public InterleavedFastqRecordReader(Configuration conf, FileSplit split) throws IOException {
+            file = split.getPath();
+            start = split.getStart();
+            end = start + split.getLength();
+
+            FileSystem fs = file.getFileSystem(conf);
+            FSDataInputStream fileIn = fs.open(file);
+
+            CompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);
+            CompressionCodec        codec        = codecFactory.getCodec(file);
+
+            if (codec == null) { // no codec.  Uncompressed file.
+                positionAtFirstRecord(fileIn);
+                inputStream = fileIn;
+            } else { 
+                // compressed file
+                if (start != 0) {
+                    throw new RuntimeException("Start position for compressed file is not 0! (found " + start + ")");
+                }
+
+                inputStream = codec.createInputStream(fileIn);
+                end = Long.MAX_VALUE; // read until the end of the file
+            }
+
+            lineReader = new LineReader(inputStream);
+        }
+
+        /**
+         * Position the input stream at the start of the first record.
+         */
+        private void positionAtFirstRecord(FSDataInputStream stream) throws IOException {
+            Text buffer = new Text();
+
+            if (true) { // (start > 0) // use start>0 to assume that files start with valid data
+                // Advance to the start of the first record that ends with /1
+                // We use a temporary LineReader to read lines until we find the
+                // position of the right one.  We then seek the file to that position.
+                stream.seek(start);
+                LineReader reader = new LineReader(stream);
+
+                int bytesRead = 0;
+                do {
+                    bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                    int bufferLength = buffer.getLength();
+                    if (bytesRead > 0 && (bufferLength <= 0 ||
+                                          buffer.getBytes()[0] != '@' ||
+                                          (bufferLength >= 2 && buffer.getBytes()[bufferLength - 2] != '/') ||
+                                          (bufferLength >= 1 && buffer.getBytes()[bufferLength - 1] != '1'))) {
+                        start += bytesRead;
+                    } else {
+                        // line starts with @.  Read two more and verify that it starts with a +
+                        //
+                        // If this isn't the start of a record, we want to backtrack to its end
+                        long backtrackPosition = start + bytesRead;
+
+                        bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                        bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                        if (bytesRead > 0 && buffer.getLength() > 0 && buffer.getBytes()[0] == '+') {
+                            break; // all good!
+                        } else {
+                            // backtrack to the end of the record we thought was the start.
+                            start = backtrackPosition;
+                            stream.seek(start);
+                            reader = new LineReader(stream);
+                        }
+                    }
+                } while (bytesRead > 0);
+
+                stream.seek(start);
+            }
+
+            pos = start;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {}
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public Void getCurrentKey() {
+            return null;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public Text getCurrentValue() {
+            return currentValue;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public boolean nextKeyValue() throws IOException, InterruptedException {
+            currentValue = new Text();
+
+            return next(currentValue);
+        }
+
+        /**
+         * Close this RecordReader to future operations.
+         */
+        public void close() throws IOException {
+            inputStream.close();
+        }
+
+        /**
+         * Create an object of the appropriate type to be used as a key.
+         */
+        public Text createKey() {
+            return new Text();
+        }
+
+        /**
+         * Create an object of the appropriate type to be used as a value.
+         */
+        public Text createValue() {
+            return new Text();
+        }
+
+        /**
+         * Returns the current position in the input.
+         */
+        public long getPos() { 
+            return pos; 
+        }
+
+        /**
+         * How much of the input has the RecordReader consumed i.e.
+         */
+        public float getProgress() {
+            if (start == end)
+                return 1.0f;
+            else
+                return Math.min(1.0f, (pos - start) / (float)(end - start));
+        }
+
+        public String makePositionMessage() {
+            return file.toString() + ":" + pos;
+        }
+
+        protected boolean lowLevelFastqRead(Text readName, Text value) throws IOException {
+            // ID line
+            readName.clear();
+            long skipped = appendLineInto(readName, true);
+            pos += skipped;
+            if (skipped == 0)
+                return false; // EOF
+            if (readName.getBytes()[0] != '@')
+                throw new RuntimeException("unexpected fastq record didn't start with '@' at " + makePositionMessage() + ". Line: " + readName + ". \n");
+
+            value.append(readName.getBytes(), 0, readName.getLength());
+
+            // sequence
+            appendLineInto(value, false);
+
+            // separator line
+            appendLineInto(value, false);
+
+            // quality
+            appendLineInto(value, false);
+
+            return true;
+        }
+
+
+        /**
+         * Reads the next key/value pair from the input for processing.
+         */
+        public boolean next(Text value) throws IOException {
+            if (pos >= end)
+                return false; // past end of slice
+            try {
+                Text readName1 = new Text();
+                Text readName2 = new Text();
+
+                value.clear();
+
+                // first read of the pair
+                boolean gotData = lowLevelFastqRead(readName1, value);
+
+                if (!gotData)
+                    return false;
+
+
+
+                // second read of the pair
+                gotData = lowLevelFastqRead(readName2, value);
+
+                if (!gotData)
+                    return false;
+
+                return true;
+            } catch (EOFException e) {
+                throw new RuntimeException("unexpected end of file in fastq record at " + makePositionMessage());
+            }
+        }
+
+
+        private int appendLineInto(Text dest, boolean eofOk) throws EOFException, IOException {
+            Text buf = new Text();
+            int bytesRead = lineReader.readLine(buf, MAX_LINE_LENGTH);
+
+            if (bytesRead < 0 || (bytesRead == 0 && !eofOk))
+                throw new EOFException();
+
+            dest.append(buf.getBytes(), 0, buf.getLength());
+            dest.append(newline, 0, 1);
+            pos += bytesRead;
+
+            return bytesRead;
+        }
+    }
+
+    public RecordReader<Void, Text> createRecordReader(
+            InputSplit genericSplit,
+            TaskAttemptContext context) throws IOException, InterruptedException {
+        context.setStatus(genericSplit.toString());
+
+        // cast as per example in TextInputFormat
+        return new InterleavedFastqRecordReader(context.getConfiguration(), 
+                                                (FileSplit)genericSplit); 
+    }
+}
diff -uNr Spark-GATK/src/main/java/org/bdgenomics/adam/io/SingleFastqInputFormat.java GATK-Spark-Clean/src/main/java/org/bdgenomics/adam/io/SingleFastqInputFormat.java
--- Spark-GATK/src/main/java/org/bdgenomics/adam/io/SingleFastqInputFormat.java	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/java/org/bdgenomics/adam/io/SingleFastqInputFormat.java	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,297 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.bdgenomics.adam.io;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.CompressionCodecFactory;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.hadoop.util.LineReader;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+
+/**
+ * This class is a Hadoop reader for single read fastq.
+ *
+ * This reader is based on the FastqInputFormat that's part of Hadoop-BAM,
+ * found at https://github.com/HadoopGenomics/Hadoop-BAM/blob/master/src/main/java/org/seqdoop/hadoop_bam/FastqInputFormat.java
+ *
+ * @author Frank Austin Nothaft (fnothaft@berkeley.edu)
+ * @date September 2014
+ */
+public class SingleFastqInputFormat extends FileInputFormat<Void,Text> {
+    
+    public static class SingleFastqRecordReader extends RecordReader<Void,Text> {
+        /*
+         * fastq format:
+         * <fastq>  :=  <block>+
+         * <block>  :=  @<seqname>\n<seq>\n\+[<seqname>]\n<qual>\n
+         * <seqname>  :=  [A-Za-z0-9_.:-]+
+         * <seq>  :=  [A-Za-z\n\.~]+
+         * <qual> :=  [!-~\n]+
+         *
+         * LP: this format is broken, no?  You can have multi-line sequence and quality strings,
+         * and the quality encoding includes '@' in its valid character range.  So how should one
+         * distinguish between \n@ as a record delimiter and and \n@ as part of a multi-line
+         * quality string?
+         *
+         * For now I'm going to assume single-line sequences.  This works for our sequencing
+         * application.  We'll see if someone complains in other applications.
+         */
+        
+        // start:  first valid data index
+        private long start;
+        // end:  first index value beyond the slice, i.e. slice is in range [start,end)
+        private long end;
+        // pos: current position in file
+        private long pos;
+        // file:  the file being read
+        private Path file;
+        
+        private LineReader lineReader;
+        private InputStream inputStream;
+        private Text currentValue;
+        private byte[] newline = "\n".getBytes();
+
+        // How long can a read get?
+        private static final int MAX_LINE_LENGTH = 10000;
+
+        public SingleFastqRecordReader(Configuration conf, FileSplit split) throws IOException {
+            file = split.getPath();
+            start = split.getStart();
+            end = start + split.getLength();
+
+            FileSystem fs = file.getFileSystem(conf);
+            FSDataInputStream fileIn = fs.open(file);
+
+            CompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);
+            CompressionCodec        codec        = codecFactory.getCodec(file);
+
+            if (codec == null) { // no codec.  Uncompressed file.
+                positionAtFirstRecord(fileIn);
+                inputStream = fileIn;
+            } else { 
+                // compressed file
+                if (start != 0) {
+                    throw new RuntimeException("Start position for compressed file is not 0! (found " + start + ")");
+                }
+
+                inputStream = codec.createInputStream(fileIn);
+                end = Long.MAX_VALUE; // read until the end of the file
+            }
+
+            lineReader = new LineReader(inputStream);
+        }
+
+        /**
+         * Position the input stream at the start of the first record.
+         */
+        private void positionAtFirstRecord(FSDataInputStream stream) throws IOException {
+            Text buffer = new Text();
+
+            if (true) { // (start > 0) // use start>0 to assume that files start with valid data
+                // Advance to the start of the first record
+                // We use a temporary LineReader to read lines until we find the
+                // position of the right one.  We then seek the file to that position.
+                stream.seek(start);
+                LineReader reader = new LineReader(stream);
+
+                int bytesRead = 0;
+                do {
+                    bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                    int bufferLength = buffer.getLength();
+                    if (bytesRead > 0 && (bufferLength <= 0 ||
+                                          buffer.getBytes()[0] != '@')) {
+                        start += bytesRead;
+                    } else {
+                        // line starts with @.  Read two more and verify that it starts with a +
+                        //
+                        // If this isn't the start of a record, we want to backtrack to its end
+                        long backtrackPosition = start + bytesRead;
+
+                        bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                        bytesRead = reader.readLine(buffer, (int)Math.min(MAX_LINE_LENGTH, end - start));
+                        if (bytesRead > 0 && buffer.getLength() > 0 && buffer.getBytes()[0] == '+') {
+                            break; // all good!
+                        } else {
+                            // backtrack to the end of the record we thought was the start.
+                            start = backtrackPosition;
+                            stream.seek(start);
+                            reader = new LineReader(stream);
+                        }
+                    }
+                } while (bytesRead > 0);
+
+                stream.seek(start);
+            }
+
+            pos = start;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {}
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public Void getCurrentKey() {
+            return null;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public Text getCurrentValue() {
+            return currentValue;
+        }
+
+        /**
+         * Added to use mapreduce API.
+         */
+        public boolean nextKeyValue() throws IOException, InterruptedException {
+            currentValue = new Text();
+
+            return next(currentValue);
+        }
+
+        /**
+         * Close this RecordReader to future operations.
+         */
+        public void close() throws IOException {
+            inputStream.close();
+        }
+
+        /**
+         * Create an object of the appropriate type to be used as a key.
+         */
+        public Text createKey() {
+            return new Text();
+        }
+
+        /**
+         * Create an object of the appropriate type to be used as a value.
+         */
+        public Text createValue() {
+            return new Text();
+        }
+
+        /**
+         * Returns the current position in the input.
+         */
+        public long getPos() { 
+            return pos; 
+        }
+
+        /**
+         * How much of the input has the RecordReader consumed i.e.
+         */
+        public float getProgress() {
+            if (start == end)
+                return 1.0f;
+            else
+                return Math.min(1.0f, (pos - start) / (float)(end - start));
+        }
+
+        public String makePositionMessage() {
+            return file.toString() + ":" + pos;
+        }
+
+        protected boolean lowLevelFastqRead(Text readName, Text value) throws IOException {
+            // ID line
+            readName.clear();
+            long skipped = appendLineInto(readName, true);
+            if (skipped == 0)
+                return false; // EOF
+            if (readName.getBytes()[0] != '@')
+                throw new RuntimeException("unexpected fastq record didn't start with '@' at " + makePositionMessage() + ". Line: " + readName + ". \n");
+
+            value.append(readName.getBytes(), 0, readName.getLength());
+
+            // sequence
+            appendLineInto(value, false);
+
+            // separator line
+            appendLineInto(value, false);
+
+            // quality
+            appendLineInto(value, false);
+
+            return true;
+        }
+
+
+        /**
+         * Reads the next key/value pair from the input for processing.
+         */
+        public boolean next(Text value) throws IOException {
+            if (pos >= end)
+                return false; // past end of slice
+            try {
+                Text readName = new Text();
+
+                value.clear();
+
+                // first read of the pair
+                boolean gotData = lowLevelFastqRead(readName, value);
+
+                return gotData;
+            } catch (EOFException e) {
+                throw new RuntimeException("unexpected end of file in fastq record at " + makePositionMessage());
+            }
+        }
+
+
+        private int appendLineInto(Text dest, boolean eofOk) throws EOFException, IOException {
+            Text buf = new Text();
+            int bytesRead = lineReader.readLine(buf, MAX_LINE_LENGTH);
+
+            if (bytesRead < 0 || (bytesRead == 0 && !eofOk))
+                throw new EOFException();
+
+            dest.append(buf.getBytes(), 0, buf.getLength());
+            dest.append(newline, 0, 1);
+            pos += bytesRead;
+
+            return bytesRead;
+        }
+    }
+
+    public RecordReader<Void, Text> createRecordReader(
+            InputSplit genericSplit,
+            TaskAttemptContext context) throws IOException, InterruptedException {
+        context.setStatus(genericSplit.toString());
+
+        // cast as per example in TextInputFormat
+        return new SingleFastqRecordReader(context.getConfiguration(), 
+                                           (FileSplit)genericSplit); 
+    }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromKnowns.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromKnowns.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromKnowns.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromKnowns.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,78 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.consensus
+
+import org.apache.spark.SparkContext
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rdd.read.realignment.IndelRealignmentTarget
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.Variant
+
+class ConsensusGeneratorFromKnowns(file: String, @transient sc: SparkContext) extends ConsensusGenerator {
+
+  val indelTable = sc.broadcast(IndelTable(file, sc))
+
+  /**
+   * Generates targets to add to initial set of indel realignment targets, if additional
+   * targets are necessary.
+   *
+   * @return Returns an option which wraps an RDD of indel realignment targets.
+   */
+  def targetsToAdd(): Option[RDD[IndelRealignmentTarget]] = {
+    val rdd: RDD[Variant] = sc.loadVariants(file)
+
+    Some(rdd.filter(v => v.getReferenceAllele.length != v.getAlternateAllele.length)
+      .map(v => ReferenceRegion(v.getContig.getContigName, v.getStart, v.getStart + v.getReferenceAllele.length))
+      .map(r => new IndelRealignmentTarget(Some(r), r)))
+  }
+
+  /**
+   * Performs any preprocessing specific to this consensus generation algorithm, e.g.,
+   * indel normalization.
+   *
+   * @param reads Reads to preprocess.
+   * @return Preprocessed reads.
+   */
+  def preprocessReadsForRealignment(reads: Iterable[RichAlignmentRecord],
+                                    reference: String,
+                                    region: ReferenceRegion): Iterable[RichAlignmentRecord] = {
+    reads
+  }
+
+  /**
+   * For all reads in this region, generates the list of consensus sequences for realignment.
+   *
+   * @param reads Reads to generate consensus sequences from.
+   * @return Consensus sequences to use for realignment.
+   */
+  def findConsensus(reads: Iterable[RichAlignmentRecord]): Iterable[Consensus] = {
+    val table = indelTable.value
+
+    // get region
+    val start = reads.map(_.record.getStart.toLong).reduce(_ min _)
+    val end = reads.map(_.getEnd.toLong).reduce(_ max _)
+    val refId = reads.head.record.getContig.getContigName
+
+    val region = ReferenceRegion(refId, start, end + 1)
+
+    // get reads
+    table.getIndelsInRegion(region)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromReads.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromReads.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromReads.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromReads.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,86 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.consensus
+
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.{ Consensus, ReferenceRegion, ReferencePosition }
+import org.bdgenomics.adam.rdd.read.realignment.IndelRealignmentTarget
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.rich.RichCigar._
+import org.bdgenomics.adam.util.MdTag
+import org.bdgenomics.adam.util.ImplicitJavaConversions._
+import org.bdgenomics.adam.util.NormalizationUtils._
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+class ConsensusGeneratorFromReads extends ConsensusGenerator {
+
+  /**
+   * No targets to add if generating consensus targets from reads.
+   *
+   * @return Returns a None.
+   */
+  def targetsToAdd(): Option[RDD[IndelRealignmentTarget]] = None
+
+  /**
+   * Performs read preprocessing by normalizing indels for all reads that have evidence of one
+   * indel.
+   *
+   * @param reads Reads to process.
+   * @return Reads with indels normalized if they contain a single indel.
+   */
+  def preprocessReadsForRealignment(reads: Iterable[RichAlignmentRecord],
+                                    reference: String,
+                                    region: ReferenceRegion): Iterable[RichAlignmentRecord] = {
+    reads.map(r => {
+      // if there are two alignment blocks (sequence matches) then there is a single indel in the read
+      if (r.samtoolsCigar.numAlignmentBlocks == 2) {
+        // left align this indel and update the mdtag
+        val cigar = leftAlignIndel(r)
+        val mdTag = MdTag.moveAlignment(r, cigar)
+
+        val newRead: RichAlignmentRecord = AlignmentRecord.newBuilder(r)
+          .setCigar(cigar.toString)
+          .setMismatchingPositions(mdTag.toString())
+          .build()
+
+        newRead
+      } else {
+        r
+      }
+    })
+  }
+
+  /**
+   * Generates concensus sequences from reads with indels.
+   */
+  def findConsensus(reads: Iterable[RichAlignmentRecord]): Iterable[Consensus] = {
+    reads.filter(r => r.mdTag.isDefined)
+      .flatMap(r => {
+        // try to generate a consensus alignment - if a consensus exists, add it to our
+        // list of consensuses to test
+        Consensus.generateAlternateConsensus(r.getSequence,
+          ReferencePosition(r.getContig.getContigName,
+            r.getStart),
+          r.samtoolsCigar)
+      })
+      .toSeq
+      .distinct
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromSmithWaterman.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromSmithWaterman.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromSmithWaterman.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGeneratorFromSmithWaterman.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,74 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.consensus
+
+import org.bdgenomics.adam.algorithms.smithwaterman.SmithWatermanConstantGapScoring
+import org.bdgenomics.adam.models.ReferenceRegion
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.rich.RichCigar._
+import org.bdgenomics.adam.util.MdTag
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+class ConsensusGeneratorFromSmithWaterman(wMatch: Double,
+                                          wMismatch: Double,
+                                          wInsert: Double,
+                                          wDelete: Double) extends ConsensusGeneratorFromReads {
+
+  /**
+   * Attempts realignment of all reads using Smith-Waterman. Accepts all realignments that have one
+   * or fewer indels.
+   *
+   * @param reads Reads to process.
+   * @return Reads with indels normalized if they contain a single indel.
+   */
+  override def preprocessReadsForRealignment(reads: Iterable[RichAlignmentRecord],
+                                             reference: String,
+                                             region: ReferenceRegion): Iterable[RichAlignmentRecord] = {
+    val rds: Iterable[RichAlignmentRecord] = reads.map(r => {
+
+      val sw = new SmithWatermanConstantGapScoring(r.record.getSequence.toString,
+        reference,
+        wMatch,
+        wMismatch,
+        wInsert,
+        wDelete)
+      println("for " + r.record.getReadName + " sw to " + sw.xStart + " with " + sw.cigarX)
+
+      // if we realign with fewer than three alignment blocks, then take the new alignment
+      if (sw.cigarX.numAlignmentBlocks <= 2) {
+        val mdTag = MdTag(r.record.getSequence.toString,
+          reference.drop(sw.xStart),
+          sw.cigarX,
+          region.start)
+
+        val newRead: RichAlignmentRecord = AlignmentRecord.newBuilder(r)
+          .setStart(sw.xStart + region.start)
+          .setCigar(sw.cigarX.toString)
+          .setMismatchingPositions(mdTag.toString())
+          .build()
+
+        newRead
+      } else {
+        r
+      }
+    })
+
+    super.preprocessReadsForRealignment(rds, reference, region)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGenerator.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGenerator.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGenerator.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/consensus/ConsensusGenerator.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,53 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.consensus
+
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.{ Consensus, ReferenceRegion }
+import org.bdgenomics.adam.rdd.read.realignment.IndelRealignmentTarget
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+
+abstract class ConsensusGenerator extends Serializable {
+
+  /**
+   * Generates targets to add to initial set of indel realignment targets, if additional
+   * targets are necessary.
+   *
+   * @return Returns an option which wraps an RDD of indel realignment targets.
+   */
+  def targetsToAdd(): Option[RDD[IndelRealignmentTarget]]
+
+  /**
+   * Performs any preprocessing specific to this consensus generation algorithm, e.g.,
+   * indel normalization.
+   *
+   * @param reads Reads to preprocess.
+   * @return Preprocessed reads.
+   */
+  def preprocessReadsForRealignment(reads: Iterable[RichAlignmentRecord],
+                                    reference: String,
+                                    region: ReferenceRegion): Iterable[RichAlignmentRecord]
+
+  /**
+   * For all reads in this region, generates the list of consensus sequences for realignment.
+   *
+   * @param reads Reads to generate consensus sequences from.
+   * @return Consensus sequences to use for realignment.
+   */
+  def findConsensus(reads: Iterable[RichAlignmentRecord]): Iterable[Consensus]
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanConstantGapScoring.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanConstantGapScoring.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanConstantGapScoring.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanConstantGapScoring.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,43 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.smithwaterman
+
+object SmithWatermanConstantGapScoring {
+
+  protected def constantGapFn(wMatch: Double, wDelete: Double, wInsert: Double, wMismatch: Double)(x: Int, y: Int, i: Char, j: Char): Double = {
+    if (i == j) {
+      wMatch
+    } else if (i == '_') {
+      wDelete
+    } else if (j == '_') {
+      wInsert
+    } else {
+      wMismatch
+    }
+  }
+
+}
+
+class SmithWatermanConstantGapScoring(xSequence: String,
+                                      ySequence: String,
+                                      wMatch: Double,
+                                      wMismatch: Double,
+                                      wInsert: Double,
+                                      wDelete: Double)
+    extends SmithWatermanGapScoringFromFn(xSequence, ySequence, SmithWatermanConstantGapScoring.constantGapFn(wMatch, wInsert, wDelete, wMismatch)) {
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanGapScoringFromFn.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanGapScoringFromFn.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanGapScoringFromFn.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWatermanGapScoringFromFn.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,73 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.smithwaterman
+
+abstract class SmithWatermanGapScoringFromFn(xSequence: String,
+                                             ySequence: String,
+                                             scoreFn: (Int, Int, Char, Char) => Double)
+    extends SmithWaterman(xSequence, ySequence) {
+
+  def buildScoringMatrix(): (Array[Array[Double]], Array[Array[Char]]) = {
+
+    val y = ySequence.length
+    val x = xSequence.length
+
+    val scoreMatrix = new Array[Array[Double]](x + 1)
+    val moveMatrix = new Array[Array[Char]](x + 1)
+    for (i <- 0 to x) {
+      scoreMatrix(i) = new Array[Double](y + 1)
+      moveMatrix(i) = new Array[Char](y + 1)
+    }
+
+    // set row/col 0 to 0
+    for (i <- 0 to x) {
+      scoreMatrix(i)(0) = 0.0
+      moveMatrix(i)(0) = 'T'
+    }
+    for (j <- 0 to y) {
+      scoreMatrix(0)(j) = 0.0
+      moveMatrix(0)(j) = 'T'
+    }
+
+    // score matrix
+    for (i <- 1 to x) {
+      for (j <- 1 to y) {
+        val m = scoreMatrix(i - 1)(j - 1) + scoreFn(i, j, xSequence(i - 1), ySequence(j - 1))
+        val d = scoreMatrix(i - 1)(j) + scoreFn(i, j, xSequence(i - 1), '_')
+        val in = scoreMatrix(i)(j - 1) + scoreFn(i, j, '_', ySequence(j - 1))
+
+        val (scoreUpdate, moveUpdate) = if (m >= d && m >= in && m > 0.0) {
+          (m, 'B')
+        } else if (d >= in && d > 0.0) {
+          (d, 'J')
+        } else if (in > 0.0) {
+          (in, 'I')
+        } else {
+          (0.0, 'T')
+        }
+
+        scoreMatrix(i)(j) = scoreUpdate
+        moveMatrix(i)(j) = moveUpdate
+      }
+    }
+
+    (scoreMatrix, moveMatrix)
+  }
+
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWaterman.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWaterman.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWaterman.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/algorithms/smithwaterman/SmithWaterman.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,180 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.algorithms.smithwaterman
+
+import htsjdk.samtools.{ Cigar, TextCigarCodec }
+import scala.annotation.tailrec
+
+abstract class SmithWaterman(xSequence: String, ySequence: String) extends Serializable {
+
+  lazy val (scoringMatrix, moveMatrix) = buildScoringMatrix()
+  lazy val (cigarX, cigarY, xStart, yStart) = trackback(scoringMatrix, moveMatrix)
+
+  /**
+   * Builds Smith-Waterman scoring matrix.
+   *
+   * @return 2D array of doubles, along with move direction at each point.
+   *
+   * @note To work with the move function, expected move directions are:
+   *
+   * * I: move in I coordinate
+   * * J: move in J coordinate
+   * * B: move in both I and J
+   * * T: terminate move
+   *
+   * @see move
+   */
+  private[smithwaterman] def buildScoringMatrix(): (Array[Array[Double]], Array[Array[Char]])
+
+  /**
+   * Finds coordinates of a matrix with highest value.
+   *
+   * @param matrix Matrix to score.
+   * @return Tuple of (i, j) coordinates.
+   */
+  private[smithwaterman] final def maxCoordinates(matrix: Array[Array[Double]]): (Int, Int) = {
+    def maxInCol(col: Array[Double]): (Double, Int) = {
+      def takeMax(a: (Double, Int), b: (Double, Int)): (Double, Int) = {
+        if (a._1 > b._1) {
+          a
+        } else {
+          b
+        }
+      }
+
+      val c: Array[(Double, Int)] = col.zipWithIndex
+
+      c.reduce(takeMax)
+    }
+
+    def maxCol(cols: Array[(Double, Int)]): (Int, Int) = {
+      def takeMax(a: (Double, Int, Int), b: (Double, Int, Int)): (Double, Int, Int) = {
+        if (a._1 > b._1) {
+          a
+        } else {
+          b
+        }
+      }
+
+      val c: Array[((Double, Int), Int)] = cols.zipWithIndex
+
+      val m: (Double, Int, Int) = c.map(kv => (kv._1._1, kv._1._2, kv._2))
+        .reduce(takeMax)
+
+      (m._2, m._3)
+    }
+
+    maxCol(matrix.map(maxInCol))
+  }
+
+  /**
+   * Converts a reversed non-numeric CIGAR into a normal CIGAR.
+   *
+   * @note A reversed non-numeric CIGAR is a CIGAR where each alignment block
+   * has length = 1, and the alignment block ordering goes from end-to-beginning. E.g.,
+   * the equivalent of the CIGAR 4M2D1M would be MDDMMMM.
+   *
+   * @param nnc Reversed non-numeric CIGAR.
+   * @return A normal CIGAR.
+   */
+  private[smithwaterman] def cigarFromRNNCigar(nnc: String): String = {
+
+    @tailrec def buildCigar(last: Char, runCount: Int, nnc: String, cigar: String): String = {
+      if (nnc.length == 0) {
+        (runCount.toString + last) + cigar
+      } else {
+        val (next, nrc, nc) = if (nnc.head == last) {
+          (last, runCount + 1, cigar)
+        } else {
+          (nnc.head, 1, (runCount.toString + last) + cigar)
+        }
+
+        buildCigar(next, nrc, nnc.drop(1), nc)
+      }
+    }
+
+    buildCigar(nnc.head, 1, nnc.drop(1), "")
+  }
+
+  /**
+   * Recursive function to do backtrack.
+   *
+   * @param matrix Matrix to track back upon.
+   * @param i Current position in x sequence.
+   * @param j Current position in y sequence.
+   * @param cX Current reversed non-numeric CIGAR for the X sequence.
+   * @param cY Current reversed non-numeric CIGAR for the Y sequence.
+   * @return Returns the alignment CIGAR for the X and Y sequences, along with start indices.
+   *
+   * @note To work with the move function, expected move directions are:
+   *
+   * * I: move in I coordinate
+   * * J: move in J coordinate
+   * * B: move in both I and J
+   * * T: terminate move
+   *
+   * @see buildScoringMatrix
+   */
+  @tailrec private[smithwaterman] final def move(matrix: Array[Array[Char]],
+                                                 i: Int,
+                                                 j: Int,
+                                                 cX: String,
+                                                 cY: String): (String, String, Int, Int) = {
+    if (matrix(i)(j) == 'T') {
+      // return if told to terminate
+      (cigarFromRNNCigar(cX), cigarFromRNNCigar(cY), i, j)
+    } else {
+      // find next move
+      val (in, jn, cXn, cYn) = if (matrix(i)(j) == 'B') {
+        (i - 1, j - 1, cX + "M", cY + "M")
+      } else if (matrix(i)(j) == 'J') {
+        (i - 1, j, cX + "I", cY + "D")
+      } else {
+        (i, j - 1, cX + "D", cY + "I")
+      }
+
+      // recurse
+      move(matrix, in, jn, cXn, cYn)
+    }
+  }
+
+  /**
+   * Runs trackback on scoring matrix.
+   *
+   * @param scoreMatrix Scored matrix to track back on.
+   * @param moveMatrix Move matrix to track back on.
+   * @return Tuple of Cigar for X, Y.
+   */
+  private[smithwaterman] def trackback(scoreMatrix: Array[Array[Double]],
+                                       moveMatrix: Array[Array[Char]]): (Cigar, Cigar, Int, Int) = {
+    assert(scoreMatrix.length == xSequence.length + 1)
+    assert(scoreMatrix.forall(_.length == ySequence.length + 1))
+    assert(moveMatrix.length == xSequence.length + 1)
+    assert(moveMatrix.forall(_.length == ySequence.length + 1))
+
+    // get the position of the max scored box - start trackback here
+    val (sx, sy) = maxCoordinates(scoreMatrix)
+
+    // run trackback
+    val (cX, cY, xI, yI) = move(moveMatrix, sy, sx, "", "")
+
+    // get cigars and return
+    (TextCigarCodec.decode(cX), TextCigarCodec.decode(cY), xI, yI)
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/AlignmentRecordConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/AlignmentRecordConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/AlignmentRecordConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/AlignmentRecordConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,202 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import htsjdk.samtools.{ SAMFileHeader, SAMRecord }
+import org.bdgenomics.adam.models.{ RecordGroupDictionary, SAMFileHeaderWritable, SequenceDictionary }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+import org.bdgenomics.adam.instrumentation.Timers._
+
+class AlignmentRecordConverter extends Serializable {
+
+  /**
+   * Converts a single record to FASTQ. FASTQ format is:
+   *
+   * @readName
+   * sequence
+   * +<optional readname>
+   * ASCII quality scores
+   *
+   * @param adamRecord Read to convert to FASTQ.
+   * @return Returns this read in string form.
+   */
+  def convertToFastq(adamRecord: AlignmentRecord,
+                     maybeAddSuffix: Boolean = false): String = {
+    val readNameSuffix =
+      if (maybeAddSuffix &&
+        !AlignmentRecordConverter.readNameHasPairedSuffix(adamRecord) &&
+        adamRecord.getFirstOfPair != null) {
+        "/" + (
+          if (adamRecord.getFirstOfPair)
+            "1"
+          else
+            "2"
+        )
+      } else {
+        ""
+      }
+
+    "@%s%s\n%s\n+\n%s".format(
+      adamRecord.getReadName,
+      readNameSuffix,
+      adamRecord.getSequence,
+      adamRecord.getQual
+    )
+  }
+
+  /**
+   * Converts a single ADAM record into a SAM record.
+   *
+   * @param adamRecord ADAM formatted alignment record to convert.
+   * @param header SAM file header to use.
+   * @return Returns the record converted to SAMtools format. Can be used for output to SAM/BAM.
+   */
+  def convert(adamRecord: AlignmentRecord, header: SAMFileHeaderWritable): SAMRecord = ConvertToSAMRecord.time {
+
+    // get read group dictionary from header
+    val rgDict = header.header.getSequenceDictionary
+
+    // attach header
+    val builder: SAMRecord = new SAMRecord(header.header)
+
+    // set canonically necessary fields
+    builder.setReadName(adamRecord.getReadName.toString)
+    builder.setReadString(adamRecord.getSequence)
+    //builder.setInferredInsertSize(adamRecord.getRecordGroupPredictedMedianInsertSize)
+    builder.setInferredInsertSize(adamRecord.getInferredInsertSize)
+    adamRecord.getQual match {
+      case null      => builder.setBaseQualityString("*")
+      case s: String => builder.setBaseQualityString(s)
+    }
+
+    // set read group flags
+    Option(adamRecord.getRecordGroupName)
+      .map(_.toString)
+      .map(rgDict.getSequenceIndex)
+      .foreach(v => builder.setAttribute("RG", v.toString))
+    Option(adamRecord.getRecordGroupLibrary)
+      .foreach(v => builder.setAttribute("LB", v.toString))
+    Option(adamRecord.getRecordGroupPlatformUnit)
+      .foreach(v => builder.setAttribute("PU", v.toString))
+
+    // set the reference name, and alignment position, for mate
+    Option(adamRecord.getMateContig)
+      .map(_.getContigName)
+      .map(_.toString)
+      .foreach(builder.setMateReferenceName)
+    Option(adamRecord.getMateAlignmentStart)
+      .foreach(s => builder.setMateAlignmentStart(s.toInt + 1))
+
+    // set flags
+    Option(adamRecord.getReadPaired).foreach(p => {
+      builder.setReadPairedFlag(p.booleanValue)
+
+      // only set flags if read is paired
+      if (p) {
+        Option(adamRecord.getMateNegativeStrand)
+          .foreach(v => builder.setMateNegativeStrandFlag(v.booleanValue))
+        Option(adamRecord.getMateMapped)
+          .foreach(v => builder.setMateUnmappedFlag(!v.booleanValue))
+        Option(adamRecord.getProperPair)
+          .foreach(v => builder.setProperPairFlag(v.booleanValue))
+        Option(adamRecord.getFirstOfPair)
+          .foreach(v => builder.setFirstOfPairFlag(v.booleanValue))
+        Option(adamRecord.getSecondOfPair)
+          .foreach(v => builder.setSecondOfPairFlag(v.booleanValue))
+      }
+    })
+    Option(adamRecord.getDuplicateRead)
+      .foreach(v => builder.setDuplicateReadFlag(v.booleanValue))
+    Option(adamRecord.getReadMapped)
+      .foreach(m => {
+        builder.setReadUnmappedFlag(!m.booleanValue)
+
+        // only set alignment flags if read is aligned
+        if (m) {
+          // if we are aligned, we must have a reference
+          assert(adamRecord.getContig != null, "Cannot have null contig if aligned.")
+          builder.setReferenceName(adamRecord.getContig.getContigName)
+
+          // set the cigar, if provided
+          Option(adamRecord.getCigar).map(_.toString).foreach(builder.setCigarString)
+          // set the old cigar, if provided
+          Option(adamRecord.getOldCigar).map(_.toString).foreach(v => builder.setAttribute("OC", v))
+          // set mapping flags
+          Option(adamRecord.getReadNegativeStrand)
+            .foreach(v => builder.setReadNegativeStrandFlag(v.booleanValue))
+          Option(adamRecord.getPrimaryAlignment)
+            .foreach(v => builder.setNotPrimaryAlignmentFlag(!v.booleanValue))
+          Option(adamRecord.getSupplementaryAlignment)
+            .foreach(v => builder.setSupplementaryAlignmentFlag(v.booleanValue))
+          Option(adamRecord.getStart)
+            .foreach(s => builder.setAlignmentStart(s.toInt + 1))
+          Option(adamRecord.getOldPosition)
+            .foreach(s => builder.setAttribute("OP", s.toInt + 1))
+          Option(adamRecord.getMapq).foreach(v => builder.setMappingQuality(v))
+        } else {
+          // mapping quality must be 0 if read is unmapped
+          builder.setMappingQuality(0)
+        }
+      })
+    Option(adamRecord.getFailedVendorQualityChecks)
+      .foreach(v => builder.setReadFailsVendorQualityCheckFlag(v.booleanValue))
+    Option(adamRecord.getMismatchingPositions)
+      .map(_.toString)
+      .foreach(builder.setAttribute("MD", _))
+
+    // add all other tags
+    if (adamRecord.getAttributes != null) {
+      val mp = RichAlignmentRecord(adamRecord).tags
+      mp.foreach(a => {
+        builder.setAttribute(a.tag, a.value)
+      })
+    }
+
+    // return sam record 
+    builder
+  }
+
+  /**
+   * Creates a SAM formatted header. This can be used with SAM or BAM files.
+   *
+   * @param sd Reference sequence dictionary to use for conversion.
+   * @param rgd Dictionary containing record groups.
+   * @return Converted SAM formatted record.
+   */
+  def createSAMHeader(sd: SequenceDictionary, rgd: RecordGroupDictionary): SAMFileHeader = {
+    val samSequenceDictionary = sd.toSAMSequenceDictionary
+    val samHeader = new SAMFileHeader
+    samHeader.setSequenceDictionary(samSequenceDictionary)
+    rgd.recordGroups.foreach(group => samHeader.addReadGroup(group.toSAMReadGroupRecord()))
+
+    samHeader
+  }
+}
+
+object AlignmentRecordConverter {
+
+  def readNameHasPairedSuffix(adamRecord: AlignmentRecord): Boolean = {
+    adamRecord.getReadName.length() > 2 &&
+      adamRecord.getReadName.charAt(adamRecord.getReadName.length() - 2) == '/' &&
+      (adamRecord.getReadName.charAt(adamRecord.getReadName.length() - 1) == '1' ||
+        adamRecord.getReadName.charAt(adamRecord.getReadName.length() - 1) == '2')
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FastaConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FastaConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FastaConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FastaConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,227 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext._
+import org.bdgenomics.formats.avro.{ Contig, NucleotideContigFragment }
+import scala.Int
+import scala.Predef._
+import scala.Some
+import scala.collection.mutable
+
+/**
+ * Object for converting an RDD containing FASTA sequence data into ADAM FASTA data.
+ */
+private[adam] object FastaConverter {
+
+  case class FastaDescriptionLine(val fileIndex: Long = -1L, val seqId: Int = 0, val descriptionLine: Option[String] = None) {
+    val (contigName, contigDescription) = parseDescriptionLine(descriptionLine, fileIndex)
+
+    private def parseDescriptionLine(descriptionLine: Option[String], id: Long): (Option[String], Option[String]) = {
+      if (descriptionLine.isEmpty) {
+        assert(id == -1L, "Cannot have a headerless line in a file with more than one fragment.")
+        (None, None)
+      } else {
+        val splitIndex = descriptionLine.get.indexOf(' ')
+        if (splitIndex >= 0) {
+          val split = descriptionLine.get.splitAt(splitIndex)
+
+          val contigName: String = split._1.stripPrefix(">").trim
+          val contigDescription: String = split._2.trim
+
+          (Some(contigName), Some(contigDescription))
+
+        } else {
+          (Some(descriptionLine.get.stripPrefix(">").trim), None)
+        }
+      }
+    }
+  }
+
+  /**
+   * Converts an RDD containing ints and strings into an RDD containing ADAM nucleotide
+   * contig fragments.
+   *
+   * @note Input dataset is assumed to have come in from a Hadoop TextInputFormat reader. This sets
+   * a specific format for the RDD's Key-Value pairs.
+   *
+   * @throws AssertionError Thrown if there appear to be multiple sequences in a single file
+   * that do not have descriptions.
+   * @throws IllegalArgumentError Thrown if a sequence does not have sequence data.
+   *
+   * @param rdd RDD containing Long,String tuples, where the Long corresponds to the number
+   * of the file line, and the String is the line of the file.
+   * @param maxFragmentLength The maximum length of fragments in the contig.
+   * @return An RDD of ADAM FASTA data.
+   */
+  def apply(rdd: RDD[(Long, String)],
+            maxFragmentLength: Long = 10000L): RDD[NucleotideContigFragment] = {
+    val filtered = rdd.map(kv => (kv._1, kv._2.trim()))
+      .filter((kv: (Long, String)) => !kv._2.startsWith(";"))
+
+    val descriptionLines: Map[Long, FastaDescriptionLine] = getDescriptionLines(filtered)
+    val indexToContigDescription = rdd.context.broadcast(descriptionLines)
+
+    val sequenceLines = filtered.filter(kv => !isDescriptionLine(kv._2))
+
+    val keyedSequences = if (indexToContigDescription.value.size == 0) {
+      sequenceLines.keyBy(kv => -1L)
+    } else {
+      sequenceLines.keyBy(row => findContigIndex(row._1, indexToContigDescription.value.keys.toList))
+    }
+    val groupedContigs = keyedSequences.groupByKey()
+
+    val converter = new FastaConverter(maxFragmentLength)
+
+    groupedContigs.flatMap {
+      case (id, lines) =>
+
+        val descriptionLine = indexToContigDescription.value.getOrElse(id, FastaDescriptionLine())
+        assert(lines.size != 0, "Sequence " + descriptionLine.seqId + " has no sequence data.")
+
+        val sequence: Seq[String] = lines.toSeq.sortBy(_._1).map(kv => cleanSequence(kv._2))
+        converter.convert(descriptionLine.contigName,
+          descriptionLine.seqId,
+          sequence,
+          descriptionLine.contigDescription)
+    }
+
+  }
+
+  private def cleanSequence(sequence: String): String = {
+    sequence.stripSuffix("*")
+  }
+
+  private def isDescriptionLine(line: String): Boolean = {
+    line.startsWith(">")
+  }
+
+  def getDescriptionLines(rdd: RDD[(Long, String)]): Map[Long, FastaDescriptionLine] = {
+
+    rdd.filter(kv => isDescriptionLine(kv._2))
+      .collect()
+      .zipWithIndex
+      .map(kv => (kv._1._1, FastaDescriptionLine(kv._1._1, kv._2, Some(kv._1._2))))
+      .toMap
+  }
+
+  def findContigIndex(rowIdx: Long, indices: List[Long]): Long = {
+    val idx = indices.filter(_ <= rowIdx)
+    idx.max
+  }
+}
+
+/**
+ * Conversion methods for single FASTA sequences into ADAM FASTA data.
+ */
+private[converters] class FastaConverter(fragmentLength: Long) extends Serializable {
+
+  /**
+   * Remaps the fragments that we get coming in into our expected fragment size.
+   *
+   * @param sequences Fragments coming in.
+   * @return A sequence of strings "recut" to the proper fragment size.
+   */
+  def mapFragments(sequences: Seq[String]): Seq[String] = {
+    // internal "fsm" variables
+    var sequence: StringBuilder = new StringBuilder
+    var sequenceSeq: mutable.MutableList[String] = mutable.MutableList()
+    var extendLength = fragmentLength + 1300
+
+    /**
+     * Adds a string fragment to our accumulator. If this string fragment causes the accumulator
+     * to grow longer than our max fragment size, we split the accumulator and add it to the end
+     * of our list of fragments.
+     *
+     * @param seq Fragment string to add.
+     */
+    def addFragment(seq: String) {
+      sequence.append(seq)
+
+      while (sequence.length > extendLength) {
+        sequenceSeq += sequence.take(extendLength.toInt).toString()
+        sequence = sequence.drop(fragmentLength.toInt)
+      }
+    }
+
+    (1 to 500).foreach(p => sequence.append('N'))
+
+    // run addFragment on all fragments
+    sequences.foreach(addFragment)
+
+    // if we still have a remaining sequence that is not part of a fragment, add it
+    // --TODO need to modify to 1300
+    if (sequence.length >= 500) {
+      sequenceSeq += sequence.toString()
+    }
+
+    // return our fragments
+    sequenceSeq.toSeq
+  }
+
+  /**
+   * Converts a single FASTA sequence into an ADAM FASTA contig.
+   *
+   * @throws IllegalArgumentException Thrown if sequence contains an illegal character.
+   *
+   * @param name String option for the sequence name.
+   * @param id Numerical identifier for the sequence.
+   * @param sequence Nucleotide sequence.
+   * @param description Optional description of the sequence.
+   * @return The converted ADAM FASTA contig.
+   */
+  def convert(name: Option[String],
+              id: Int,
+              sequence: Seq[String],
+              description: Option[String]): Seq[NucleotideContigFragment] = {
+
+    // get sequence length
+    val sequenceLength = sequence.map(_.length).reduce(_ + _)
+
+    // map sequences into fragments
+    val sequencesAsFragments = mapFragments(sequence)
+
+    // get number of fragments
+    val fragmentCount = sequencesAsFragments.length
+
+    // make new builder and set up non-optional fields
+    val fragments = sequencesAsFragments.zipWithIndex
+      .map(si => {
+        val (bases, index) = si
+
+        val contig = Contig.newBuilder
+          .setContigLength(sequenceLength)
+
+        val builder = NucleotideContigFragment.newBuilder()
+          .setFragmentSequence(bases)
+          .setFragmentNumber(index)
+          .setFragmentStartPosition(index * fragmentLength)
+          .setNumberOfFragmentsInContig(fragmentCount)
+
+        // map over optional fields
+        name.foreach(contig.setContigName(_))
+        description.foreach(builder.setDescription(_))
+        builder.setContig(contig.build)
+        // build and return
+        builder.build()
+      })
+
+    fragments
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FastqRecordConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FastqRecordConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FastqRecordConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FastqRecordConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,104 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import org.apache.hadoop.io.Text
+import org.apache.spark.Logging
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+class FastqRecordConverter extends Serializable with Logging {
+
+  def convertPair(element: (Void, Text)): Iterable[AlignmentRecord] = {
+    val lines = element._2.toString.split('\n')
+    assert(lines.length == 8, "Record has wrong format:\n" + element._2.toString)
+
+    // get fields for first read in pair
+    val firstReadName = lines(0).drop(1)
+    val firstReadSequence = lines(1)
+    val firstReadQualities = lines(3)
+
+    assert(firstReadSequence.length == firstReadQualities.length,
+      "Read " + firstReadName + " has different sequence and qual length.")
+
+    // get fields for second read in pair
+    val secondReadName = lines(4).drop(1)
+    val secondReadSequence = lines(5)
+    val secondReadQualities = lines(7)
+
+    assert(secondReadSequence.length == secondReadQualities.length,
+      "Read " + secondReadName + " has different sequence and qual length.")
+
+    // build and return iterators
+    Iterable(AlignmentRecord.newBuilder()
+      .setReadName(firstReadName)
+      .setSequence(firstReadSequence)
+      .setQual(firstReadQualities)
+      .setReadPaired(true)
+      .setProperPair(true)
+      .setFirstOfPair(true)
+      .setSecondOfPair(false)
+      .setReadNegativeStrand(null)
+      .setMateNegativeStrand(null)
+      .setPrimaryAlignment(null)
+      .setSecondaryAlignment(null)
+      .setSupplementaryAlignment(null)
+      .build(),
+      AlignmentRecord.newBuilder()
+        .setReadName(secondReadName)
+        .setSequence(secondReadSequence)
+        .setQual(secondReadQualities)
+        .setReadPaired(true)
+        .setProperPair(true)
+        .setFirstOfPair(false)
+        .setSecondOfPair(true)
+        .setReadNegativeStrand(null)
+        .setMateNegativeStrand(null)
+        .setPrimaryAlignment(null)
+        .setSecondaryAlignment(null)
+        .setSupplementaryAlignment(null)
+        .build())
+  }
+
+  def convertRead(element: (Void, Text)): AlignmentRecord = {
+    val lines = element._2.toString.split('\n')
+    assert(lines.length == 4, "Record has wrong format:\n" + element._2.toString)
+
+    // get fields for first read in pair
+    val readName = lines(0).drop(1)
+    val readSequence = lines(1)
+    val readQualities = lines(3)
+
+    assert(readSequence.length == readQualities.length,
+      "Read " + readName + " has different sequence and qual length.")
+
+    AlignmentRecord.newBuilder()
+      .setReadName(readName)
+      .setSequence(readSequence)
+      .setQual(readQualities)
+      .setReadPaired(false)
+      .setProperPair(null)
+      .setFirstOfPair(null)
+      .setSecondOfPair(null)
+      .setReadNegativeStrand(null)
+      .setMateNegativeStrand(null)
+      .setPrimaryAlignment(null)
+      .setSecondaryAlignment(null)
+      .setSupplementaryAlignment(null)
+      .build()
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FragmentConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FragmentConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/FragmentConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/FragmentConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,115 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.{ActiveRegion, ReferencePosition, ReferenceRegion}
+import org.bdgenomics.formats.avro._
+import scala.annotation.tailrec
+
+private[converters] object FragmentCollector extends Serializable {
+  def apply(fragment: NucleotideContigFragment): (Contig, FragmentCollector) = {
+    (fragment.getContig,
+      FragmentCollector(Seq((ReferenceRegion(fragment).get, fragment.getFragmentSequence))))
+  }
+}
+
+private[converters] case class FragmentCollector(fragments: Seq[(ReferenceRegion, String)]) {
+}
+
+object FragmentConverter extends Serializable {
+
+  private def mergeFragments(f1: FragmentCollector,
+                             f2: FragmentCollector): FragmentCollector = {
+    assert(!(f1.fragments.isEmpty || f2.fragments.isEmpty))
+
+    // join fragments from each and sort
+    val fragments = (f1.fragments ++ f2.fragments).sortBy(_._1)
+
+    var fragmentList = List[(ReferenceRegion, String)]()
+
+    @tailrec def fragmentCombiner(lastFragment: (ReferenceRegion, String),
+                                  iter: Iterator[(ReferenceRegion, String)]) {
+      if (!iter.hasNext) {
+        // prepend fragment to list
+        fragmentList = lastFragment :: fragmentList
+      } else {
+        // extract our next fragment, and our last fragment
+        val (lastRegion, lastString) = lastFragment
+        val (thisRegion, thisString) = iter.next
+
+        // are our fragments adjacent?
+        val newLastFragment = if (lastRegion.isAdjacent(thisRegion)) {
+          // if they are, merge the fragments
+          // we have sorted these fragments before we started, so the orientation is already known
+          (lastRegion.hull(thisRegion), lastString + thisString)
+        } else {
+          // if they aren't, prepend the last fragment to the list
+          // and use the current fragment as the new last fragment
+          fragmentList = lastFragment :: fragmentList
+          (thisRegion, thisString)
+        }
+
+        // recurse
+        fragmentCombiner(newLastFragment, iter)
+      }
+    }
+
+    // convert to an iterator and peek at the first element and recurse
+    val fragmentIter = fragments.toIterator
+    fragmentCombiner(fragmentIter.next, fragmentIter)
+
+    // create a new collector
+    FragmentCollector(fragmentList.toSeq)
+  }
+
+  private[converters] def convertFragment(kv: (Contig, FragmentCollector)): Seq[AlignmentRecord] = {
+    // extract kv pair
+    val (contig, fragment) = kv
+
+    // extract the fragment string and region
+    fragment.fragments.map(p => {
+      val (fragmentRegion, fragmentString) = p
+
+      // build record
+      AlignmentRecord.newBuilder()
+        .setContig(contig)
+        .setStart(fragmentRegion.start)
+        .setEnd(fragmentRegion.end)
+        .setSequence(fragmentString)
+        .build()
+    })
+  }
+
+  def convertRdd(rdd: RDD[NucleotideContigFragment]): RDD[AlignmentRecord] = {
+    rdd.map(FragmentCollector(_))
+      .reduceByKey(mergeFragments)
+      .flatMap(convertFragment)
+  }
+
+  /*
+  def convertRich(rdd: RDD[NucleotideContigFragment], records: RDD[AlignmentRecord], fragmentLength: Long): RDD[RichNucleotideContigFragment] = {
+    val crossRecords = records.filter(r => r.getStart/fragmentLength != r.getEnd/fragmentLength)
+    val keyRecords = records.groupBy(r => ReferencePosition(r.getContig.getContigName, r.getStart/fragmentLength)).
+      union(crossRecords.groupBy(r => ReferencePosition(r.getContig.getContigName, r.getEnd/fragmentLength)))
+    val keyRdd = rdd.keyBy(f => ReferencePosition(f.getContig.getContigName, f.getFragmentStartPosition/fragmentLength))
+    keyRdd.join(keyRecords).map(f => RichNucleotideContigFragment.fragmentToRichFragment(f._2._1, RichNucleotideContigFragment.calculateScore(f._2._1, f._2._2.toSeq)))
+  }
+  */
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/GenotypesToVariantsConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/GenotypesToVariantsConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/GenotypesToVariantsConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/GenotypesToVariantsConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,71 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import org.bdgenomics.adam.util._
+import scala.math.{ pow, sqrt }
+
+private[adam] class GenotypesToVariantsConverter(validateSamples: Boolean = false,
+                                                 failOnValidationError: Boolean = false) extends Serializable {
+
+  /**
+   * Computes root mean squared (RMS) values for a series of doubles.
+   *
+   * @param values A series of doubles.
+   * @return The RMS of this series.
+   */
+  def rms(values: Seq[Double]): Double = {
+    if (values.length > 0) {
+      sqrt(values.map(pow(_, 2.0)).reduce(_ + _) / values.length.toDouble)
+    } else {
+      0.0
+    }
+  }
+
+  /**
+   * Computes root mean squared (RMS) values for a series of phred scaled quality scores.
+   *
+   * @param values A series of phred scores.
+   * @return The RMS of this series.
+   */
+  def rms(values: Seq[Int]): Int = {
+    if (values.length > 0) {
+      PhredUtils.successProbabilityToPhred(rms(values.map(PhredUtils.phredToSuccessProbability)))
+    } else {
+      0
+    }
+  }
+
+  /**
+   * Finds variant quality from genotype qualities. Variant quality is defined as the likelihood
+   * that at least 1 variant exists in the set of samples we have seen. This can be rephrased as
+   * the likelihood that there are not 0 variants in the set of samples we have seen. We can
+   * assume that all of our genotypes are Bernouli with p=genotype quality. Then, this calculation
+   * becomes:
+   *
+   * P(X = 0) = product, g in genotypes -> (1 - Pg)
+   * P(X > 0) = 1 - P(X = 0)
+   *
+   * Where Pg is the per genotype likelihood that the genotype is correct, and X is the number
+   * of times we see a variant.
+   *
+   * @param values An array of non-phred scaled genotype quality scores.
+   * @return A non-phred scaled variant likelihood.
+   */
+  def variantQualityFromGenotypes(values: Seq[Double]): Double = 1.0 - values.reduce(_ * _)
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/SAMRecordConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/SAMRecordConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/SAMRecordConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/SAMRecordConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,210 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import htsjdk.samtools.{
+  CigarElement,
+  SAMReadGroupRecord,
+  SAMRecord,
+  SAMUtils
+}
+import org.apache.spark.Logging
+import org.bdgenomics.adam.models.{
+  Attribute,
+  RecordGroupDictionary,
+  SequenceDictionary,
+  SequenceRecord
+}
+import org.bdgenomics.adam.util.AttributeUtils
+import org.bdgenomics.formats.avro.AlignmentRecord
+import scala.collection.JavaConverters._
+
+class SAMRecordConverter extends Serializable with Logging {
+  def convert(samRecord: SAMRecord,
+              dict: SequenceDictionary,
+              readGroups: RecordGroupDictionary): AlignmentRecord = {
+    try {
+      val cigar: String = samRecord.getCigarString
+      val startTrim = if (cigar == "*") {
+        0
+      } else {
+        val count = cigar.takeWhile(_.isDigit).toInt
+        val operator = cigar.dropWhile(_.isDigit).head
+
+        if (operator == 'H') {
+          count
+        } else {
+          0
+        }
+      }
+      val endTrim = if (cigar.endsWith("H")) {
+        // must reverse string as takeWhile is not implemented in reverse direction
+        cigar.dropRight(1).reverse.takeWhile(_.isDigit).reverse.toInt
+      } else {
+        0
+      }
+
+      val builder: AlignmentRecord.Builder = AlignmentRecord.newBuilder
+        .setReadName(samRecord.getReadName)
+        .setSequence(samRecord.getReadString)
+        .setCigar(cigar)
+        .setBasesTrimmedFromStart(startTrim)
+        .setBasesTrimmedFromEnd(endTrim)
+        .setOrigQual(SAMUtils.phredToFastq(samRecord.getOriginalBaseQualities))
+
+      // if the quality string is "*", then we null it in the record
+      // or, in other words, we only set the quality string if it is not "*"
+      val qual = samRecord.getBaseQualityString
+      if (qual != "*") {
+        builder.setQual(qual)
+      }
+
+      // Only set the reference information if the read is aligned, matching the mate reference
+      // This prevents looking up a -1 in the sequence dictionary
+      val readReference: Int = samRecord.getReferenceIndex
+      if (readReference != SAMRecord.NO_ALIGNMENT_REFERENCE_INDEX) {
+        builder.setContig(SequenceRecord.toADAMContig(dict(samRecord.getReferenceName).get))
+
+        // set read alignment flag
+        val start: Int = samRecord.getAlignmentStart
+        assert(start != 0, "Start cannot equal 0 if contig is set.")
+        builder.setStart((start - 1).asInstanceOf[Long])
+
+        // set OP and OC flags, if applicable
+        if (samRecord.getAttribute("OP") != null) {
+          builder.setOldPosition(samRecord.getIntegerAttribute("OP").toLong - 1)
+          builder.setOldCigar(samRecord.getStringAttribute("OC"))
+        }
+
+        val end = samRecord.getCigar.getCigarElements
+          .asScala
+          .filter((p: CigarElement) => p.getOperator.consumesReferenceBases())
+          .foldLeft(start - 1) {
+            (pos, cigarEl) => pos + cigarEl.getLength
+          }
+        builder.setEnd(end)
+        // set mapping quality
+        val mapq: Int = samRecord.getMappingQuality
+
+        if (mapq != SAMRecord.UNKNOWN_MAPPING_QUALITY) {
+          builder.setMapq(mapq)
+        }
+
+        // set mapping flags
+        // oddly enough, it appears that reads can show up with mapping info (mapq, cigar, position)
+        // even if the read unmapped flag is set...
+        if (samRecord.getReadUnmappedFlag) {
+          builder.setReadMapped(false)
+        } else {
+          builder.setReadMapped(true)
+          if (samRecord.getReadNegativeStrandFlag) {
+            builder.setReadNegativeStrand(true)
+          }
+          if (!samRecord.getNotPrimaryAlignmentFlag) {
+            builder.setPrimaryAlignment(true)
+          } else {
+            // if the read is not a primary alignment, it can be either secondary or supplementary
+            // - secondary: not the best linear alignment
+            // - supplementary: part of a chimeric alignment
+            builder.setSupplementaryAlignment(samRecord.getSupplementaryAlignmentFlag)
+            builder.setSecondaryAlignment(!samRecord.getSupplementaryAlignmentFlag)
+          }
+        }
+      }
+
+      // Position of the mate/next segment
+      val mateReference: Int = samRecord.getMateReferenceIndex
+
+      if (mateReference != SAMRecord.NO_ALIGNMENT_REFERENCE_INDEX) {
+        builder.setMateContig(SequenceRecord.toADAMContig(dict(samRecord.getMateReferenceName).get))
+
+        val mateStart = samRecord.getMateAlignmentStart
+        if (mateStart > 0) {
+          // We subtract one here to be 0-based offset
+          builder.setMateAlignmentStart(mateStart - 1)
+        }
+      }
+      builder.setInferredInsertSize(samRecord.getInferredInsertSize)
+
+      // The Avro scheme defines all flags as defaulting to 'false'. We only need to set the flags that are true.
+      if (samRecord.getFlags != 0) {
+        if (samRecord.getReadPairedFlag) {
+          builder.setReadPaired(true)
+          if (samRecord.getMateNegativeStrandFlag) {
+            builder.setMateNegativeStrand(true)
+          }
+          if (!samRecord.getMateUnmappedFlag) {
+            builder.setMateMapped(true)
+          }
+          if (samRecord.getProperPairFlag) {
+            builder.setProperPair(true)
+          }
+          if (samRecord.getFirstOfPairFlag) {
+            builder.setFirstOfPair(true)
+          }
+          if (samRecord.getSecondOfPairFlag) {
+            builder.setSecondOfPair(true)
+          }
+        }
+        if (samRecord.getDuplicateReadFlag) {
+          builder.setDuplicateRead(true)
+        }
+        if (samRecord.getReadFailsVendorQualityCheckFlag) {
+          builder.setFailedVendorQualityChecks(true)
+        }
+      }
+
+      if (samRecord.getAttributes != null) {
+        var tags = List[Attribute]()
+        samRecord.getAttributes.asScala.foreach {
+          attr =>
+            if (attr.tag == "MD") {
+              builder.setMismatchingPositions(attr.value.toString)
+            } else {
+              tags ::= AttributeUtils.convertSAMTagAndValue(attr)
+            }
+        }
+        builder.setAttributes(tags.mkString("\t"))
+      }
+
+      val recordGroup: SAMReadGroupRecord = samRecord.getReadGroup
+      if (recordGroup != null) {
+        Option(recordGroup.getRunDate).foreach(date => builder.setRecordGroupRunDateEpoch(date.getTime))
+
+        builder.setRecordGroupName(recordGroup.getReadGroupId)
+          .setRecordGroupSequencingCenter(recordGroup.getSequencingCenter)
+          .setRecordGroupDescription(recordGroup.getDescription)
+          .setRecordGroupFlowOrder(recordGroup.getFlowOrder)
+          .setRecordGroupKeySequence(recordGroup.getKeySequence)
+          .setRecordGroupLibrary(recordGroup.getLibrary)
+          .setRecordGroupPredictedMedianInsertSize(recordGroup.getPredictedMedianInsertSize)
+          .setRecordGroupPlatform(recordGroup.getPlatform)
+          .setRecordGroupPlatformUnit(recordGroup.getPlatformUnit)
+          .setRecordGroupSample(recordGroup.getSample)
+      }
+
+      builder.build
+    } catch {
+      case t: Throwable => {
+        log.error("Conversion of read: " + samRecord + " failed.")
+        throw t
+      }
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/VariantAnnotationConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/VariantAnnotationConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/VariantAnnotationConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/VariantAnnotationConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,192 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+/*
+* Copyright (c) 2014. Mount Sinai School of Medicine
+*
+* Licensed under the Apache License, Version 2.0 (the "License");
+* you may not use this file except in compliance with the License.
+* You may obtain a copy of the License at
+*
+*     http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing, software
+* distributed under the License is distributed on an "AS IS" BASIS,
+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+* See the License for the specific language governing permissions and
+* limitations under the License.
+*/
+
+package org.bdgenomics.adam.converters
+
+import htsjdk.variant.variantcontext.VariantContext
+import org.apache.avro.Schema
+import org.apache.avro.specific.SpecificRecord
+import htsjdk.variant.vcf._
+import org.bdgenomics.formats.avro.{ DatabaseVariantAnnotation, Genotype, VariantCallingAnnotations }
+
+object AttrKey {
+  def apply(adamKey: String, hdrLine: VCFCompoundHeaderLine): AttrKey = {
+    new AttrKey(adamKey, null, hdrLine)
+  }
+}
+
+case class AttrKey(adamKey: String, attrConverter: (Object => Object), hdrLine: VCFCompoundHeaderLine) {
+  val vcfKey: String = hdrLine.getID
+}
+
+object VariantAnnotationConverter extends Serializable {
+
+  private def attrAsInt(attr: Object): Object = attr match {
+    case a: String            => java.lang.Integer.valueOf(a)
+    case a: java.lang.Integer => a
+    case a: java.lang.Number  => java.lang.Integer.valueOf(a.intValue)
+  }
+  private def attrAsLong(attr: Object): Object = attr match {
+    case a: String           => java.lang.Long.valueOf(a)
+    case a: java.lang.Long   => a
+    case a: java.lang.Number => java.lang.Long.valueOf(a.longValue)
+  }
+  private def attrAsFloat(attr: Object): Object = attr match {
+    case a: String           => java.lang.Float.valueOf(a)
+    case a: java.lang.Float  => a
+    case a: java.lang.Number => java.lang.Float.valueOf(a.floatValue)
+  }
+  private def attrAsString(attr: Object): Object = attr match {
+    case a: String => a
+  }
+  private def attrAsBoolean(attr: Object): Object = attr match {
+    case a: java.lang.Boolean => a
+    case a: String            => java.lang.Boolean.valueOf(a)
+  }
+
+  val COSMIC_KEYS: List[AttrKey] = List(
+    AttrKey("geneSymbol", attrAsString _, new VCFInfoHeaderLine("GENE,", 1, VCFHeaderLineType.String, "Gene name")),
+    AttrKey("strand", attrAsString _, new VCFInfoHeaderLine("STRAND,", 1, VCFHeaderLineType.String, "Gene strand")),
+    AttrKey("cds", attrAsString _, new VCFInfoHeaderLine("CDS,", 1, VCFHeaderLineType.String, "CDS annotation")),
+    AttrKey("cnt", attrAsString _, new VCFInfoHeaderLine("CNT,", 1, VCFHeaderLineType.Integer, "How many samples have this mutation")))
+
+  val DBNSFP_KEYS: List[AttrKey] = List(
+    AttrKey("phylop", attrAsFloat _, new VCFInfoHeaderLine("PHYLOP", 1, VCFHeaderLineType.Float, "PhyloP score. The larger the score, the more conserved the site.")),
+    AttrKey("siftPred", attrAsString _, new VCFInfoHeaderLine("SIFT_PRED", 1, VCFHeaderLineType.Character, "SIFT Prediction: D (damaging), T (tolerated)")),
+    AttrKey("siftScore", attrAsFloat _, new VCFInfoHeaderLine("SIFT_SCORE", 1, VCFHeaderLineType.Float, "SIFT Score")),
+    AttrKey("ancestralAllele", attrAsString _, new VCFInfoHeaderLine("AA", 1, VCFHeaderLineType.String, "Ancestral allele")))
+
+  val CLINVAR_KEYS: List[AttrKey] = List(
+    AttrKey("dbSnpId", attrAsInt _, new VCFInfoHeaderLine("dbSNP ID", 1, VCFHeaderLineType.Integer, "dbSNP ID")),
+    AttrKey("geneSymbol", attrAsString _, new VCFInfoHeaderLine("GENEINFO", 1, VCFHeaderLineType.String, "Pairs each of gene symbol:gene id.  The gene symbol and id are delimited by a colon (:) and each pair is delimited by a vertical bar")))
+
+  val OMIM_KEYS: List[AttrKey] = List(
+    AttrKey("omimId", attrAsString _, new VCFInfoHeaderLine("VAR", 1, VCFHeaderLineType.String, "MIM entry with variant mapped to rsID")))
+
+  val INFO_KEYS: Seq[AttrKey] = Seq(
+    AttrKey("clippingRankSum", attrAsFloat _, new VCFInfoHeaderLine("ClippingRankSum", 1, VCFHeaderLineType.Float, "Z-score From Wilcoxon rank sum test of Alt vs. Ref number of hard clipped bases")),
+    AttrKey("readDepth", attrAsInt _, VCFStandardHeaderLines.getInfoLine(VCFConstants.DEPTH_KEY)),
+    AttrKey("fisherStrandBiasPValue", attrAsFloat _, VCFStandardHeaderLines.getInfoLine(VCFConstants.STRAND_BIAS_KEY)),
+    AttrKey("haplotypeScore", attrAsFloat _, new VCFInfoHeaderLine("HaplotypeScore", 1, VCFHeaderLineType.Float, "Consistency of the site with at most two segregating haplotypes")),
+    AttrKey("inbreedingCoefficient", attrAsFloat _, new VCFInfoHeaderLine("InbreedingCoeff", 1, VCFHeaderLineType.Float, "Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation")),
+    AttrKey("rmsMapQ", attrAsFloat _, VCFStandardHeaderLines.getInfoLine(VCFConstants.RMS_MAPPING_QUALITY_KEY)),
+    AttrKey("mapq0Reads", attrAsInt _, VCFStandardHeaderLines.getInfoLine(VCFConstants.MAPPING_QUALITY_ZERO_KEY)),
+    AttrKey("mqRankSum", attrAsFloat _, new VCFInfoHeaderLine("MQRankSum", 1, VCFHeaderLineType.Float, "Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities")),
+    AttrKey("usedForNegativeTrainingSet", attrAsBoolean _, new VCFInfoHeaderLine("NEGATIVE_TRAIN_SITE", 1, VCFHeaderLineType.Flag, "This variant was used to build the negative training set of bad variants")),
+    AttrKey("usedForPositiveTrainingSet", attrAsBoolean _, new VCFInfoHeaderLine("POSITIVE_TRAIN_SITE", 1, VCFHeaderLineType.Flag, "This variant was used to build the positive training set of good variants")),
+    AttrKey("variantQualityByDepth", attrAsFloat _, new VCFInfoHeaderLine("QD", 1, VCFHeaderLineType.Float, "Variant Confidence/Quality by Depth")),
+    AttrKey("readPositionRankSum", attrAsFloat _, new VCFInfoHeaderLine("ReadPosRankSum", 1, VCFHeaderLineType.Float, "Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias")),
+    AttrKey("vqslod", attrAsFloat _, new VCFInfoHeaderLine("VQSLOD", 1, VCFHeaderLineType.Float, "Log odds ratio of being a true variant versus being false under the trained gaussian mixture model")),
+    AttrKey("culprit", attrAsString _, new VCFInfoHeaderLine("culprit", 1, VCFHeaderLineType.String, "The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out")))
+
+  val FORMAT_KEYS: Seq[AttrKey] = Seq(
+    AttrKey("alleles", VCFStandardHeaderLines.getFormatLine(VCFConstants.GENOTYPE_KEY)),
+    AttrKey("gtQuality", VCFStandardHeaderLines.getFormatLine(VCFConstants.GENOTYPE_QUALITY_KEY)),
+    AttrKey("readDepth", VCFStandardHeaderLines.getFormatLine(VCFConstants.DEPTH_KEY)),
+    AttrKey("alleleDepths", VCFStandardHeaderLines.getFormatLine(VCFConstants.GENOTYPE_ALLELE_DEPTHS)),
+    AttrKey("gtFilters", VCFStandardHeaderLines.getFormatLine(VCFConstants.GENOTYPE_FILTER_KEY)),
+    AttrKey("genotypeLikelihoods", VCFStandardHeaderLines.getFormatLine(VCFConstants.GENOTYPE_PL_KEY)),
+    AttrKey("phaseQuality", attrAsInt _, new VCFFormatHeaderLine(VCFConstants.PHASE_QUALITY_KEY, 1, VCFHeaderLineType.Float, "Read-backed phasing quality")),
+    AttrKey("phaseSetId", attrAsInt _, new VCFFormatHeaderLine(VCFConstants.PHASE_SET_KEY, 1, VCFHeaderLineType.Integer, "Phase set")),
+    AttrKey("minReadDepth", attrAsInt _, new VCFFormatHeaderLine("MIN_DP", 1, VCFHeaderLineType.Integer, "Minimum DP observed within the GVCF block")),
+    AttrKey("strandBiasComponents", attrAsInt _, new VCFFormatHeaderLine("SB", 4, VCFHeaderLineType.Integer, "Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.")))
+
+  lazy val infoHeaderLines: Seq[VCFCompoundHeaderLine] = INFO_KEYS.map(_.hdrLine)
+  lazy val formatHeaderLines: Seq[VCFCompoundHeaderLine] = FORMAT_KEYS.map(_.hdrLine)
+
+  lazy val VCF2VariantCallingAnnotations: Map[String, (Int, Object => Object)] =
+    createFieldMap(INFO_KEYS, VariantCallingAnnotations.getClassSchema)
+  lazy val VCF2GenotypeAnnotations: Map[String, (Int, Object => Object)] =
+    createFieldMap(FORMAT_KEYS, Genotype.getClassSchema)
+
+  private lazy val EXTERNAL_DATABASE_KEYS: Seq[AttrKey] = OMIM_KEYS ::: CLINVAR_KEYS ::: DBNSFP_KEYS // ::: COSMIC_KEYS
+  lazy val VCF2DatabaseAnnotations: Map[String, (Int, Object => Object)] = createFieldMap(EXTERNAL_DATABASE_KEYS, DatabaseVariantAnnotation.getClassSchema)
+
+  private def createFieldMap(keys: Seq[AttrKey], schema: Schema): Map[String, (Int, Object => Object)] = {
+    keys.filter(_.attrConverter != null).map(field => {
+      val avroField = schema.getField(field.adamKey)
+      field.vcfKey -> (avroField.pos, field.attrConverter)
+    })(collection.breakOut)
+  }
+
+  private def fillRecord[T <% SpecificRecord](fieldMap: Map[String, (Int, Object => Object)], vc: VariantContext, record: T): T = {
+    for ((v, a) <- fieldMap) {
+      val attr = vc.getAttribute(v)
+      if (attr != null && attr != VCFConstants.MISSING_VALUE_v4) {
+        record.put(a._1, a._2(attr))
+      }
+    }
+    record
+  }
+
+  private def fillKeys[T <% SpecificRecord](keys: Seq[AttrKey], vc: VariantContext, record: T): T = {
+    fillRecord(createFieldMap(keys, record.getSchema), vc, record)
+  }
+
+  def convert(vc: VariantContext, annotation: DatabaseVariantAnnotation): DatabaseVariantAnnotation = {
+    fillRecord(VCF2DatabaseAnnotations, vc, annotation)
+  }
+
+  def convert(vc: VariantContext, call: VariantCallingAnnotations): VariantCallingAnnotations = {
+    fillRecord(VCF2VariantCallingAnnotations, vc, call)
+  }
+
+  def convert(g: htsjdk.variant.variantcontext.Genotype, genotype: Genotype): Genotype = {
+    for ((v, a) <- VariantAnnotationConverter.VCF2GenotypeAnnotations) {
+      // Add extended attributes if present
+      val attr = g.getExtendedAttribute(v)
+      if (attr != null && attr != VCFConstants.MISSING_VALUE_v4) {
+        genotype.put(a._1, a._2(attr))
+      }
+    }
+    genotype
+  }
+
+  def mergeAnnotations(leftRecord: DatabaseVariantAnnotation, rightRecord: DatabaseVariantAnnotation): DatabaseVariantAnnotation = {
+    val mergedAnnotation = DatabaseVariantAnnotation.newBuilder(leftRecord).build()
+    val numFields = DatabaseVariantAnnotation.getClassSchema.getFields.size
+
+    def insertField(fieldIdx: Int) =
+      {
+        val value = rightRecord.get(fieldIdx)
+        if (value != null) {
+          mergedAnnotation.put(fieldIdx, value)
+        }
+      }
+    (0 until numFields).foreach(insertField(_))
+
+    mergedAnnotation
+
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/VariantContextConverter.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/VariantContextConverter.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/converters/VariantContextConverter.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/converters/VariantContextConverter.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,364 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.converters
+
+import htsjdk.variant.variantcontext.{
+  Allele,
+  GenotypesContext,
+  GenotypeLikelihoods,
+  VariantContext => BroadVariantContext,
+  VariantContextBuilder
+}
+import java.util.Collections
+import org.apache.spark.Logging
+import org.bdgenomics.adam.models.{ VariantContext => ADAMVariantContext, SequenceDictionary }
+import org.bdgenomics.formats.avro._
+import scala.collection.JavaConversions._
+
+object VariantContextConverter {
+  private val NON_REF_ALLELE = Allele.create("<NON_REF>", false /* !Reference */ )
+  private lazy val splitFromMultiAllelicField = Genotype.SCHEMA$.getField("splitFromMultiAllelic")
+
+  // One conversion method for each way of representing an Allele
+  private def convertAllele(vc: BroadVariantContext, allele: Allele): GenotypeAllele = {
+    if (allele.isNoCall) GenotypeAllele.NoCall
+    else if (allele.isReference) GenotypeAllele.Ref
+    else if (allele == NON_REF_ALLELE || !vc.hasAlternateAllele(allele)) GenotypeAllele.OtherAlt
+    else GenotypeAllele.Alt
+  }
+
+  private def convertAllele(allele: String, isRef: Boolean = false): Seq[Allele] = {
+    if (allele == null)
+      Seq()
+    else
+      Seq(Allele.create(allele.toString, isRef))
+  }
+
+  private def convertAlleles(v: Variant): java.util.Collection[Allele] = {
+    convertAllele(v.getReferenceAllele, true) ++ convertAllele(v.getAlternateAllele)
+  }
+
+  private def convertAlleles(g: Genotype): java.util.List[Allele] = {
+    var alleles = g.getAlleles
+    if (alleles == null) return Collections.emptyList[Allele]
+    else g.getAlleles.map {
+      case GenotypeAllele.NoCall                        => Allele.NO_CALL
+      case GenotypeAllele.Ref | GenotypeAllele.OtherAlt => Allele.create(g.getVariant.getReferenceAllele.toString, true)
+      case GenotypeAllele.Alt                           => Allele.create(g.getVariant.getAlternateAllele.toString)
+    }
+  }
+
+}
+
+/**
+ * This class converts VCF data to and from ADAM. This translation occurs at the abstraction level
+ * of the GATK VariantContext which represents VCF data, and at the ADAMVariantContext level, which
+ * aggregates ADAM variant/genotype/annotation data together.
+ *
+ * If an annotation has a corresponding set of fields in the VCF standard, a conversion to/from the
+ * GATK VariantContext should be implemented in this class.
+ */
+class VariantContextConverter(dict: Option[SequenceDictionary] = None) extends Serializable with Logging {
+  import VariantContextConverter._
+
+  // Mappings between the CHROM names typically used and the more specific RefSeq accessions
+  private lazy val contigToRefSeq: Map[String, String] = dict match {
+    case Some(d) => d.records.filter(_.refseq.isDefined).map(r => r.name -> r.refseq.get).toMap
+    case _       => Map.empty
+  }
+
+  private lazy val refSeqToContig: Map[String, String] = dict match {
+    case Some(d) => d.records.filter(_.refseq.isDefined).map(r => r.refseq.get -> r.name).toMap
+    case _       => Map.empty
+  }
+  /**
+   * Converts a single GATK variant into ADAMVariantContext(s).
+   *
+   * @param vc GATK Variant context to convert.
+   * @return ADAM variant contexts
+   */
+  def convert(vc: BroadVariantContext): Seq[ADAMVariantContext] = {
+
+    // INFO field variant calling annotations, e.g. MQ
+    lazy val calling_annotations: VariantCallingAnnotations = extractVariantCallingAnnotations(vc)
+
+    vc.getAlternateAlleles.toList match {
+      case List(NON_REF_ALLELE) => {
+        val variant = createADAMVariant(vc, None /* No alternate allele */ )
+        val genotypes = extractReferenceGenotypes(vc, variant, calling_annotations)
+        return Seq(ADAMVariantContext(variant, genotypes, None))
+      }
+      case List(allele) => {
+        assert(allele.isNonReference,
+          "Assertion failed when converting: " + vc.toString)
+        val variant = createADAMVariant(vc, Some(allele.getDisplayString))
+        val genotypes = extractReferenceModelGenotypes(vc, variant, calling_annotations)
+        return Seq(ADAMVariantContext(variant, genotypes, None))
+      }
+      case List(allele, NON_REF_ALLELE) => {
+        assert(allele.isNonReference,
+          "Assertion failed when converting: " + vc.toString)
+        val variant = createADAMVariant(vc, Some(allele.getDisplayString))
+        val genotypes = extractReferenceModelGenotypes(vc, variant, calling_annotations)
+        return Seq(ADAMVariantContext(variant, genotypes, None))
+      }
+      case alleles :+ NON_REF_ALLELE => {
+        throw new IllegalArgumentException("Multi-allelic site with non-ref symbolic allele" +
+          vc.toString)
+      }
+      case _ => {
+        // Default case is multi-allelic without reference model
+        val vcb = new VariantContextBuilder(vc)
+        return vc.getAlternateAlleles.flatMap(allele => {
+          val idx = vc.getAlleleIndex(allele)
+          assert(idx >= 1, "Unexpected index for alternate allele: " + vc.toString)
+          vcb.alleles(List(vc.getReference, allele, NON_REF_ALLELE))
+
+          def punchOutGenotype(g: htsjdk.variant.variantcontext.Genotype, idx: Int): htsjdk.variant.variantcontext.Genotype = {
+
+            val gb = new htsjdk.variant.variantcontext.GenotypeBuilder(g)
+            // TODO: Multi-allelic genotypes are locally phased, add phase set
+            gb.phased(true)
+
+            if (g.hasAD) {
+              val ad = g.getAD
+              gb.AD(Array(ad(0), ad(idx)))
+            }
+
+            // Recompute PLs as needed to reflect stripped alleles.
+            // TODO: Collapse other alternate alleles into a single set of probabilities.
+            if (g.hasPL) {
+              val oldPLs = g.getPL
+              val maxIdx = oldPLs.length
+              val newPLs = GenotypeLikelihoods.getPLIndecesOfAlleles(0, idx).filter(_ < maxIdx).map(oldPLs(_))
+              // Normalize new likelihoods in log-space
+              gb.PL(newPLs.map(_ - newPLs.min))
+            }
+            gb.make
+          }
+
+          // We purposely retain "invalid" genotype alleles, that will eventually become
+          // "OtherAlt" entries, but won't validate against the reduced VariantContext
+          val gc = GenotypesContext.create // Fixup genotypes
+          gc.addAll(vc.getGenotypes.map(punchOutGenotype(_, idx)))
+          vcb.genotypesNoValidation(gc)
+
+          // Recursively convert now bi-allelic VariantContexts, setting any multi-allelic
+          // specific fields afterwards
+          val adamVCs = convert(vcb.make)
+          adamVCs.flatMap(_.genotypes).foreach(g => g.put(splitFromMultiAllelicField.pos, true))
+          adamVCs
+        })
+
+      }
+    }
+
+    /*
+    val annotation: Option[DatabaseVariantAnnotation] =
+      if (extractExternalAnnotations)
+        Some(extractVariantDatabaseAnnotation(variant, vc))
+      else
+        None
+     */
+  }
+
+  def convertToAnnotation(vc: BroadVariantContext): DatabaseVariantAnnotation = {
+    val variant = vc.getAlternateAlleles.toList match {
+      case List(NON_REF_ALLELE) => {
+        createADAMVariant(vc, None /* No alternate allele */ )
+      }
+      case List(allele) => {
+        assert(allele.isNonReference,
+          "Assertion failed when converting: " + vc.toString)
+        createADAMVariant(vc, Some(allele.getDisplayString))
+      }
+      case List(allele, NON_REF_ALLELE) => {
+        assert(allele.isNonReference,
+          "Assertion failed when converting: " + vc.toString)
+        createADAMVariant(vc, Some(allele.getDisplayString))
+      }
+      case alleles :+ NON_REF_ALLELE => {
+        throw new IllegalArgumentException("Multi-allelic site with non-ref symbolic allele " +
+          vc.toString)
+      }
+      case _ => {
+        throw new IllegalArgumentException("Multi-allelic site " + vc.toString)
+      }
+    }
+
+    extractVariantDatabaseAnnotation(variant, vc)
+  }
+
+  private def createContig(vc: BroadVariantContext): Contig = {
+    val contigName = contigToRefSeq.getOrElse(vc.getChr, vc.getChr)
+
+    Contig.newBuilder()
+      .setContigName(contigName)
+      .build()
+  }
+
+  private def createADAMVariant(vc: BroadVariantContext, alt: Option[String]): Variant = {
+    // VCF CHROM, POS, REF and ALT
+    val builder = Variant.newBuilder
+      .setContig(createContig(vc))
+      .setStart(vc.getStart - 1 /* ADAM is 0-indexed */ )
+      .setEnd(vc.getEnd /* ADAM is 0-indexed, so the 1-indexed inclusive end becomes exclusive */ )
+      .setReferenceAllele(vc.getReference.getBaseString)
+    alt.foreach(builder.setAlternateAllele(_))
+    builder.build
+  }
+
+  private def extractVariantDatabaseAnnotation(variant: Variant, vc: BroadVariantContext): DatabaseVariantAnnotation = {
+    val annotation = DatabaseVariantAnnotation.newBuilder()
+      .setVariant(variant)
+      .build
+
+    VariantAnnotationConverter.convert(vc, annotation)
+
+  }
+
+  private def extractGenotypes(
+    vc: BroadVariantContext,
+    variant: Variant,
+    annotations: VariantCallingAnnotations,
+    setPL: (htsjdk.variant.variantcontext.Genotype, Genotype.Builder) => Unit): Seq[Genotype] = {
+
+    val genotypes: Seq[Genotype] = vc.getGenotypes.map(
+      (g: htsjdk.variant.variantcontext.Genotype) => {
+        val genotype: Genotype.Builder = Genotype.newBuilder
+          .setVariant(variant)
+          .setVariantCallingAnnotations(annotations)
+          .setSampleId(g.getSampleName)
+          .setAlleles(g.getAlleles.map(VariantContextConverter.convertAllele(vc, _)))
+          .setIsPhased(g.isPhased)
+
+        if (g.hasGQ) genotype.setGenotypeQuality(g.getGQ)
+        if (g.hasDP) genotype.setReadDepth(g.getDP)
+        if (g.hasAD) {
+          val ad = g.getAD
+          genotype.setReferenceReadDepth(ad(0)).setAlternateReadDepth(ad(1))
+        }
+        setPL(g, genotype)
+
+        VariantAnnotationConverter.convert(g, genotype.build)
+      }).toSeq
+
+    genotypes
+  }
+
+  private def extractNonReferenceGenotypes(vc: BroadVariantContext, variant: Variant, annotations: VariantCallingAnnotations): Seq[Genotype] = {
+    assert(vc.isBiallelic)
+    extractGenotypes(vc, variant, annotations,
+      (g: htsjdk.variant.variantcontext.Genotype, b: Genotype.Builder) => {
+        if (g.hasPL) b.setGenotypeLikelihoods(g.getPL.toList.map(p => p: java.lang.Integer))
+      })
+  }
+
+  private def extractReferenceGenotypes(vc: BroadVariantContext, variant: Variant, annotations: VariantCallingAnnotations): Seq[Genotype] = {
+    assert(vc.isBiallelic)
+    extractGenotypes(vc, variant, annotations, (g, b) => {
+      if (g.hasPL) b.setNonReferenceLikelihoods(g.getPL.toList.map(p => p: java.lang.Integer))
+    })
+  }
+
+  private def extractReferenceModelGenotypes(vc: BroadVariantContext, variant: Variant, annotations: VariantCallingAnnotations): Seq[Genotype] = {
+    extractGenotypes(vc, variant, annotations, (g, b) => {
+      if (g.hasPL) {
+        val pls = g.getPL.map(p => p: java.lang.Integer)
+        val splitAt: Int = g.getPloidy match {
+          case 1 => 2
+          case 2 => 3
+          case _ => assert(false, "Ploidy > 2 not supported for this operation"); 0
+        }
+        b.setGenotypeLikelihoods(pls.slice(0, splitAt).toList)
+        b.setNonReferenceLikelihoods(pls.slice(splitAt, pls.length).toList)
+      }
+    })
+  }
+
+  private def extractVariantCallingAnnotations(vc: BroadVariantContext): VariantCallingAnnotations = {
+    val call: VariantCallingAnnotations.Builder = VariantCallingAnnotations.newBuilder
+
+    // VCF QUAL, FILTER and INFO fields
+    if (vc.hasLog10PError) {
+      call.setVariantCallErrorProbability(vc.getPhredScaledQual.asInstanceOf[Float])
+    }
+
+    if (vc.filtersWereApplied && vc.isFiltered) {
+      call.setVariantIsPassing(false).setVariantFilters(new java.util.ArrayList(vc.getFilters))
+    } else if (vc.filtersWereApplied) {
+      call.setVariantIsPassing(true)
+    }
+
+    VariantAnnotationConverter.convert(vc, call.build())
+  }
+
+  /**
+   * Convert an ADAMVariantContext into the equivalent GATK VariantContext
+   * @param vc
+   * @return GATK VariantContext
+   */
+  def convert(vc: ADAMVariantContext): BroadVariantContext = {
+    val variant: Variant = vc.variant
+    val vcb = new VariantContextBuilder()
+      .chr(refSeqToContig.getOrElse(variant.getContig.getContigName.toString,
+        variant.getContig.getContigName.toString))
+      .start(variant.getStart + 1 /* Recall ADAM is 0-indexed */ )
+      .stop(variant.getStart + variant.getReferenceAllele.length)
+      .alleles(VariantContextConverter.convertAlleles(variant))
+
+    vc.databases.flatMap(d => Option(d.getDbSnpId)).foreach(d => vcb.id("rs" + d))
+
+    // TODO: Extract provenance INFO fields
+    try {
+      vcb.genotypes(vc.genotypes.map(g => {
+        val gb = new htsjdk.variant.variantcontext.GenotypeBuilder(
+          g.getSampleId.toString, VariantContextConverter.convertAlleles(g))
+
+        Option(g.getIsPhased).foreach(gb.phased(_))
+        Option(g.getGenotypeQuality).foreach(gb.GQ(_))
+        Option(g.getReadDepth).foreach(gb.DP(_))
+
+        if (g.getReferenceReadDepth != null && g.getAlternateReadDepth != null)
+          gb.AD(Array(g.getReferenceReadDepth, g.getAlternateReadDepth))
+
+        if (g.getVariantCallingAnnotations != null) {
+          val callAnnotations = g.getVariantCallingAnnotations()
+          if (callAnnotations.getVariantFilters != null)
+            gb.filters(callAnnotations.getVariantFilters.map(_.toString))
+        }
+
+        if (g.getGenotypeLikelihoods != null && !g.getGenotypeLikelihoods.isEmpty)
+          gb.PL(g.getGenotypeLikelihoods.map(p => p: Int).toArray)
+
+        gb.make
+      }))
+
+      vcb.make
+    } catch {
+      case t: Throwable => {
+        log.error("Encountered error when converting variant context with variant: \n" +
+          vc.variant.variant + "\n" +
+          "and genotypes: \n" +
+          vc.genotypes.mkString("\n"))
+        throw t
+      }
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/instrumentation/Timers.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/instrumentation/Timers.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/instrumentation/Timers.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/instrumentation/Timers.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,81 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.instrumentation
+
+import org.bdgenomics.utils.instrumentation.Metrics
+
+/**
+ * Contains [[Timers]] that are used to instrument ADAM.
+ */
+object Timers extends Metrics {
+
+  // File Loading
+  val LoadAlignmentRecords = timer("Load Alignment Records")
+  val BAMLoad = timer("BAM File Load")
+  val ParquetLoad = timer("Parquet File Load")
+
+  // Trim Reads
+  val TrimReadsInDriver = timer("Trim Reads")
+  val TrimRead = timer("Trim Reads")
+  val TrimCigar = timer("Trim Cigar")
+  val TrimMDTag = timer("Trim MD Tag")
+
+  // Trim Low Quality Read Groups
+  val TrimLowQualityInDriver = timer("Trim Low Quality Read Groups")
+
+  // Mark Duplicates
+  val MarkDuplicatesInDriver = timer("Mark Duplicates")
+  val CreateReferencePositionPair = timer("Create Reference Position Pair")
+  val PerformDuplicateMarking = timer("Perform Duplicate Marking")
+  val ScoreAndMarkReads = timer("Score and Mark Reads")
+  val MarkReads = timer("Mark Reads")
+
+  // Recalibrate Base Qualities
+  val BQSRInDriver = timer("Base Quality Recalibration")
+  val CreateKnownSnpsTable = timer("Create Known SNPs Table")
+  val ComputeCovariates = timer("Compute Covariates")
+  val ObservationAccumulatorComb = timer("Observation Accumulator: comb")
+  val ObservationAccumulatorSeq = timer("Observation Accumulator: seq")
+  val RecalibrateRead = timer("Recalibrate Read")
+  val ComputeQualityScore = timer("Compute Quality Score")
+  val GetExtraValues = timer("Get Extra Values")
+
+  // Realign Indels
+  val RealignIndelsInDriver = timer("Realign Indels")
+  val FindTargets = timer("Find Targets")
+  val CreateIndelRealignmentTargets = timer("Create Indel Realignment Targets for Read")
+  val SortTargets = timer("Sort Targets")
+  val JoinTargets = timer("Join Targets")
+  val MapTargets = timer("Map Targets")
+  val RealignTargetGroup = timer("Realign Target Group")
+  val GetReferenceFromReads = timer("Get Reference From Reads")
+  val SweepReadOverReferenceForQuality = timer("Sweep Read Over Reference For Quality")
+
+  // Sort Reads
+  val SortReads = timer("Sort Reads")
+
+  // File Saving
+  val SAMSave = timer("SAM Save")
+  val ConvertToSAM = timer("Convert To SAM")
+  val ConvertToSAMRecord = timer("Convert To SAM Record")
+  val SaveAsADAM = timer("Save File In ADAM Format")
+  val WriteADAMRecord = timer("Write ADAM Record")
+  val WriteBAMRecord = timer("Write BAM Record")
+  val WriteSAMRecord = timer("Write SAM Record")
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ActiveRegion.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ActiveRegion.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ActiveRegion.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ActiveRegion.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,13 @@
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * Created by lhy on 9/1/15.
+ */
+object ActiveRegion {
+}
+
+case class ActiveRegion(activeRegionLoc: ReferenceRegion, reads: Seq[AlignmentRecord]) {
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ActivityProfileState.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ActivityProfileState.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ActivityProfileState.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ActivityProfileState.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,14 @@
+package org.bdgenomics.adam.models
+
+/**
+ * Created by lhy on 10/20/15.
+ */
+object ActivityProfileState {
+  val NONE = 0
+  val HIGH_QUALITY_SOFT_CLIPS = 1
+}
+
+//resultState:1 to NONE, 2 to HIGH_QUALITY_SOFT_CLIPS
+case class ActivityProfileState(isActiveProb:Double, resultState:Int, resultValue:Double) {
+  override def toString = "ActivityProfileState{ isActiveProb=" + isActiveProb + ", resultState=" + resultState + ", resultValue=" + resultValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Alphabet.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Alphabet.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Alphabet.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Alphabet.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,107 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import scala.util.Try
+
+/**
+ * Created by bryan on 4/17/15.
+ *
+ * An alphabet of symbols and related operations
+ *
+ */
+trait Alphabet {
+
+  /** the symbols in this alphabet */
+  val symbols: Seq[Symbol]
+
+  /**
+   * flag if symbols are case-sensitive.
+   * if true, this alphabet will treat symbols representing upper and lower case symbols as distinct
+   * if false, this alphabet will treat upper or lower case chars as the same for its symbols
+   */
+  val caseSensitive: Boolean
+
+  /** map of the symbol char to the symbol */
+  lazy val symbolMap: Map[Char, Symbol] =
+    if (caseSensitive)
+      symbols.map(symbol => symbol.label -> symbol).toMap
+    else
+      symbols.flatMap(symbol => Seq(symbol.label.toLower -> symbol, symbol.label.toUpper -> symbol)).toMap
+
+  /**
+   *
+   * @param s Each char in this string represents a symbol on the alphabet.
+   *          If the char is not in the alphabet then a NoSuchElementException is thrown
+   * @return the reversed complement of the given string.
+   * @throws IllegalArgumentException if the string contains a symbol which is not in the alphabet
+   */
+  def reverseComplementExact(s: String): String = {
+    reverseComplement(s,
+      (symbol: Char) => throw new IllegalArgumentException("Character %s not found in alphabet.".format(symbol))
+    )
+  }
+
+  /**
+   *
+   * @param s Each char in this string represents a symbol on the alphabet.
+   * @param notFound If the char is not in the alphabet then this function is called.
+   *                 default behavior is to return a new Symbol representing the unknown character,
+   *                 so that the unknown char is treated as the complement
+   * @return the reversed complement of the given string.
+   */
+  def reverseComplement(s: String, notFound: (Char => Symbol) = ((c: Char) => Symbol(c, c))) = {
+    s.map(x => Try(apply(x)).getOrElse(notFound(x)).complement).reverse
+  }
+
+  /** number of symbols in the alphabet */
+  def size = symbols.size
+
+  /**
+   * @param c char to lookup as a symbol in this alphabet
+   * @return the given symbol
+   */
+  def apply(c: Char): Symbol = symbolMap(c)
+
+}
+
+/**
+ * A symbol in an alphabet
+ * @param label a character which represents the symbol
+ * @param complement acharacter which represents the complement of the symbol
+ */
+case class Symbol(label: Char, complement: Char)
+
+/**
+ * The standard DNA alphabet with A,T,C, and G
+ */
+class DNAAlphabet extends Alphabet {
+
+  override val caseSensitive = false
+
+  override val symbols = Seq(
+    Symbol('A', 'T'),
+    Symbol('T', 'A'),
+    Symbol('G', 'C'),
+    Symbol('C', 'G')
+  )
+}
+
+object Alphabet {
+  val dna = new DNAAlphabet
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Attribute.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Attribute.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Attribute.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Attribute.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,50 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+/**
+ * A wrapper around the attrTuple (key) and value pair.  Includes the attrTuple-type explicitly, rather than
+ * embedding the corresponding information in the type of 'value', because otherwise it'd be difficult
+ * to extract the correct type for Byte and NumericSequence values.
+ *
+ * Roughly analogous to Picards SAMTagAndValue.
+ *
+ * @param tag The string key associated with this pair.
+ * @param tagType An enumerated value representing the type of the 'value' parameter.
+ * @param value The 'value' half of the pair.
+ */
+case class Attribute(tag: String, tagType: TagType.Value, value: Any) {
+  override def toString: String = "%s:%s:%s".format(tag, tagType, value.toString)
+}
+
+object TagType extends Enumeration {
+
+  class TypeVal(val abbreviation: String) extends Val(nextId, abbreviation) {
+    override def toString(): String = abbreviation
+  }
+  def TypeValue(abbreviation: String): Val = new TypeVal(abbreviation)
+
+  // These String values come from the SAM file format spec: http://samtools.sourceforge.net/SAMv1.pdf
+  val Character = TypeValue("A")
+  val Integer = TypeValue("i")
+  val Float = TypeValue("f")
+  val String = TypeValue("Z")
+  val ByteSequence = TypeValue("H")
+  val NumericSequence = TypeValue("B")
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Consensus.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Consensus.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Consensus.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Consensus.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,71 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import htsjdk.samtools.{ Cigar, CigarOperator }
+import org.bdgenomics.adam.util.ImplicitJavaConversions._
+
+object Consensus {
+
+  def generateAlternateConsensus(sequence: String, start: ReferencePosition, cigar: Cigar): Option[Consensus] = {
+
+    var readPos = 0
+    var referencePos = start.pos
+
+    if (cigar.getCigarElements.count(elem => elem.getOperator == CigarOperator.I || elem.getOperator == CigarOperator.D) == 1) {
+      cigar.getCigarElements.foreach(cigarElement => {
+        cigarElement.getOperator match {
+          case CigarOperator.I => return Some(new Consensus(sequence.substring(readPos, readPos + cigarElement.getLength), ReferenceRegion(start.referenceName, referencePos, referencePos + 1)))
+          case CigarOperator.D => return Some(new Consensus("", ReferenceRegion(start.referenceName, referencePos, referencePos + cigarElement.getLength + 1)))
+          case _ => {
+            if (cigarElement.getOperator.consumesReadBases && cigarElement.getOperator.consumesReferenceBases) {
+              readPos += cigarElement.getLength
+              referencePos += cigarElement.getLength
+            } else {
+              return None
+            }
+          }
+        }
+      })
+      None
+    } else {
+      None
+    }
+  }
+
+}
+
+case class Consensus(consensus: String, index: ReferenceRegion) {
+
+  def insertIntoReference(reference: String, refStart: Long, refEnd: Long): String = {
+    if (index.start < refStart || index.start > refEnd || index.end - 1 < refStart || index.end - 1 > refEnd) {
+      throw new IllegalArgumentException("Consensus and reference do not overlap: " + index + " vs. " + refStart + " to " + refEnd)
+    } else {
+      reference.substring(0, (index.start - refStart).toInt) + consensus + reference.substring((index.end - 1 - refStart).toInt)
+    }
+  }
+
+  override def toString: String = {
+    if (index.start + 1 != index.end) {
+      "Deletion over " + index.toString
+    } else {
+      "Inserted " + consensus + " at " + index.toString
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/FeatureHierarchy.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/FeatureHierarchy.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/FeatureHierarchy.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/FeatureHierarchy.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,141 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.formats.avro.{ Strand, Feature }
+
+import scala.collection.JavaConversions
+import scala.collection.JavaConversions._
+import scala.collection._
+
+object BaseFeature {
+
+  def strandChar(strand: Strand): Char =
+    strand match {
+      case Strand.Forward     => '+'
+      case Strand.Reverse     => '-'
+      case Strand.Independent => '.'
+    }
+
+  def frameChar(feature: Feature): Char = {
+    val opt: Map[String, String] = feature.getAttributes
+    opt.get("frame").map(_.charAt(0)).getOrElse('.')
+  }
+
+  def attributeString(feature: Feature): String =
+    feature.getAttributes.mkString(",")
+
+  def attrs(f: Feature): Map[String, String] =
+    JavaConversions.mapAsScalaMap(f.getAttributes)
+
+  def attributeString(feature: Feature, attrName: String): Option[String] =
+    attrs(feature).get(attrName).map(_.toString)
+
+  def attributeLong(feature: Feature, attrName: String): Option[Long] =
+    attrs(feature).get(attrName).map(_.toString).map(_.toLong)
+
+  def attributeDouble(feature: Feature, attrName: String): Option[Double] =
+    attrs(feature).get(attrName).map(_.toString).map(_.toDouble)
+
+  def attributeInt(feature: Feature, attrName: String): Option[Int] =
+    attrs(feature).get(attrName).map(_.toString).map(_.toInt)
+}
+
+class BaseFeature(val feature: Feature) extends Serializable {
+  override def toString: String = feature.toString
+}
+
+class GTFFeature(override val feature: Feature) extends BaseFeature(feature) {
+  def getSeqname: String = feature.getContig.getContigName.toString
+
+  def getSource: String = feature.getSource.toString
+
+  def getFeature: String = feature.getFeatureType.toString
+
+  def getStart: Long = feature.getStart
+
+  def getEnd: Long = feature.getEnd
+
+  def getScore: Double = feature.getValue
+
+  def getStrand: Char = BaseFeature.strandChar(feature.getStrand)
+
+  def getFrame: Char = BaseFeature.frameChar(feature)
+
+  def getAttribute: String = BaseFeature.attributeString(feature)
+}
+
+object BEDFeature {
+
+  def parseBlockSizes(attribute: Option[String]): Array[Int] =
+    attribute.getOrElse("").split(",").map(_.toInt)
+
+  def parseBlockStarts(attribute: Option[String]): Array[Int] =
+    attribute.getOrElse("").split(",").map(_.toInt)
+}
+
+class BEDFeature(override val feature: Feature) extends BaseFeature(feature) {
+  def getChrom: String = feature.getContig.getContigName.toString
+
+  def getChromStart: Long = feature.getStart
+
+  def getChromEnd: Long = feature.getEnd
+
+  def getName: String = feature.getFeatureId.toString
+
+  def getScore: Double = feature.getValue
+
+  def getStrand: Char = BaseFeature.strandChar(feature.getStrand)
+
+  def getThickStart: Option[Long] = BaseFeature.attributeLong(feature, "thickStart")
+
+  def getThickEnd: Option[Long] = BaseFeature.attributeLong(feature, "thickEnd")
+
+  def getItemRGB: Option[String] = BaseFeature.attributeString(feature, "itemRgb")
+
+  def getBlockCount: Option[Int] = BaseFeature.attributeInt(feature, "blockCount")
+
+  def getBlockSizes: Array[Int] = BEDFeature.parseBlockSizes(BaseFeature.attributeString(feature, "blockSizes"))
+
+  def getBlockStarts: Array[Int] = BEDFeature.parseBlockStarts(BaseFeature.attributeString(feature, "blockStarts"))
+}
+
+/**
+ * See: http://genome.ucsc.edu/FAQ/FAQformat.html#format12
+ */
+class NarrowPeakFeature(override val feature: Feature) extends BaseFeature(feature) {
+  def getChrom: String = feature.getContig.getContigName.toString
+
+  def getChromStart: Long = feature.getStart
+
+  def getChromEnd: Long = feature.getEnd
+
+  def getName: String = feature.getFeatureId.toString
+
+  def getScore: Double = feature.getValue
+
+  def getStrand: Char = BaseFeature.strandChar(feature.getStrand)
+
+  def getSignalValue: Option[Double] = BaseFeature.attributeDouble(feature, "signalValue")
+
+  def getPValue: Option[Double] = BaseFeature.attributeDouble(feature, "pValue")
+
+  def getQValue: Option[Double] = BaseFeature.attributeDouble(feature, "qValue")
+
+  def getPeak: Option[Long] = BaseFeature.attributeLong(feature, "peak")
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Gene.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Gene.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Gene.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Gene.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,234 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.apache.spark.SparkContext
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.formats.avro.{ Strand, Feature }
+
+/**
+ * A 'gene model' is a small, hierarchical collection of objects: Genes, Transcripts, and Exons.
+ * Each Gene contains a collection of Transcripts, and each Transcript contains a collection of
+ * Exons, and together they describe how the genome is transcribed and translated into a family
+ * of related proteins (or other RNA products that aren't translated at all).
+ *
+ * This review,
+ * Gerstein et al. "What is a gene, post-ENCODE? History and updated definition" Genome Research (2007)
+ * http://genome.cshlp.org/content/17/6/669.full
+ *
+ * is a reasonably good overview both of what the term 'gene' has meant in the past as well as
+ * where it might be headed in the future.
+ *
+ * Here, we aren't trying to answer any of these questions about "what is a gene," but rather to
+ * provide the routines necessary to _re-assemble_ hierarchical models of genes that have been
+ * flattened into features (GFF, GTF, or BED)
+ *
+ * @param id A name, presumably unique within a gene dataset, of a Gene
+ * @param names Common names for the gene, possibly shared with other genes (for historical or
+ *              ad hoc reasons)
+ * @param strand The strand of the Gene (this is from data, not derived from the Transcripts' strand(s), and
+ *               we leave open the possibility that a single Gene will have Transcripts in _both_ directions,
+ *               e.g. anti-sense transcripts)
+ * @param transcripts The Transcripts that are part of this gene model
+ */
+case class Gene(id: String, names: Seq[String], strand: Boolean, transcripts: Iterable[Transcript]) {
+
+  /**
+   * Finds the union of all the locations of the transcripts for this gene,
+   * across all the reference sequences indicates by the transcripts themselves.
+   * @return A Seq of ReferenceRegions
+   */
+  lazy val regions =
+    ReferenceUtils.unionReferenceSet(transcripts.map(_.region)).toSeq
+
+}
+
+/**
+ * A transcript model (here represented as a value of the Transcript class) is a simple,
+ * hierarchical model containing a collection of exon models as well as an associated
+ * gene identifier, transcript identifier, and a set of common names (synonyms).
+ *
+ * @param id the (unique) identifier of the Transcript
+ * @param names Common names for the transcript
+ * @param geneId The (unique) identifier of the gene to which the transcript belongs
+ * @param exons The set of exons in the transcript model; each of these contain a
+ *              reference region whose coordinates are in genomic space.
+ * @param cds the set of CDS regions (the subset of the exons that are coding) for this
+ *            transcript
+ * @param utrs
+ */
+case class Transcript(id: String,
+                      names: Seq[String],
+                      geneId: String,
+                      strand: Boolean,
+                      exons: Iterable[Exon],
+                      cds: Iterable[CDS],
+                      utrs: Iterable[UTR]) {
+
+  lazy val region = exons.map(_.region).reduceLeft[ReferenceRegion]((acc, ex) => acc.hull(ex))
+
+  /**
+   * Returns the concatenated sequence of the exons of the Transcript.
+   *
+   * @param referenceSequence The reference sequence (e.g. chromosomal sequence) with
+   *                          respect to which the Transcript's coordinates or locators
+   *                          are given
+   * @return the String representation of this Transcript's spliced mRNA sequence
+   */
+  def extractTranscribedRNASequence(referenceSequence: String): String = {
+    val minStart = exons.map(_.region.start).toSeq.sorted.head.toInt
+    // takes the max...
+
+    val maxEnd = -exons.map(-_.region.end).toSeq.sorted.head.toInt
+    if (strand)
+      referenceSequence.substring(minStart, maxEnd)
+    else
+      Alphabet.dna.reverseComplement(referenceSequence.substring(minStart, maxEnd))
+  }
+
+  /**
+   * Returns the _coding sequence_ of the Transcript -- essentially, the subset of the
+   * exon(s) after the translation-start codon and before the translation-stop codon,
+   * as annotated in the Exon and CDS objects of the transcript.
+   *
+   * @param referenceSequence The reference sequence (e.g. chromosomal sequence) with
+   *                          respect to which the Transcript's coordinates or locators
+   *                          are given
+   * @return the String representation of this Transcript's spliced mRNA sequence
+   */
+  def extractCodingSequence(referenceSequence: String): String = {
+    val builder = new StringBuilder()
+    val cdses =
+      if (strand)
+        cds.toSeq.sortBy(_.region.start)
+      else
+        cds.toSeq.sortBy(_.region.start).reverse
+
+    cdses.foreach(cds => builder.append(cds.extractSequence(referenceSequence)))
+    builder.toString()
+  }
+
+  /**
+   * Returns the _exonic_ sequence of the Transcript -- basically, the sequences of
+   * each exon concatenated together, with the intervening intronic sequences omitted.
+   *
+   * @param referenceSequence The reference sequence (e.g. chromosomal sequence) with
+   *                          respect to which the Transcript's coordinates or locators
+   *                          are given
+   * @return the String representation of this Transcript's spliced mRNA sequence
+   */
+  def extractSplicedmRNASequence(referenceSequence: String): String = {
+    val builder = new StringBuilder()
+    val exs =
+      if (strand)
+        exons.toSeq.sortBy(_.region.start)
+      else
+        exons.toSeq.sortBy(_.region.start).reverse
+
+    exs.foreach(exon => builder.append(exon.extractSequence(referenceSequence)))
+    builder.toString()
+  }
+
+}
+
+/**
+ * A trait for values (usually regions or collections of regions) that can be
+ * subsetted or extracted out of a larger region string -- for example, exons
+ * or transcripts which have a sequence defined in terms of their coordinates
+ * against a reference chromosome.  Passing the sequence of the reference
+ * chromosome to a transcript's 'extractSequence' method will return the
+ * sequence of the transcript.
+ */
+trait Extractable {
+
+  /**
+   * Returns the subset of the given referenceSequence that corresponds to this
+   * Extractable (not necessarily a contiguous substring, and possibly reverse-
+   * complemented or transformed in other ways).
+   *
+   * @param referenceSequence The reference sequence (e.g. chromosomal sequence) with
+   *                          respect to which the Extractable's coordinates or locators
+   *                          are given
+   * @return the String representation of this Extractable
+   */
+  def extractSequence(referenceSequence: String): String
+}
+
+abstract class BlockExtractable(strand: Boolean, region: ReferenceRegion)
+    extends Extractable {
+
+  override def extractSequence(referenceSequence: String): String =
+    if (strand)
+      referenceSequence.substring(region.start.toInt, region.end.toInt)
+    else
+      Alphabet.dna.reverseComplement(referenceSequence.substring(region.start.toInt, region.end.toInt))
+}
+
+/**
+ * An exon model (here represented as a value of the Exon class) is a representation of a
+ * single exon from a transcript in genomic coordinates.
+ *
+ * NOTE: we're not handling shared exons here
+ *
+ * @param transcriptId the (unique) identifier of the transcript to which the exon belongs
+ * @param region The region (in genomic coordinates) to which the exon maps
+ */
+case class Exon(exonId: String, transcriptId: String, strand: Boolean, region: ReferenceRegion)
+    extends BlockExtractable(strand, region) {
+}
+
+/**
+ * Coding Sequence annotations, should be a subset of an Exon for a particular Transcript
+ * @param transcriptId
+ * @param strand
+ * @param region
+ */
+case class CDS(transcriptId: String, strand: Boolean, region: ReferenceRegion)
+    extends BlockExtractable(strand, region) {
+}
+
+/**
+ * UnTranslated Regions
+ *
+ * @param transcriptId
+ * @param strand
+ * @param region
+ */
+case class UTR(transcriptId: String, strand: Boolean, region: ReferenceRegion)
+    extends BlockExtractable(strand, region) {
+}
+
+object ReferenceUtils {
+
+  def unionReferenceSet(refs: Iterable[ReferenceRegion]): Iterable[ReferenceRegion] = {
+
+    def folder(acc: Seq[ReferenceRegion], tref: ReferenceRegion): Seq[ReferenceRegion] =
+      acc match {
+        case Seq() => Seq(tref)
+        case (first: ReferenceRegion) +: rest =>
+          if (first.overlaps(tref))
+            first.hull(tref) +: rest
+          else
+            tref +: first +: rest
+      }
+
+    refs.toSeq.sorted.foldLeft(Seq[ReferenceRegion]())(folder)
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/IndelTable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/IndelTable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/IndelTable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/IndelTable.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,95 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.apache.spark.{ Logging, SparkContext }
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.formats.avro.Variant
+
+class IndelTable(private val table: Map[String, Iterable[Consensus]]) extends Serializable with Logging {
+  log.info("Indel table has %s contigs and %s entries".format(table.size,
+    table.values.map(_.size).sum))
+
+  /**
+   * Returns all known indels within the given reference region. If none are known, returns an empty Seq.
+   *
+   * @param region Region to look for known indels.
+   * @return Returns a sequence of consensuses.
+   */
+  def getIndelsInRegion(region: ReferenceRegion): Seq[Consensus] = {
+    if (table.contains(region.referenceName)) {
+      val bucket = table(region.referenceName)
+
+      bucket.filter(_.index.overlaps(region)).toSeq
+    } else {
+      Seq()
+    }
+  }
+}
+
+object IndelTable {
+
+  /**
+   * Creates an indel table from a file containing known indels.
+   *
+   * @param knownIndelsFile Path to file with known indels.
+   * @param sc SparkContext to use for loading.
+   * @return Returns a table with the known indels populated.
+   */
+  def apply(knownIndelsFile: String, sc: SparkContext): IndelTable = {
+    val files = knownIndelsFile.split(',')
+    var rdd: RDD[Variant] = sc.loadVariants(files(0))
+    for(i<-1 until files.length) {
+      rdd ++= sc.loadVariants(files(i))
+    }
+    //val rdd: RDD[Variant] = sc.loadVariants(knownIndelsFile)
+    apply(rdd)
+  }
+
+  /**
+   * Creates an indel table from an RDD containing known variants.
+   *
+   * @param variants RDD of variants.
+   * @return Returns a table with known indels populated.
+   */
+  def apply(variants: RDD[Variant]): IndelTable = {
+    val consensus: Map[String, Iterable[Consensus]] = variants.filter(v => v.getReferenceAllele.length != v.getAlternateAllele.length)
+      .map(v => {
+        val referenceName = v.getContig.getContigName.toString
+        val consensus = if (v.getReferenceAllele.length > v.getAlternateAllele.length) {
+          // deletion
+          val deletionLength = v.getReferenceAllele.length - v.getAlternateAllele.length
+          val start = v.getStart + v.getAlternateAllele.length
+
+          Consensus("", ReferenceRegion(referenceName, start, start + deletionLength))
+        } else {
+          val start = v.getStart + v.getReferenceAllele.length
+
+          Consensus(v.getAlternateAllele.toString.drop(v.getReferenceAllele.length), ReferenceRegion(referenceName, start, start + 1))
+        }
+
+        (referenceName, consensus)
+      }).groupByKey()
+      .collect()
+      .toMap
+
+    new IndelTable(consensus)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Interval.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Interval.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/Interval.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/Interval.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,34 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+/**
+ * An interval is a region on a coordinate space that has a defined width. This
+ * can be used to express a region of a genome, a transcript, a gene, etc.
+ */
+trait Interval {
+
+  /**
+   * A width is the key property of an interval, which can represent a genomic
+   * region, a transcript, a gene, etc.
+   *
+   * @return The width of this interval.
+   */
+  def width: Long
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/MyReferencePositionPair.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/MyReferencePositionPair.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/MyReferencePositionPair.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/MyReferencePositionPair.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,129 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.apache.spark.Logging
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+object MyReferencePositionPair extends Logging {
+
+  def apply(reads: Iterable[AlignmentRecord]): MyReferencePositionPair = {
+    def getNum(s: String): Int = {
+      s match {
+        case "1" => 1
+        case "2" => 2
+        case "3" => 3
+        case "4" => 4
+        case "5" => 5
+        case "6" => 6
+        case "7" => 7
+        case "8" => 8
+        case "9" => 9
+        case "10" => 10
+        case "11" => 11
+        case "12" => 12
+        case "13" => 13
+        case "14" => 14
+        case "15" => 15
+        case "16" => 16
+        case "17" => 17
+        case "18" => 18
+        case "19" => 19
+        case "20" => 20
+        case "21" => 21
+        case "22" => 22
+        case "X" => 23
+        case "Y" => 24
+        case "MT" => 25
+        case "GL000207.1" => 26
+        case "GL000226.1"	=> 27
+        case "GL000229.1" => 28
+        case "GL000231.1" => 29
+        case "GL000210.1" => 30
+        case "GL000239.1" => 31
+        case "GL000235.1" => 32
+        case "GL000201.1" => 33
+        case "GL000247.1" => 34
+        case "GL000245.1" => 35
+        case "GL000197.1" => 36
+        case "GL000203.1" => 37
+        case "GL000246.1" => 38
+        case "GL000249.1" => 39
+        case "GL000196.1" => 40
+        case "GL000248.1" => 41
+        case "GL000244.1" => 42
+        case "GL000238.1" => 43
+        case "GL000202.1" => 44
+        case "GL000234.1" => 45
+        case "GL000232.1" => 46
+        case "GL000206.1" => 47
+        case "GL000240.1" => 48
+        case "GL000236.1" => 49
+        case "GL000241.1" => 50
+        case "GL000243.1" => 51
+        case "GL000242.1" => 52
+        case "GL000230.1" => 53
+        case "GL000237.1" => 54
+        case "GL000233.1" => 55
+        case "GL000204.1" => 56
+        case "GL000198.1" => 57
+        case "GL000208.1" => 58
+        case "GL000191.1" => 59
+        case "GL000227.1" => 60
+        case "GL000228.1" => 61
+        case "GL000214.1" => 62
+        case "GL000221.1" => 63
+        case "GL000209.1" => 64
+        case "GL000218.1" => 65
+        case "GL000220.1" => 66
+        case "GL000213.1" => 67
+        case "GL000211.1" => 68
+        case "GL000199.1" => 69
+        case "GL000217.1" => 70
+        case "GL000216.1" => 71
+        case "GL000215.1" => 72
+        case "GL000205.1" => 73
+        case "GL000219.1" => 74
+        case "GL000224.1" => 75
+        case "GL000223.1" => 76
+        case "GL000195.1" => 77
+        case "GL000212.1" => 78
+        case "GL000222.1" => 79
+        case "GL000200.1" => 80
+        case "GL000193.1" => 81
+        case "GL000194.1" => 82
+        case "GL000225.1" => 83
+        case "GL000192.1" => 84
+        case _ => 100
+      }
+    }
+    val read1pos = MyReferencePosition(reads.head)
+    val read2pos = MyReferencePosition(reads.last)
+    if(getNum(read1pos.referenceName) < getNum(read2pos.referenceName))
+      new MyReferencePositionPair(read1pos, read2pos)
+    else if(getNum(read1pos.referenceName) > getNum(read2pos.referenceName))
+      new MyReferencePositionPair(read2pos, read1pos)
+    else if(read1pos.pos < read2pos.pos)
+      new MyReferencePositionPair(read1pos, read2pos)
+    else
+      new MyReferencePositionPair(read2pos, read1pos)
+  }
+}
+
+case class MyReferencePositionPair(read1refPos: MyReferencePosition, read2refPos: MyReferencePosition)
\ No newline at end of file
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/MyReferencePosition.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/MyReferencePosition.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/MyReferencePosition.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/MyReferencePosition.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,16 @@
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * Created by lhy on 2/26/16.
+ */
+object MyReferencePosition extends Serializable {
+  def apply(record: AlignmentRecord): MyReferencePosition = {
+    val pos = new RichAlignmentRecord(record).fivePrimePosition
+    new MyReferencePosition(record.getContig.getContigName, pos, record.getReadNegativeStrand)
+  }
+}
+
+case class MyReferencePosition(referenceName: String, pos: Long, neg: Boolean)
\ No newline at end of file
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ProgramRecord.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ProgramRecord.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ProgramRecord.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ProgramRecord.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,57 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import htsjdk.samtools.SAMProgramRecord
+import org.bdgenomics.adam.rdd.ADAMContext._
+
+object ProgramRecord {
+
+  def apply(pr: SAMProgramRecord): ProgramRecord = {
+    // ID is a required field
+    val id: String = pr.getId
+
+    // these fields are optional and can be left null, so must check for null...
+    val commandLine: Option[String] = Option(pr.getCommandLine).map(_.toString)
+    val name: Option[String] = Option(pr.getProgramName).map(_.toString)
+    val version: Option[String] = Option(pr.getProgramVersion).map(_.toString)
+    val previousID: Option[String] = Option(pr.getPreviousProgramGroupId).map(_.toString)
+
+    new ProgramRecord(id, commandLine, name, version, previousID)
+  }
+}
+
+case class ProgramRecord(id: String,
+                         commandLine: Option[String],
+                         name: Option[String],
+                         version: Option[String],
+                         previousID: Option[String]) {
+
+  def toSAMProgramRecord(): SAMProgramRecord = {
+    val pr = new SAMProgramRecord(id)
+
+    // set optional fields
+    commandLine.foreach(cl => pr.setCommandLine(cl))
+    name.foreach(n => pr.setProgramName(n))
+    version.foreach(v => pr.setProgramVersion(v))
+    previousID.foreach(id => pr.setPreviousProgramGroupId(id))
+
+    pr
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReadBucket.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReadBucket.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReadBucket.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReadBucket.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,111 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import org.bdgenomics.adam.serialization.AvroSerializer
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * This class is similar to SingleReadBucket, except it breaks the reads down further.
+ *
+ * Rather than stopping at primary/secondary/unmapped, this will break it down further into whether they are paired
+ * or unpaired, and then whether they are the first or second of the pair.
+ *
+ * This is useful as this will usually map a single read in any of the sequences.
+ */
+case class ReadBucket(unpairedPrimaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      pairedFirstPrimaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      pairedSecondPrimaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      unpairedSecondaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      pairedFirstSecondaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      pairedSecondSecondaryMappedReads: Iterable[AlignmentRecord] = Seq.empty,
+                      unmappedReads: Iterable[AlignmentRecord] = Seq.empty) {
+  def allReads(): Iterable[AlignmentRecord] =
+    unpairedPrimaryMappedReads ++
+      pairedFirstPrimaryMappedReads ++
+      pairedSecondPrimaryMappedReads ++
+      unpairedSecondaryMappedReads ++
+      pairedFirstSecondaryMappedReads ++
+      pairedSecondSecondaryMappedReads ++
+      unmappedReads
+}
+
+class ReadBucketSerializer extends Serializer[ReadBucket] {
+  val recordSerializer = new AvroSerializer[AlignmentRecord]()
+
+  def writeArray(kryo: Kryo, output: Output, reads: Iterable[AlignmentRecord]): Unit = {
+    output.writeInt(reads.size, true)
+    for (read <- reads) {
+      recordSerializer.write(kryo, output, read)
+    }
+  }
+
+  def readArray(kryo: Kryo, input: Input): Seq[AlignmentRecord] = {
+    val numReads = input.readInt(true)
+    (0 until numReads).foldLeft(List[AlignmentRecord]()) {
+      (a, b) => recordSerializer.read(kryo, input, classOf[AlignmentRecord]) :: a
+    }
+  }
+
+  def write(kryo: Kryo, output: Output, bucket: ReadBucket) = {
+    writeArray(kryo, output, bucket.unpairedPrimaryMappedReads)
+    writeArray(kryo, output, bucket.pairedFirstPrimaryMappedReads)
+    writeArray(kryo, output, bucket.pairedSecondPrimaryMappedReads)
+    writeArray(kryo, output, bucket.unpairedSecondaryMappedReads)
+    writeArray(kryo, output, bucket.pairedFirstSecondaryMappedReads)
+    writeArray(kryo, output, bucket.pairedSecondSecondaryMappedReads)
+    writeArray(kryo, output, bucket.unmappedReads)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[ReadBucket]): ReadBucket = {
+    val unpairedPrimaryReads = readArray(kryo, input)
+    val pairedFirstPrimaryMappedReads = readArray(kryo, input)
+    val pairedSecondPrimaryMappedReads = readArray(kryo, input)
+    val unpairedSecondaryReads = readArray(kryo, input)
+    val pairedFirstSecondaryMappedReads = readArray(kryo, input)
+    val pairedSecondSecondaryMappedReads = readArray(kryo, input)
+    val unmappedReads = readArray(kryo, input)
+    new ReadBucket(
+      unpairedPrimaryReads,
+      pairedFirstPrimaryMappedReads,
+      pairedSecondPrimaryMappedReads,
+      unpairedSecondaryReads,
+      pairedFirstSecondaryMappedReads,
+      pairedSecondSecondaryMappedReads,
+      unmappedReads)
+  }
+}
+
+object ReadBucket {
+  implicit def singleReadBucketToReadBucket(bucket: SingleReadBucket): ReadBucket = {
+    val (pairedPrimary, unpairedPrimary) = bucket.primaryMapped.partition(_.getReadPaired)
+    val (pairedFirstPrimary, pairedSecondPrimary) = pairedPrimary.partition(_.getFirstOfPair)
+    val (pairedSecondary, unpairedSecondary) = bucket.secondaryMapped.partition(_.getReadPaired)
+    val (pairedFirstSecondary, pairedSecondSecondary) = pairedSecondary.partition(_.getFirstOfPair)
+
+    new ReadBucket(unpairedPrimary,
+      pairedFirstPrimary,
+      pairedSecondPrimary,
+      unpairedSecondary,
+      pairedFirstSecondary,
+      pairedSecondSecondary,
+      bucket.unmapped)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/RecordGroupDictionary.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/RecordGroupDictionary.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/RecordGroupDictionary.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/RecordGroupDictionary.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,215 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import java.util.Date
+import htsjdk.samtools.{ SAMFileHeader, SAMFileReader, SAMReadGroupRecord }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+object RecordGroupDictionary {
+
+  /**
+   * Builds a record group dictionary from a SAM file header.
+   *
+   * @param header SAM file header with attached read groups.
+   * @return Returns a new record group dictionary with the read groups attached to the file header.
+   */
+  def fromSAMHeader(header: SAMFileHeader): RecordGroupDictionary = {
+    // force implicit conversion
+    val readGroups: List[SAMReadGroupRecord] = header.getReadGroups
+    new RecordGroupDictionary(readGroups.map(RecordGroup(_)))
+  }
+
+  /**
+   * Builds a record group dictionary from a SAM file reader by collecting the header, and the
+   * read groups attached to the header.
+   *
+   * @param samReader SAM file header with attached read groups.
+   * @return Returns a new record group dictionary with the read groups attached to the file header.
+   */
+  def fromSAMReader(samReader: SAMFileReader): RecordGroupDictionary = {
+    fromSAMHeader(samReader.getFileHeader)
+  }
+
+}
+
+/**
+ * Builds a dictionary containing record groups. Record groups must have a unique name across all
+ * samples in the dictionary. This dictionary provides numerical IDs for each group; these IDs
+ * are only consistent when referencing a single dictionary.
+ *
+ * @param recordGroups A seq of record groups to popualate the dictionary.
+ *
+ * @throws AssertionError Throws an assertion error if there are multiple record groups with the
+ * same name.
+ */
+class RecordGroupDictionary(val recordGroups: Seq[RecordGroup]) extends Serializable {
+  lazy val recordGroupMap = recordGroups.map(v => (v.recordGroupName, v))
+    .sortBy(kv => kv._1)
+    .zipWithIndex
+    .map(kv => {
+      val ((name, group), index) = kv
+      (name, (group, index))
+    }).toMap
+
+  assert(recordGroupMap.size == recordGroups.length,
+    "Read group dictionary contains multiple samples with identical read group names.")
+
+  def ++(that: RecordGroupDictionary): RecordGroupDictionary = {
+    new RecordGroupDictionary(recordGroups ++ that.recordGroups)
+  }
+
+  /**
+   * Returns the numerical index for a given record group name.
+   *
+   * @note This index is only guaranteed to have the same value for a given record group name
+   * when the index is pulled from the same record group dictionary.
+   *
+   * @param recordGroupName The record group to find an index for.
+   * @return Returns a numerical index for a record group.
+   */
+  def getIndex(recordGroupName: String): Int = {
+    recordGroupMap(recordGroupName)._2
+  }
+
+  /**
+   * Returns the record group entry for an associated name.
+   *
+   * @param recordGroupName The record group to look up.
+   * @return Detailed info for a record group.
+   */
+  def apply(recordGroupName: String): RecordGroup = {
+    recordGroupMap(recordGroupName)._1
+  }
+}
+
+object RecordGroup {
+  /**
+   * Creates a record group from read data. Returns a None option if the record group information
+   * is not populated in the read.
+   *
+   * @param read Read to populate data from.
+   * @return Returns a filled Option if record group data is attached to the read, else returns None.
+   */
+  def apply(read: AlignmentRecord): Option[RecordGroup] = {
+    for {
+      rgs <- Option(read.getRecordGroupSample)
+      rgn <- Option(read.getRecordGroupName)
+    } yield new RecordGroup(
+      rgs.toString,
+      rgn.toString,
+      Option(read.getRecordGroupSequencingCenter).map(_.toString),
+      Option(read.getRecordGroupDescription).map(_.toString),
+      Option(read.getRecordGroupRunDateEpoch).map(_.toLong),
+      Option(read.getRecordGroupFlowOrder).map(_.toString),
+      Option(read.getRecordGroupKeySequence).map(_.toString),
+      Option(read.getRecordGroupLibrary).map(_.toString),
+      Option(read.getRecordGroupPredictedMedianInsertSize).map(_.toInt),
+      Option(read.getRecordGroupPlatform).map(_.toString),
+      Option(read.getRecordGroupPlatformUnit).map(_.toString)
+    )
+  }
+
+  /**
+   * Converts a SAMReadGroupRecord into a RecordGroup.
+   *
+   * @param samRGR SAMReadGroupRecord to convert.
+   * @return Returns an equivalent ADAM format record group.
+   */
+  def apply(samRGR: SAMReadGroupRecord): RecordGroup = {
+    assert(samRGR.getSample != null,
+      "Sample ID is not set for read group " + samRGR.getReadGroupId)
+    new RecordGroup(samRGR.getSample,
+      samRGR.getReadGroupId,
+      Option(samRGR.getSequencingCenter).map(_.toString),
+      Option(samRGR.getDescription).map(_.toString),
+      Option(samRGR.getRunDate).map(_.getTime),
+      Option(samRGR.getFlowOrder).map(_.toString),
+      Option(samRGR.getKeySequence).map(_.toString),
+      Option(samRGR.getLibrary).map(_.toString),
+      Option({
+        // must explicitly reference as a java.lang.integer to avoid implicit conversion
+        val i: java.lang.Integer = samRGR.getPredictedMedianInsertSize
+        i
+      }).map(_.toInt),
+      Option(samRGR.getPlatform).map(_.toString),
+      Option(samRGR.getPlatformUnit).map(_.toString))
+  }
+}
+
+class RecordGroup(val sample: String,
+                  val recordGroupName: String,
+                  val sequencingCenter: Option[String] = None,
+                  val description: Option[String] = None,
+                  val runDateEpoch: Option[Long] = None,
+                  val flowOrder: Option[String] = None,
+                  val keySequence: Option[String] = None,
+                  val library: Option[String] = None,
+                  val predictedMedianInsertSize: Option[Int] = None,
+                  val platform: Option[String] = None,
+                  val platformUnit: Option[String] = None) extends Serializable {
+
+  /**
+   * Compares equality to another object. Only checks equality via the sample and
+   * recordGroupName fields.
+   *
+   * @param o Object to compare against.
+   * @return Returns true if the object is a RecordGroup, and the sample and group name are
+   * equal. Else, returns false.
+   */
+  override def equals(o: Any): Boolean = o match {
+    case rg: RecordGroup => rg.sample == sample && rg.recordGroupName == recordGroupName
+    case _               => false
+  }
+
+  /**
+   * Generates a hash from the sample and record group name fields.
+   *
+   * @return Hash code for this object.
+   */
+  override def hashCode(): Int = (sample + recordGroupName).hashCode()
+
+  /**
+   * Converts a record group into a SAM formatted record group.
+   *
+   * @return A SAM formatted record group.
+   */
+  def toSAMReadGroupRecord(): SAMReadGroupRecord = {
+    val rgr = new SAMReadGroupRecord(recordGroupName)
+
+    // set fields
+    rgr.setSample(sample)
+    sequencingCenter.foreach(rgr.setSequencingCenter)
+    description.foreach(rgr.setDescription)
+    runDateEpoch.foreach(e => rgr.setRunDate(new Date(e)))
+    flowOrder.foreach(rgr.setFlowOrder)
+    keySequence.foreach(rgr.setFlowOrder)
+    library.foreach(rgr.setLibrary)
+    predictedMedianInsertSize.foreach(is => {
+      // force implicit conversion
+      val insertSize: java.lang.Integer = is
+      rgr.setPredictedMedianInsertSize(insertSize)
+    })
+    platform.foreach(rgr.setPlatform)
+    platformUnit.foreach(rgr.setPlatformUnit)
+
+    // return
+    rgr
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferencePositionPair.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferencePositionPair.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferencePositionPair.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferencePositionPair.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,207 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import Ordering.Option
+import org.apache.spark.Logging
+import org.bdgenomics.adam.instrumentation.Timers.CreateReferencePositionPair
+import org.bdgenomics.adam.models.ReferenceRegion._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+object ReferencePositionPair extends Logging {
+  def apply(singleReadBucket: SingleReadBucket): ReferencePositionPair = CreateReferencePositionPair.time {
+    val firstOfPair = (singleReadBucket.primaryMapped.filter(_.getFirstOfPair) ++
+      singleReadBucket.unmapped.filter(_.getFirstOfPair)).toSeq
+    val secondOfPair = (singleReadBucket.primaryMapped.filter(_.getSecondOfPair) ++
+      singleReadBucket.unmapped.filter(_.getSecondOfPair)).toSeq
+
+    def getPos(r: AlignmentRecord): ReferencePosition = {
+      if (r.getReadMapped) {
+        new RichAlignmentRecord(r).fivePrimeReferencePosition
+      } else {
+        //ReferencePosition(r.getSequence, 0L)
+        ReferencePosition("ROR", 0L)
+      }
+    }
+
+    if (firstOfPair.size + secondOfPair.size > 0) {
+      val read1refPos = firstOfPair.lift(0).map(getPos)
+      val read2refPos = secondOfPair.lift(0).map(getPos)
+      read1refPos match {
+        case None => new ReferencePositionPair(read2refPos, read1refPos)
+        case Some(r1) => read2refPos match {
+          case None => new ReferencePositionPair(read1refPos, read2refPos)
+          case Some(r2) => (if(r1.referenceName == "ROR") (new ReferencePositionPair(read2refPos, None))
+                            else if(r2.referenceName == "ROR") (new ReferencePositionPair(read1refPos, None))
+                            else if(r1.pos < r2.pos) (new ReferencePositionPair(read1refPos, read2refPos))
+                            else (new ReferencePositionPair(read2refPos, read1refPos)))
+        }
+      }
+      //new ReferencePositionPair(read1refPos, read2refPos)
+      //new ReferencePositionPair(firstOfPair.lift(0).map(getPos),
+      //  secondOfPair.lift(0).map(getPos))
+    } else {
+      new ReferencePositionPair((singleReadBucket.primaryMapped ++
+        singleReadBucket.unmapped).toSeq.lift(0).map(getPos),
+        None)
+    }
+  }
+
+  def apply(reads: Iterable[AlignmentRecord]): ReferencePositionPair = {
+    def getNum(s: String): Long = {
+      s match {
+        case "1" => 1
+        case "2" => 2
+        case "3" => 3
+        case "4" => 4
+        case "5" => 5
+        case "6" => 6
+        case "7" => 7
+        case "8" => 8
+        case "9" => 9
+        case "10" => 10
+        case "11" => 11
+        case "12" => 12
+        case "13" => 13
+        case "14" => 14
+        case "15" => 15
+        case "16" => 16
+        case "17" => 17
+        case "18" => 18
+        case "19" => 19
+        case "20" => 20
+        case "21" => 21
+        case "22" => 22
+        case "X" => 23
+        case "Y" => 24
+        case "MT" => 25
+        case "GL000207.1" => 26
+        case "GL000226.1"	=> 27
+        case "GL000229.1" => 28
+        case "GL000231.1" => 29
+        case "GL000210.1" => 30
+        case "GL000239.1" => 31
+        case "GL000235.1" => 32
+        case "GL000201.1" => 33
+        case "GL000247.1" => 34
+        case "GL000245.1" => 35
+        case "GL000197.1" => 36
+        case "GL000203.1" => 37
+        case "GL000246.1" => 38
+        case "GL000249.1" => 39
+        case "GL000196.1" => 40
+        case "GL000248.1" => 41
+        case "GL000244.1" => 42
+        case "GL000238.1" => 43
+        case "GL000202.1" => 44
+        case "GL000234.1" => 45
+        case "GL000232.1" => 46
+        case "GL000206.1" => 47
+        case "GL000240.1" => 48
+        case "GL000236.1" => 49
+        case "GL000241.1" => 50
+        case "GL000243.1" => 51
+        case "GL000242.1" => 52
+        case "GL000230.1" => 53
+        case "GL000237.1" => 54
+        case "GL000233.1" => 55
+        case "GL000204.1" => 56
+        case "GL000198.1" => 57
+        case "GL000208.1" => 58
+        case "GL000191.1" => 59
+        case "GL000227.1" => 60
+        case "GL000228.1" => 61
+        case "GL000214.1" => 62
+        case "GL000221.1" => 63
+        case "GL000209.1" => 64
+        case "GL000218.1" => 65
+        case "GL000220.1" => 66
+        case "GL000213.1" => 67
+        case "GL000211.1" => 68
+        case "GL000199.1" => 69
+        case "GL000217.1" => 70
+        case "GL000216.1" => 71
+        case "GL000215.1" => 72
+        case "GL000205.1" => 73
+        case "GL000219.1" => 74
+        case "GL000224.1" => 75
+        case "GL000223.1" => 76
+        case "GL000195.1" => 77
+        case "GL000212.1" => 78
+        case "GL000222.1" => 79
+        case "GL000200.1" => 80
+        case "GL000193.1" => 81
+        case "GL000194.1" => 82
+        case "GL000225.1" => 83
+        case "GL000192.1" => 84
+        case _ => 100
+      }
+    }
+    val read1pos = new RichAlignmentRecord(reads.head).fivePrimeReferencePosition
+    val read2pos = new RichAlignmentRecord(reads.last).fivePrimeReferencePosition
+    if(getNum(read1pos.referenceName) < getNum(read2pos.referenceName))
+      new ReferencePositionPair(Some(read1pos), Some(read2pos))
+    else if(getNum(read1pos.referenceName) > getNum(read2pos.referenceName))
+      new ReferencePositionPair(Some(read2pos), Some(read1pos))
+    else if(read1pos.pos < read2pos.pos)
+      new ReferencePositionPair(Some(read1pos), Some(read2pos))
+    else
+      new ReferencePositionPair(Some(read2pos), Some(read1pos))
+  }
+}
+
+case class ReferencePositionPair(read1refPos: Option[ReferencePosition],
+                                 read2refPos: Option[ReferencePosition])
+
+class ReferencePositionPairSerializer extends Serializer[ReferencePositionPair] {
+  val rps = new ReferencePositionSerializer()
+
+  def writeOptionalReferencePos(kryo: Kryo, output: Output, optRefPos: Option[ReferencePosition]) = {
+    optRefPos match {
+      case None =>
+        output.writeBoolean(false)
+      case Some(refPos) =>
+        output.writeBoolean(true)
+        rps.write(kryo, output, refPos)
+    }
+  }
+
+  def readOptionalReferencePos(kryo: Kryo, input: Input): Option[ReferencePosition] = {
+    val exists = input.readBoolean()
+    if (exists) {
+      Some(rps.read(kryo, input, classOf[ReferencePosition]))
+    } else {
+      None
+    }
+  }
+
+  def write(kryo: Kryo, output: Output, obj: ReferencePositionPair) = {
+    writeOptionalReferencePos(kryo, output, obj.read1refPos)
+    writeOptionalReferencePos(kryo, output, obj.read2refPos)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[ReferencePositionPair]): ReferencePositionPair = {
+    val read1ref = readOptionalReferencePos(kryo, input)
+    val read2ref = readOptionalReferencePos(kryo, input)
+    new ReferencePositionPair(read1ref, read2ref)
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferencePosition.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferencePosition.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferencePosition.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferencePosition.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,106 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import org.bdgenomics.formats.avro._
+
+object PositionOrdering extends ReferenceOrdering[ReferencePosition] {
+}
+object OptionalPositionOrdering extends OptionalReferenceOrdering[ReferencePosition] {
+  val baseOrdering = PositionOrdering
+}
+
+object ReferencePosition extends Serializable {
+
+  implicit def orderingForPositions = PositionOrdering
+  implicit def orderingForOptionalPositions = OptionalPositionOrdering
+
+  /**
+   * The UNMAPPED value is a convenience value, which can be used to indicate a position
+   * which is not located anywhere along the reference sequences (see, e.g. its use in
+   * GenomicPositionPartitioner).
+   */
+  val UNMAPPED = new ReferencePosition("", 0)
+
+  /**
+   * Generates a reference position from a read. This function generates the
+   * position from the start mapping position of the read.
+   *
+   * @param record Read from which to generate a reference position.
+   * @return Reference position of the start position.
+   *
+   * @see fivePrime
+   */
+  def apply(record: AlignmentRecord): ReferencePosition = {
+    new ReferencePosition(record.getContig.getContigName.toString, record.getStart)
+  }
+
+  /**
+   * Generates a reference position from a called variant.
+   *
+   * @param variant Called variant from which to generate a
+   * reference position.
+   * @return The reference position of this variant.
+   */
+  def apply(variant: Variant): ReferencePosition = {
+    new ReferencePosition(variant.getContig.getContigName, variant.getStart)
+  }
+
+  /**
+   * Generates a reference position from a genotype.
+   *
+   * @param genotype Genotype from which to generate a reference position.
+   * @return The reference position of this genotype.
+   */
+  def apply(genotype: Genotype): ReferencePosition = {
+    val variant = genotype.getVariant
+    new ReferencePosition(variant.getContig.getContigName, variant.getStart)
+  }
+
+  def apply(referenceName: String, pos: Long): ReferencePosition = {
+    new ReferencePosition(referenceName, pos)
+  }
+
+  def apply(referenceName: String, pos: Long, orientation: Strand): ReferencePosition = {
+    new ReferencePosition(referenceName, pos, orientation)
+  }
+}
+
+class ReferencePosition(override val referenceName: String,
+                        val pos: Long,
+                        override val orientation: Strand = Strand.Independent) extends ReferenceRegion(referenceName, pos, pos + 1, orientation) {
+}
+
+class ReferencePositionSerializer extends Serializer[ReferencePosition] {
+  private val enumValues = Strand.values()
+
+  def write(kryo: Kryo, output: Output, obj: ReferencePosition) = {
+    output.writeString(obj.referenceName)
+    output.writeLong(obj.pos)
+    output.writeInt(obj.orientation.ordinal)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[ReferencePosition]): ReferencePosition = {
+    val refName = input.readString()
+    val pos = input.readLong()
+    val orientation = input.readInt()
+    new ReferencePosition(refName, pos, enumValues(orientation))
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferenceRegion.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferenceRegion.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/ReferenceRegion.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/ReferenceRegion.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,251 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.formats.avro._
+import scala.math.{ max, min }
+
+trait ReferenceOrdering[T <: ReferenceRegion] extends Ordering[T] {
+  private def regionCompare(a: T,
+                            b: T): Int = {
+    if (a.referenceName != b.referenceName) {
+      a.referenceName.compareTo(b.referenceName)
+    } else if (a.start != b.start) {
+      a.start.compareTo(b.start)
+    } else {
+      a.end.compareTo(b.end)
+    }
+  }
+
+  def compare(a: T,
+              b: T): Int = {
+    val rc = regionCompare(a, b)
+    if (rc == 0) {
+      a.orientation.ordinal compare b.orientation.ordinal
+    } else {
+      rc
+    }
+  }
+}
+
+trait OptionalReferenceOrdering[T <: ReferenceRegion] extends Ordering[Option[T]] {
+  val baseOrdering: ReferenceOrdering[T]
+
+  def compare(a: Option[T],
+              b: Option[T]): Int = (a, b) match {
+    case (None, None)         => 0
+    case (Some(pa), Some(pb)) => baseOrdering.compare(pa, pb)
+    case (Some(pa), None)     => -1
+    case (None, Some(pb))     => -1
+  }
+}
+
+object RegionOrdering extends ReferenceOrdering[ReferenceRegion] {
+}
+object OptionalRegionOrdering extends OptionalReferenceOrdering[ReferenceRegion] {
+  val baseOrdering = RegionOrdering
+}
+
+object ReferenceRegion {
+
+  implicit def orderingForPositions = RegionOrdering
+  implicit def orderingForOptionalPositions = OptionalRegionOrdering
+
+  /**
+   * Generates a reference region from read data. Returns None if the read is not mapped;
+   * else, returns the inclusive region from the start to the end of the read alignment.
+   *
+   * @param record Read to create region from.
+   * @return Region corresponding to inclusive region of read alignment, if read is mapped.
+   */
+  def apply(record: AlignmentRecord): ReferenceRegion = {
+    ReferenceRegion(record.getContig.getContigName.toString, record.getStart, record.getEnd)
+  }
+
+  /**
+   * Generates a region from a given position -- the region will have a length of 1.
+   * @param pos The position to convert
+   * @return A 1-wide region at the same location as pos
+   */
+  def apply(pos: ReferencePosition): ReferenceRegion =
+    ReferenceRegion(pos.referenceName, pos.pos, pos.pos + 1)
+
+  /**
+   * Generates a reference region from assembly data. Returns None if the assembly does not
+   * have an ID or a start position.
+   *
+   * @param fragment Assembly fragment from which to generate data.
+   * @return Region corresponding to inclusive region of contig fragment.
+   */
+  def apply(fragment: NucleotideContigFragment): Option[ReferenceRegion] = {
+    val contig = fragment.getContig
+    if (contig != null && contig.getContigName != null &&
+      fragment.getFragmentStartPosition != null) {
+      val fragmentSequence = fragment.getFragmentSequence
+      Some(ReferenceRegion(contig.getContigName,
+        fragment.getFragmentStartPosition,
+        fragment.getFragmentStartPosition + fragmentSequence.length))
+    } else {
+      None
+    }
+  }
+
+  def apply(feature: Feature): ReferenceRegion = {
+    new ReferenceRegion(feature.getContig.getContigName.toString, feature.getStart, feature.getEnd)
+  }
+}
+
+/**
+ * Represents a contiguous region of the reference genome.
+ *
+ * @param referenceName The name of the sequence (chromosome) in the reference genome
+ * @param start The 0-based residue-coordinate for the start of the region
+ * @param end The 0-based residue-coordinate for the first residue <i>after</i> the start
+ *            which is <i>not</i> in the region -- i.e. [start, end) define a 0-based
+ *            half-open interval.
+ */
+case class ReferenceRegion(referenceName: String, start: Long, end: Long, orientation: Strand = Strand.Independent) extends Comparable[ReferenceRegion] with Interval {
+
+  assert(start >= 0 && end >= start, "Failed when trying to create region %s %d %d on %s strand.".format(referenceName, start, end, orientation))
+
+  def width: Long = end - start
+
+  def disorient: ReferenceRegion = new ReferenceRegion(referenceName, start, end)
+
+  /**
+   * Merges two reference regions that are contiguous.
+   *
+   * @throws AssertionError Thrown if regions are not overlapping or adjacent.
+   *
+   * @param region Other region to merge with this region.
+   * @return The merger of both unions.
+   *
+   * @see hull
+   */
+  def merge(region: ReferenceRegion): ReferenceRegion = {
+    assert(overlaps(region) || isAdjacent(region), "Cannot merge two regions that do not overlap or are not adjacent")
+    hull(region)
+  }
+
+  /**
+   * Calculates the intersection of two reference regions.
+   *
+   * @param region Region to intersect with.
+   * @return A smaller reference region.
+   */
+  def intersection(region: ReferenceRegion): ReferenceRegion = {
+    assert(overlaps(region), "Cannot calculate the intersection of non-overlapping regions.")
+    ReferenceRegion(referenceName, max(start, region.start), min(end, region.end))
+  }
+
+  /**
+   * Creates a region corresponding to the convex hull of two regions. Has no preconditions about the adjacency or
+   * overlap of two regions. However, regions must be in the same reference space.
+   *
+   * @throws AssertionError Thrown if regions are in different reference spaces.
+   *
+   * @param region Other region to compute hull of with this region.
+   * @return The convex hull of both unions.
+   *
+   * @see merge
+   */
+  def hull(region: ReferenceRegion): ReferenceRegion = {
+    assert(orientation == region.orientation, "Cannot compute convex hull of differently oriented regions.")
+    assert(referenceName == region.referenceName, "Cannot compute convex hull of regions on different references.")
+    ReferenceRegion(referenceName, min(start, region.start), max(end, region.end))
+  }
+
+  /**
+   * Returns whether two regions are adjacent. Adjacent regions do not overlap, but have no separation between start/end.
+   *
+   * @param region Region to compare against.
+   * @return True if regions are adjacent.
+   */
+  def isAdjacent(region: ReferenceRegion): Boolean =
+    distance(region).map(_ == 1).getOrElse(false)
+
+  /**
+   * Returns the distance between this reference region and another region in the reference space.
+   *
+   * @note Distance here is defined as the minimum distance between any point within this region, and
+   * any point within the other region we are measuring against. If the two sets overlap, the distance
+   * will be 0. If the sets abut, the distance will be 1. Else, the distance will be greater.
+   *
+   * @param other Region to compare against.
+   * @return Returns an option containing the distance between two points. If the point is not in
+   * our reference space, we return an empty option (None).
+   */
+  def distance(other: ReferenceRegion): Option[Long] =
+    if (referenceName == other.referenceName && orientation == other.orientation)
+      if (overlaps(other))
+        Some(0)
+      else if (other.start >= end)
+        Some(other.start - end + 1)
+      else
+        Some(start - other.end + 1)
+    else
+      None
+
+  def contains(other: ReferencePosition): Boolean = {
+    orientation == other.orientation &&
+      referenceName == other.referenceName &&
+      start <= other.pos && end > other.pos
+  }
+
+  def contains(other: ReferenceRegion): Boolean = {
+    orientation == other.orientation &&
+      referenceName == other.referenceName &&
+      start <= other.start && end >= other.end
+  }
+
+  def overlaps(other: ReferenceRegion): Boolean = {
+    orientation == other.orientation &&
+      referenceName == other.referenceName &&
+      end > other.start && start < other.end
+  }
+
+  def compareTo(that: ReferenceRegion): Int = {
+    RegionOrdering.compare(this, that)
+  }
+
+  def length(): Long = {
+    end - start
+  }
+}
+
+class ReferenceRegionSerializer extends Serializer[ReferenceRegion] {
+  private val enumValues = Strand.values()
+
+  def write(kryo: Kryo, output: Output, obj: ReferenceRegion) = {
+    output.writeString(obj.referenceName)
+    output.writeLong(obj.start)
+    output.writeLong(obj.end)
+    output.writeInt(obj.orientation.ordinal())
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[ReferenceRegion]): ReferenceRegion = {
+    val referenceName = input.readString()
+    val start = input.readLong()
+    val end = input.readLong()
+    val orientation = input.readInt()
+    new ReferenceRegion(referenceName, start, end, enumValues(orientation))
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SAMFileHeaderWritable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SAMFileHeaderWritable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SAMFileHeaderWritable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SAMFileHeaderWritable.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,59 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import htsjdk.samtools.{ SAMFileHeader, SAMProgramRecord }
+import org.bdgenomics.adam.rdd.ADAMContext._
+
+object SAMFileHeaderWritable {
+  def apply(header: SAMFileHeader): SAMFileHeaderWritable = {
+    new SAMFileHeaderWritable(header)
+  }
+}
+
+class SAMFileHeaderWritable(@transient hdr: SAMFileHeader) extends Serializable {
+  // extract fields that are needed in order to recreate the SAMFileHeader
+  protected val text = {
+    val txt: String = hdr.getTextHeader
+    Option(txt)
+  }
+  protected val sd = SequenceDictionary(hdr.getSequenceDictionary)
+  protected val pgl = {
+    val pgs: List[SAMProgramRecord] = hdr.getProgramRecords
+    pgs.map(ProgramRecord(_))
+  }
+  protected val comments = {
+    val cmts: List[java.lang.String] = hdr.getComments
+    cmts.flatMap(Option(_)).map(_.toString) // don't trust samtools to return non-nulls
+  }
+  protected val rgs = RecordGroupDictionary.fromSAMHeader(hdr)
+
+  // recreate header when requested to get around header not being serializable
+  @transient lazy val header = {
+    val h = new SAMFileHeader()
+
+    // add back optional fields
+    text.foreach(h.setTextHeader)
+    h.setSequenceDictionary(sd.toSAMSequenceDictionary)
+    pgl.foreach(p => h.addProgramRecord(p.toSAMProgramRecord))
+    comments.foreach(h.addComment)
+    rgs.recordGroups.foreach(rg => h.addReadGroup(rg.toSAMReadGroupRecord))
+
+    h
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SequenceDictionary.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SequenceDictionary.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SequenceDictionary.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SequenceDictionary.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,290 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.apache.avro.generic.IndexedRecord
+import org.bdgenomics.formats.avro.{ AlignmentRecord, NucleotideContigFragment, Contig }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import htsjdk.samtools.{ SamReader, SAMFileHeader, SAMSequenceRecord, SAMSequenceDictionary }
+import scala.collection._
+
+/**
+ * SequenceDictionary contains the (bijective) map between Ints (the referenceId) and Strings (the referenceName)
+ * from the header of a BAM file, or the combined result of multiple such SequenceDictionaries.
+ */
+
+object SequenceDictionary {
+  def apply(): SequenceDictionary = new SequenceDictionary()
+  def apply(records: SequenceRecord*): SequenceDictionary = new SequenceDictionary(records.toVector)
+  def apply(dict: SAMSequenceDictionary): SequenceDictionary = {
+    new SequenceDictionary(dict.getSequences.map(SequenceRecord.fromSAMSequenceRecord).toVector)
+  }
+  def apply(header: SAMFileHeader): SequenceDictionary = SequenceDictionary(header.getSequenceDictionary)
+  def apply(reader: SamReader): SequenceDictionary = SequenceDictionary(reader.getFileHeader)
+
+  def toSAMSequenceDictionary(dictionary: SequenceDictionary): SAMSequenceDictionary = {
+    new SAMSequenceDictionary(dictionary.records.map(SequenceRecord.toSAMSequenceRecord).toList)
+  }
+
+  /**
+   * Extracts a SAM sequence dictionary from a SAM file header and returns an
+   * ADAM sequence dictionary.
+   *
+   * @see fromSAMSequenceDictionary
+   *
+   * @param header SAM file header.
+   * @return Returns an ADAM style sequence dictionary.
+   */
+  def fromSAMHeader(header: SAMFileHeader): SequenceDictionary = {
+    val samDict = header.getSequenceDictionary
+
+    fromSAMSequenceDictionary(samDict)
+  }
+
+  /**
+   * Converts a picard/samtools SAMSequenceDictionary into an ADAM sequence dictionary.
+   *
+   * @see fromSAMHeader
+   * @see fromVCFHeader
+   *
+   * @param samDict SAM style sequence dictionary.
+   * @return Returns an ADAM style sequence dictionary.
+   */
+  def fromSAMSequenceDictionary(samDict: SAMSequenceDictionary): SequenceDictionary = {
+    val samDictRecords: List[SAMSequenceRecord] = samDict.getSequences
+    new SequenceDictionary(samDictRecords.map(SequenceRecord.fromSAMSequenceRecord).toVector)
+  }
+
+  def fromSAMReader(samReader: SamReader): SequenceDictionary =
+    fromSAMHeader(samReader.getFileHeader)
+}
+
+class SequenceDictionary(val records: Vector[SequenceRecord]) extends Serializable {
+  def this() = this(Vector.empty[SequenceRecord])
+
+  private val byName: Map[String, SequenceRecord] = records.view.map(r => r.name -> r).toMap
+  assert(byName.size == records.length, "SequenceRecords with duplicate names aren't permitted")
+
+  def isCompatibleWith(that: SequenceDictionary): Boolean = {
+    for (record <- that.records) {
+      val myRecord = byName.get(record.name)
+      if (myRecord.isDefined && myRecord.get != record)
+        return false
+    }
+    true
+  }
+
+  def apply(name: String): Option[SequenceRecord] = byName.get(name)
+  def containsRefName(name: String): Boolean = byName.containsKey(name)
+
+  def +(record: SequenceRecord): SequenceDictionary = this ++ SequenceDictionary(record)
+  def ++(that: SequenceDictionary): SequenceDictionary = {
+    new SequenceDictionary(records ++ that.records.filter(r => !byName.contains(r.name)))
+  }
+
+  override def hashCode = records.hashCode()
+  override def equals(o: Any) = o match {
+    case that: SequenceDictionary => records.equals(that.records)
+    case _                        => false
+  }
+
+  /**
+   * Converts this ADAM style sequence dictionary into a SAM style sequence dictionary.
+   *
+   * @return Returns a SAM formatted sequence dictionary.
+   */
+  def toSAMSequenceDictionary: SAMSequenceDictionary = {
+    new SAMSequenceDictionary(records.map(_ toSAMSequenceRecord).toList)
+  }
+
+  override def toString: String = {
+    records.map(_.toString).fold("SequenceDictionary{")(_ + "\n" + _) + "}"
+  }
+}
+
+/**
+ * Utility class within the SequenceDictionary; represents unique reference name-to-id correspondence
+ *
+ */
+class SequenceRecord(
+    val name: String,
+    val length: Long,
+    val url: Option[String] = None,
+    val md5: Option[String] = None,
+    val refseq: Option[String] = None,
+    val genbank: Option[String] = None,
+    val assembly: Option[String] = None,
+    val species: Option[String] = None) extends Serializable {
+
+  assert(name != null && !name.isEmpty, "SequenceRecord.name is null or empty")
+  assert(length > 0, "SequenceRecord.length <= 0")
+
+  override def toString: String = "%s->%s".format(name, length)
+
+  /**
+   * Converts this sequence record into a SAM sequence record.
+   *
+   * @return A SAM formatted sequence record.
+   */
+  def toSAMSequenceRecord: SAMSequenceRecord = {
+    val rec = new SAMSequenceRecord(name.toString, length.toInt)
+
+    // set md5 if available
+    md5.foreach(s => rec.setAttribute(SAMSequenceRecord.MD5_TAG, s.toUpperCase))
+
+    // set URL if available
+    url.foreach(rec.setAttribute(SAMSequenceRecord.URI_TAG, _))
+
+    // set species if available
+    species.foreach(rec.setAttribute(SAMSequenceRecord.SPECIES_TAG, _))
+
+    // set assembly if available
+    assembly.foreach(rec.setAssembly)
+
+    // set refseq accession number if available
+    refseq.foreach(rec.setAttribute("REFSEQ", _))
+
+    // set genbank accession number if available
+    genbank.foreach(rec.setAttribute("GENBANK", _))
+
+    // return record
+    rec
+  }
+
+  override def equals(o: Any): Boolean = o match {
+    case that: SequenceRecord =>
+      name == that.name && length == that.length && optionEq(md5, that.md5) && optionEq(url, that.url)
+    case _ => false
+  }
+
+  // No md5/url is "equal" to any md5/url in this setting
+  private def optionEq(o1: Option[String], o2: Option[String]) = (o1, o2) match {
+    case (Some(c1), Some(c2)) => c1 == c2
+    case _                    => true
+  }
+}
+
+object SequenceRecord {
+  val REFSEQ_TAG = "REFSEQ"
+  val GENBANK_TAG = "GENBANK"
+
+  def apply(name: String,
+            length: Long,
+            md5: String = null,
+            url: String = null,
+            refseq: String = null,
+            genbank: String = null,
+            assembly: String = null,
+            species: String = null): SequenceRecord = {
+    new SequenceRecord(
+      name,
+      length,
+      Option(url).map(_.toString),
+      Option(md5).map(_.toString),
+      Option(refseq).map(_.toString),
+      Option(genbank).map(_.toString),
+      Option(assembly).map(_.toString),
+      Option(species).map(_.toString))
+  }
+
+  /*
+   * Generates a sequence record from a SAMSequence record.
+   *
+   * @param seqRecord SAM Sequence record input.
+   * @return A new ADAM sequence record.
+   */
+  def fromSAMSequenceRecord(record: SAMSequenceRecord): SequenceRecord = {
+    SequenceRecord(
+      record.getSequenceName,
+      record.getSequenceLength,
+      md5 = record.getAttribute(SAMSequenceRecord.MD5_TAG),
+      url = record.getAttribute(SAMSequenceRecord.URI_TAG),
+      refseq = record.getAttribute(REFSEQ_TAG),
+      genbank = record.getAttribute(GENBANK_TAG),
+      assembly = record.getAssembly,
+      species = record.getAttribute(SAMSequenceRecord.SPECIES_TAG))
+
+  }
+  def toSAMSequenceRecord(record: SequenceRecord): SAMSequenceRecord = {
+    val sam = new SAMSequenceRecord(record.name, record.length.toInt)
+    record.md5.foreach(v => sam.setAttribute(SAMSequenceRecord.MD5_TAG, v.toString))
+    record.url.foreach(v => sam.setAttribute(SAMSequenceRecord.URI_TAG, v.toString))
+    sam
+  }
+
+  def fromADAMContig(contig: Contig): SequenceRecord = {
+    SequenceRecord(
+      contig.getContigName.toString,
+      contig.getContigLength,
+      md5 = contig.getContigName,
+      url = contig.getReferenceURL,
+      assembly = contig.getAssembly,
+      species = contig.getSpecies)
+  }
+
+  def toADAMContig(record: SequenceRecord): Contig = {
+    val builder = Contig.newBuilder()
+      .setContigName(record.name)
+      .setContigLength(record.length)
+    record.md5.foreach(builder.setContigMD5)
+    record.url.foreach(builder.setReferenceURL)
+    record.assembly.foreach(builder.setAssembly)
+    record.species.foreach(builder.setSpecies)
+    builder.build
+  }
+
+  def fromADAMContigFragment(fragment: NucleotideContigFragment): SequenceRecord = {
+    fromADAMContig(fragment.getContig)
+  }
+
+  /**
+   * Convert an Read into one or more SequenceRecords.
+   * The reason that we can't simply use the "fromSpecificRecord" method, below, is that each Read
+   * can (through the fact that it could be a pair of reads) contain 1 or 2 possible SequenceRecord entries
+   * for the SequenceDictionary itself.  Both have to be extracted, separately.
+   *
+   * @param rec The Read from which to extract the SequenceRecord entries
+   * @return a list of all SequenceRecord entries derivable from this record.
+   */
+  def fromADAMRecord(rec: AlignmentRecord): Set[SequenceRecord] = {
+    assert(rec != null, "Read was null")
+    if ((!rec.getReadPaired || rec.getFirstOfPair) && (rec.getContig != null || rec.getMateContig != null)) {
+      // The contig should be null for unmapped read
+      List(Option(rec.getContig), Option(rec.getMateContig))
+        .flatten
+        .map(fromADAMContig)
+        .toSet
+    } else
+      Set()
+  }
+
+  def fromSpecificRecord(rec: IndexedRecord): SequenceRecord = {
+    val schema = rec.getSchema
+    if (schema.getField("referenceId") != null) {
+      SequenceRecord(
+        rec.get(schema.getField("referenceName").pos()).toString,
+        rec.get(schema.getField("referenceLength").pos()).asInstanceOf[Long],
+        url = rec.get(schema.getField("referenceUrl").pos()).toString)
+    } else if (schema.getField("contig") != null) {
+      val pos = schema.getField("contig").pos()
+      fromADAMContig(rec.get(pos).asInstanceOf[Contig])
+    } else {
+      throw new AssertionError("Missing information to generate SequenceRecord")
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SingleReadBucket.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SingleReadBucket.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SingleReadBucket.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SingleReadBucket.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,84 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import com.esotericsoftware.kryo.io.{ Output, Input }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.serialization.AvroSerializer
+import org.apache.spark.Logging
+import org.apache.spark.rdd.RDD
+
+object SingleReadBucket extends Logging {
+  def apply(rdd: RDD[AlignmentRecord]): RDD[SingleReadBucket] = {
+    rdd.groupBy(p => (p.getRecordGroupName, p.getReadName))
+      .map(kv => {
+        val (_, reads) = kv
+
+        // split by mapping
+        val (mapped, unmapped) = reads.partition(_.getReadMapped)
+        val (primaryMapped, secondaryMapped) = mapped.partition(_.getPrimaryAlignment)
+
+        // TODO: consider doing validation here (e.g. read says mate mapped but it doesn't exist)
+        new SingleReadBucket(primaryMapped, secondaryMapped, unmapped)
+      })
+  }
+}
+
+case class SingleReadBucket(primaryMapped: Iterable[AlignmentRecord] = Seq.empty,
+                            secondaryMapped: Iterable[AlignmentRecord] = Seq.empty,
+                            unmapped: Iterable[AlignmentRecord] = Seq.empty) {
+  // Note: not a val in order to save serialization/memory cost
+  def allReads = {
+    primaryMapped ++ secondaryMapped ++ unmapped
+  }
+}
+
+class SingleReadBucketSerializer extends Serializer[SingleReadBucket] {
+  val recordSerializer = new AvroSerializer[AlignmentRecord]()
+
+  def writeArray(kryo: Kryo, output: Output, reads: Seq[AlignmentRecord]): Unit = {
+    output.writeInt(reads.size, true)
+    for (read <- reads) {
+      recordSerializer.write(kryo, output, read)
+    }
+  }
+
+  def readArray(kryo: Kryo, input: Input): Seq[AlignmentRecord] = {
+    val numReads = input.readInt(true)
+    (0 until numReads).foldLeft(List[AlignmentRecord]()) {
+      (a, b) => recordSerializer.read(kryo, input, classOf[AlignmentRecord]) :: a
+    }
+  }
+
+  def write(kryo: Kryo, output: Output, groupedReads: SingleReadBucket) = {
+    writeArray(kryo, output, groupedReads.primaryMapped.toSeq)
+    writeArray(kryo, output, groupedReads.secondaryMapped.toSeq)
+    writeArray(kryo, output, groupedReads.unmapped.toSeq)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[SingleReadBucket]): SingleReadBucket = {
+    val primaryReads = readArray(kryo, input)
+    val secondaryReads = readArray(kryo, input)
+    val unmappedReads = readArray(kryo, input)
+    new SingleReadBucket(primaryReads, secondaryReads, unmappedReads)
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SnpTable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SnpTable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/SnpTable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/SnpTable.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,97 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.adam.rich.RichVariant
+import org.bdgenomics.adam.rich.DecadentRead._
+import org.apache.spark.Logging
+import org.apache.spark.rdd.RDD
+import scala.collection.immutable._
+import scala.collection.mutable
+import java.io.File
+
+class SnpTable(private val table: Map[String, Set[Long]]) extends Serializable with Logging {
+  log.info("SNP table has %s contigs and %s entries".format(table.size, table.values.map(_.size).sum))
+
+  /**
+   * Is there a known SNP at the reference location of this Residue?
+   */
+  def isMasked(residue: Residue): Boolean =
+    contains(residue.referencePosition)
+
+  /**
+   * Is there a known SNP at the given reference location?
+   */
+  def contains(location: ReferencePosition): Boolean = {
+    val bucket = table.get(location.referenceName)
+    if (bucket.isEmpty) unknownContigWarning(location.referenceName)
+    bucket.map(_.contains(location.pos)).getOrElse(false)
+  }
+
+  private val unknownContigs = new mutable.HashSet[String]
+
+  private def unknownContigWarning(contig: String) = {
+    // This is synchronized to avoid a data race. Multiple threads may
+    // race to update `unknownContigs`, e.g. when running with a Spark
+    // master of `local[N]`.
+    synchronized {
+      if (!unknownContigs.contains(contig)) {
+        unknownContigs += contig
+        log.warn("Contig has no entries in known SNPs table: %s".format(contig))
+      }
+    }
+  }
+}
+
+object SnpTable {
+  def apply(): SnpTable = {
+    new SnpTable(Map[String, Set[Long]]())
+  }
+
+  // `knownSnpsFile` is expected to be a sites-only VCF
+  def apply(knownSnpsFile: File): SnpTable = {
+    // parse into tuples of (contig, position)
+    val lines = scala.io.Source.fromFile(knownSnpsFile).getLines()
+    val tuples = lines.filter(line => !line.startsWith("#")).flatMap(line => {
+      val split = line.split("\t")
+      val contig = split(0)
+      val pos = split(1).toLong - 1
+      val ref = split(3)
+      assert(pos >= 0)
+      assert(!ref.isEmpty)
+      ref.zipWithIndex.map {
+        case (base, idx) =>
+          assert(Seq('A', 'C', 'T', 'G', 'N').contains(base))
+          (contig, pos + idx)
+      }
+    })
+    // construct map from contig to set of positions
+    // this is done in-place to reduce overhead
+    val table = new mutable.HashMap[String, mutable.HashSet[Long]]
+    tuples.foreach(tup => table.getOrElseUpdate(tup._1, { new mutable.HashSet[Long] }) += tup._2)
+    // construct SnpTable from immutable copy of `table`
+    new SnpTable(table.mapValues(_.toSet).toMap)
+  }
+
+  def apply(variants: RDD[RichVariant]): SnpTable = {
+    val positions = variants.map(variant => (variant.getContig.getContigName.toString, variant.getStart)).collect()
+    val table = new mutable.HashMap[String, mutable.HashSet[Long]]
+    positions.foreach(tup => table.getOrElseUpdate(tup._1, { new mutable.HashSet[Long] }) += tup._2)
+    new SnpTable(table.mapValues(_.toSet).toMap)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/VariantContext.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/VariantContext.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/VariantContext.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/VariantContext.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,90 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import org.bdgenomics.formats.avro.{ Genotype, DatabaseVariantAnnotation, Variant }
+import org.bdgenomics.adam.rich.RichVariant
+import org.bdgenomics.adam.rich.RichVariant._
+
+/**
+ * Note: VariantContext inherits its name from the Picard VariantContext, and is not related to the SparkContext object.
+ * If you're looking for the latter, see [[org.bdgenomics.adam.rdd.variation.VariationContext]]
+ */
+
+object VariantContext {
+
+  /**
+   * Constructs an VariantContext from locus data. Used in merger process.
+   *
+   * @param kv Nested tuple containing (locus on reference, (variants at site, genotypes at site,
+   *           optional domain annotation at site))
+   * @return VariantContext corresponding to the data above.
+   */
+  def apply(kv: (ReferencePosition, Variant, Iterable[Genotype], Option[DatabaseVariantAnnotation])): VariantContext = {
+    new VariantContext(kv._1, kv._2, kv._3, kv._4)
+  }
+
+  /**
+   * Constructs an VariantContext from an Variant
+   *
+   * @param v Variant which is used to construct the ReferencePosition
+   * @return VariantContext corresponding to the Variant
+   */
+  def apply(v: Variant): VariantContext = {
+    apply((ReferencePosition(v), v, Seq(), None))
+  }
+
+  /**
+   * Constructs an VariantContext from an Variant and Seq[Genotype]
+   *  and DatabaseVariantAnnotation
+   *
+   * @param v Variant which is used to construct the ReferencePosition
+   * @param genotypes Seq[Genotype]
+   * @param annotation Option[DatabaseVariantAnnotation]
+   * @return VariantContext corresponding to the Variant
+   */
+  def apply(v: Variant, genotypes: Iterable[Genotype], annotation: Option[DatabaseVariantAnnotation] = None): VariantContext = {
+    apply((ReferencePosition(v), v, genotypes, annotation))
+  }
+
+  /**
+   * Builds a variant context off of a set of genotypes. Builds variants from the genotypes.
+   *
+   * @note Genotypes must be at the same position.
+   *
+   * @param genotypes List of genotypes to build variant context from.
+   * @return A variant context corresponding to the variants and genotypes at this site.
+   */
+  def buildFromGenotypes(genotypes: Seq[Genotype]): VariantContext = {
+    val position = ReferencePosition(genotypes.head)
+    assert(genotypes.map(ReferencePosition(_)).forall(_ == position),
+      "Genotypes do not all have the same position.")
+
+    val variant = genotypes.head.getVariant
+
+    new VariantContext(position, variant, genotypes, None)
+  }
+}
+
+class VariantContext(
+    val position: ReferencePosition,
+    val variant: RichVariant,
+    val genotypes: Iterable[Genotype],
+    val databases: Option[DatabaseVariantAnnotation] = None) {
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/models/VCFHeaderWritable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/VCFHeaderWritable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/models/VCFHeaderWritable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/models/VCFHeaderWritable.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,23 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.models
+
+import htsjdk.variant.vcf.VCFHeader
+
+case class VCFHeaderWritable(header: VCFHeader) {
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/AccessControl.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/AccessControl.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/AccessControl.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/AccessControl.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,30 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.plugins
+
+/**
+ * This allows filtering based on an input type.
+ */
+trait AccessControl[Input] {
+  /**
+   * The predicate associated with the current AccessControl.
+   * @return If there is no filter for AccessControl, None
+   *         If a filter is provided, a true response means access is granted; a false means access is denied.
+   */
+  def predicate: Option[Input => Boolean]
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/ADAMPlugin.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/ADAMPlugin.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/ADAMPlugin.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/ADAMPlugin.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,48 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.plugins
+
+import org.apache.avro.Schema
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext
+
+/**
+ * Defines the interface for a Plugin for the ADAMSystem.
+ *
+ * A simple interface is available in org.bdgenomics.adam.plugins.Take10Plugin.
+ */
+trait ADAMPlugin[Input, Output] {
+  /**
+   * The projection to push down into Parquet
+   * @return If all fields are required or the specific projection is not known, None
+   *         If a subset of fields on Input are required, an Avro schema with those fields
+   */
+  def projection: Option[Schema]
+
+  /**
+   * The records that are applicable to this Plugin
+   * @return If there is no filter for this plugin, None
+   *         If a filter is applicable, a true response means record is included; a false means record is excluded
+   */
+  def predicate: Option[Input => Boolean]
+
+  /**
+   * Method to create the transformations on the RDD.
+   */
+  def run(sc: SparkContext, recs: RDD[Input], args: String): RDD[Output]
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/EmptyAccessControl.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/EmptyAccessControl.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/plugins/EmptyAccessControl.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/plugins/EmptyAccessControl.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,22 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.plugins
+
+class EmptyAccessControl[T] extends AccessControl[T] with Serializable {
+  override def predicate: Option[T => Boolean] = None
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/AlignmentRecordField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/AlignmentRecordField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/AlignmentRecordField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/AlignmentRecordField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,32 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * This enumeration exist in order to reduce typo errors in the code. It needs to be kept
+ * in sync with any changes to Read.
+ *
+ * This enumeration is necessary because Parquet needs the field string names
+ * for predicates and projections.
+ */
+object AlignmentRecordField extends FieldEnumeration(AlignmentRecord.SCHEMA$) {
+
+  val contig, start, end, mapq, readName, sequence, mateAlignmentStart, cigar, qual, recordGroupId, recordGroupName, readPaired, properPair, readMapped, mateMapped, readNegativeStrand, mateNegativeStrand, firstOfPair, secondOfPair, primaryAlignment, failedVendorQualityChecks, duplicateRead, mismatchingPositions, attributes, recordGroupSequencingCenter, recordGroupDescription, recordGroupRunDateEpoch, recordGroupFlowOrder, recordGroupKeySequence, recordGroupLibrary, recordGroupPredictedMedianInsertSize, recordGroupPlatform, recordGroupPlatformUnit, recordGroupSample, mateContig, origQual, supplmentaryAlignment = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/DatabaseVariantAnnotationField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/DatabaseVariantAnnotationField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/DatabaseVariantAnnotationField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/DatabaseVariantAnnotationField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,31 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.DatabaseVariantAnnotation
+
+object DatabaseVariantAnnotationField extends FieldEnumeration(DatabaseVariantAnnotation.SCHEMA$) {
+
+  val variant, dbsnpId, //domain information
+  hgvs, geneSymbol, ensemblGeneId, ensemblTranscriptIds, //clinical fields
+  omimId, cosmicId, clinvarId, clinicalSignificance, //conservation
+  gerpNr, gerpRs, phylop, ancestralAllele, //population statistics
+  thousandGenomesAlleleCount, thousandGenomesAlleleFrequency, //effect
+  referenceAminoAcid, alternateAminoAcid, //predicted effects
+  siftScore, siftScoreConverted, siftPred, mutationTasterScore, mutationTasterScoreConverted, mutationTasterPred = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/FeatureField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/FeatureField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/FeatureField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/FeatureField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,32 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.Feature
+
+/**
+ * This enumeration exist in order to reduce typo errors in the code. It needs to be kept
+ * in sync with any changes to Feature.
+ *
+ * This enumeration is necessary because Parquet needs the field string names
+ * for predicates and projections.
+ */
+object FeatureField extends FieldEnumeration(Feature.SCHEMA$) {
+
+  val featureId, featureType, source, contig, start, end, strand, value, dbxrefs, parentIds, attributes = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/FieldEnumeration.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/FieldEnumeration.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/FieldEnumeration.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/FieldEnumeration.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,61 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.apache.avro.Schema
+
+/**
+ * The FieldValue trait (and its sister abstract class, FieldEnumeration) are meant to help
+ * clean up the use of Projection when dealing with field enumeration classes like
+ * ADAMRecordField.
+ *
+ * Projection is a class for turning fields from an Enumeration into a projected Avro Schema object.
+ *
+ * In the old way of doing this, we used a "normal" Enumeration.  Projection would receive a
+ * collection of Read.Value objects, and use their names (plus the Read.SCHEMA$
+ * static field) to turn them into a Schema for ADAMRecords.  This worked fine for Read,
+ * and not at all for generalizing to other Schemas over other field enumerations.
+ *
+ * In the new system, we embed the Avro Schema object as an argument *in* each enumeration Value.
+ * We do this in two steps:
+ * (1) instead of ADAMRecordField (e.g.) extending Enumeration, it extends FieldEnumeration and
+ * provides the appropriate (static) Schema object as an argument.
+ * (2) instead of using the (final, non-overrideable) Value method within Enumeration to provide
+ * each enum value, it calls FieldEnumeration.SchemaValue instead, which embeds the corresponding
+ * Schema in each value.
+ *
+ * Finally, Projection will extract the Schema value from the first FieldValue that is given to it
+ * and produce the corresponding (projected) Schema.
+ *
+ * This means, of course, that Projection can't handle empty field lists -- but that was always
+ * going to be an error-filled edge-case anyway (why would you want to project to zero fields?)
+ *
+ */
+trait FieldValue {
+  def schema: Schema
+}
+
+abstract class FieldEnumeration(val recordSchema: Schema) extends Enumeration {
+
+  class SchemaVal extends Val with FieldValue {
+    def schema = recordSchema
+  }
+
+  protected final def SchemaValue = new SchemaVal()
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/GenotypeField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/GenotypeField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/GenotypeField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/GenotypeField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,24 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.Genotype
+
+object GenotypeField extends FieldEnumeration(Genotype.SCHEMA$) {
+  val variant, variantCallingAnnotations, sampleId, sampleDescription, processingDescription, alleles, referenceReadDepth, alternateReadDepth, readDepth, genotypeQuality, genotypeLikelihoods, splitFromMultiAllelic, isPhased, phaseSetId, phaseQuality = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/NucleotideContigFragmentField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/NucleotideContigFragmentField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/NucleotideContigFragmentField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/NucleotideContigFragmentField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,32 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.NucleotideContigFragment
+
+/**
+ * This enumeration exist in order to reduce typo errors in the code. It needs to be kept
+ * in sync with any changes to Read.
+ *
+ * This enumeration is necessary because Parquet needs the field string names
+ * for predicates and projections.
+ */
+object NucleotideContigFragmentField extends FieldEnumeration(NucleotideContigFragment.SCHEMA$) {
+
+  val contig, description, fragmentSequence, fragmentNumber, fragmentStartPosition, numberOfFragmentsInContig, url = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/Projection.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/Projection.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/Projection.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/Projection.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,65 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.apache.avro.Schema
+import scala.collection.JavaConversions._
+import org.apache.avro.Schema.Field
+
+/**
+ * Avro utility object to create a projection of a Schema
+ */
+object Projection {
+
+  private def createProjection(fullSchema: Schema, fields: Set[String], exclude: Boolean = false): Schema = {
+    val projectedSchema = Schema.createRecord(fullSchema.getName, fullSchema.getDoc, fullSchema.getNamespace, fullSchema.isError)
+    projectedSchema.setFields(fullSchema.getFields.filter(createFilterPredicate(fields, exclude))
+      .map(p => new Field(p.name, p.schema, p.doc, p.defaultValue, p.order)))
+    projectedSchema
+  }
+
+  private def createFilterPredicate(fieldNames: Set[String], exclude: Boolean = false): Field => Boolean = {
+    val filterPred = (f: Field) => fieldNames.contains(f.name)
+    val includeOrExlcude = (contains: Boolean) => if (exclude) !contains else contains
+    filterPred.andThen(includeOrExlcude)
+  }
+
+  // TODO: Unify these various methods
+  def apply(includedFields: FieldValue*): Schema = {
+    assert(!includedFields.isEmpty, "Can't project down to zero fields!")
+    Projection(false, includedFields: _*)
+  }
+
+  def apply(includedFields: Traversable[FieldValue]): Schema = {
+    assert(includedFields.size > 0, "Can't project down to zero fields!")
+    val baseSchema = includedFields.head.schema
+    createProjection(baseSchema, includedFields.map(_.toString).toSet, false)
+  }
+
+  def apply(exclude: Boolean, includedFields: FieldValue*): Schema = {
+    val baseSchema = includedFields.head.schema
+    createProjection(baseSchema, includedFields.map(_.toString).toSet, exclude)
+  }
+}
+
+object Filter {
+
+  def apply(excludeFields: FieldValue*): Schema = {
+    Projection(true, excludeFields: _*)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/VariantCallingAnnotationsField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/VariantCallingAnnotationsField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/VariantCallingAnnotationsField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/VariantCallingAnnotationsField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,25 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.VariantCallingAnnotations
+
+class VariantCallingAnnotationsField extends FieldEnumeration(VariantCallingAnnotations.SCHEMA$) {
+
+  val readDepth, downsampled, baseQRankSum, clippingRankSum, haplotypeScore, inbreedingCoefficient, alleleCountMLE, alleleFrequencyMLE, rmsMapQ, mapq0Reads, mqRankSum, usedForNegativeTrainingSet, usedForPositiveTrainingSet, variantQualityByDepth, readPositionRankSum, vqslod, culprit, variantCallErrorProbability, variantIsPassing, variantFilters = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/VariantField.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/VariantField.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/projections/VariantField.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/projections/VariantField.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,24 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.projections
+
+import org.bdgenomics.formats.avro.Variant
+
+class VariantField extends FieldEnumeration(Variant.SCHEMA$) {
+  val contig, position, referenceAllele, variantAllele = SchemaValue
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ADAMContext.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ADAMContext.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ADAMContext.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ADAMContext.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,579 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import java.util.regex.Pattern
+import htsjdk.samtools.SAMFileHeader
+import org.apache.avro.Schema
+import org.apache.avro.generic.IndexedRecord
+import org.apache.avro.specific.SpecificRecord
+import org.apache.hadoop.fs.{ FileSystem, Path }
+import org.apache.hadoop.io.{ LongWritable, Text }
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat
+import org.apache.spark.rdd.RDD
+import org.apache.spark.rdd.MetricsContext._
+import org.apache.spark.{ Logging, SparkConf, SparkContext }
+import org.bdgenomics.adam.converters._
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.io._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.projections.{ AlignmentRecordField, NucleotideContigFragmentField, Projection }
+import org.bdgenomics.adam.rdd.contig.NucleotideContigFragmentRDDFunctions
+import org.bdgenomics.adam.rdd.features._
+import org.bdgenomics.adam.rdd.read.AlignmentRecordRDDFunctions
+import org.bdgenomics.adam.rdd.variation._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro._
+import org.bdgenomics.utils.instrumentation.Metrics
+import org.bdgenomics.utils.misc.HadoopUtil
+import org.seqdoop.hadoop_bam.util.SAMHeaderReader
+import org.seqdoop.hadoop_bam._
+import parquet.avro.{ AvroParquetInputFormat, AvroReadSupport }
+import parquet.filter2.predicate.FilterPredicate
+import parquet.hadoop.ParquetInputFormat
+import parquet.hadoop.util.ContextUtil
+import scala.collection.JavaConversions._
+import scala.collection.Map
+import scala.reflect.ClassTag
+
+object ADAMContext {
+  // Add ADAM Spark context methods
+  implicit def sparkContextToADAMContext(sc: SparkContext): ADAMContext = new ADAMContext(sc)
+
+  // Add generic RDD methods for all types of ADAM RDDs
+  implicit def rddToADAMRDD[T](rdd: RDD[T])(implicit ev1: T => IndexedRecord, ev2: Manifest[T]): ADAMRDDFunctions[T] = new ADAMRDDFunctions(rdd)
+
+  // Add methods specific to Read RDDs
+  implicit def rddToADAMRecordRDD(rdd: RDD[AlignmentRecord]) = new AlignmentRecordRDDFunctions(rdd)
+
+  // Add methods specific to the ADAMNucleotideContig RDDs
+  implicit def rddToContigFragmentRDD(rdd: RDD[NucleotideContigFragment]) = new NucleotideContigFragmentRDDFunctions(rdd)
+
+  // implicit conversions for variant related rdds
+  implicit def rddToVariantContextRDD(rdd: RDD[VariantContext]) = new VariantContextRDDFunctions(rdd)
+  implicit def rddToADAMGenotypeRDD(rdd: RDD[Genotype]) = new GenotypeRDDFunctions(rdd)
+
+  // add gene feature rdd functions
+  implicit def convertBaseFeatureRDDToGeneFeatureRDD(rdd: RDD[Feature]) = new GeneFeatureRDDFunctions(rdd)
+
+  // Add implicits for the rich adam objects
+  implicit def recordToRichRecord(record: AlignmentRecord): RichAlignmentRecord = new RichAlignmentRecord(record)
+
+  // implicit java to scala type conversions
+  implicit def listToJavaList[A](list: List[A]): java.util.List[A] = seqAsJavaList(list)
+
+  implicit def javaListToList[A](list: java.util.List[A]): List[A] = asScalaBuffer(list).toList
+
+  implicit def javaSetToSet[A](set: java.util.Set[A]): Set[A] = {
+    // toSet is necessary to make set immutable
+    asScalaSet(set).toSet
+  }
+
+  implicit def intListToJavaIntegerList(list: List[Int]): java.util.List[java.lang.Integer] = {
+    seqAsJavaList(list.map(i => i: java.lang.Integer))
+  }
+
+  //  implicit def charSequenceToString(cs: CharSequence): String = cs.toString
+
+  //  implicit def charSequenceToList(cs: CharSequence): List[Char] = cs.toCharArray.toList
+
+  implicit def mapToJavaMap[A, B](map: Map[A, B]): java.util.Map[A, B] = mapAsJavaMap(map)
+
+  implicit def javaMapToMap[A, B](map: java.util.Map[A, B]): Map[A, B] = mapAsScalaMap(map).toMap
+
+  implicit def iterableToJavaCollection[A](i: Iterable[A]): java.util.Collection[A] = asJavaCollection(i)
+
+  implicit def setToJavaSet[A](set: Set[A]): java.util.Set[A] = setAsJavaSet(set)
+}
+
+import ADAMContext._
+
+class ADAMContext(val sc: SparkContext) extends Serializable with Logging {
+
+  private[rdd] def adamBamDictionaryLoad(filePath: String): SequenceDictionary = {
+    val samHeader = SAMHeaderReader.readSAMHeaderFrom(new Path(filePath), sc.hadoopConfiguration)
+    adamBamDictionaryLoad(samHeader)
+  }
+
+  private[rdd] def adamBamDictionaryLoad(samHeader: SAMFileHeader): SequenceDictionary = {
+    SequenceDictionary(samHeader)
+  }
+
+  private[rdd] def adamBamLoadReadGroups(samHeader: SAMFileHeader): RecordGroupDictionary = {
+    RecordGroupDictionary.fromSAMHeader(samHeader)
+  }
+
+  /**
+   * This method will create a new RDD.
+   * @param filePath The path to the input data
+   * @param predicate An optional pushdown predicate to use when reading the data
+   * @param projection An option projection schema to use when reading the data
+   * @tparam T The type of records to return
+   * @return An RDD with records of the specified type
+   */
+  def loadParquet[T](filePath: String,
+                     predicate: Option[FilterPredicate] = None,
+                     projection: Option[Schema] = None)(implicit ev1: T => SpecificRecord, ev2: Manifest[T]): RDD[T] = {
+    //make sure a type was specified
+    //not using require as to make the message clearer
+    if (manifest[T] == manifest[scala.Nothing])
+      throw new IllegalArgumentException("Type inference failed; when loading please specify a specific type. " +
+        "e.g.:\nval reads: RDD[AlignmentRecord] = ...\nbut not\nval reads = ...\nwithout a return type")
+
+    log.info("Reading the ADAM file at %s to create RDD".format(filePath))
+    val job = HadoopUtil.newJob(sc)
+    ParquetInputFormat.setReadSupportClass(job, classOf[AvroReadSupport[T]])
+
+    if (predicate.isDefined) {
+      log.info("Using the specified push-down predicate")
+      ParquetInputFormat.setFilterPredicate(job.getConfiguration, predicate.get)
+    }
+
+    if (projection.isDefined) {
+      log.info("Using the specified projection schema")
+      AvroParquetInputFormat.setRequestedProjection(job, projection.get)
+    }
+
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[ParquetInputFormat[T]],
+      classOf[Void],
+      manifest[T].runtimeClass.asInstanceOf[Class[T]],
+      ContextUtil.getConfiguration(job)
+    )
+
+    val instrumented = if (Metrics.isRecording) records.instrument() else records
+    val mapped = instrumented.map(p => p._2)
+
+    if (predicate.isDefined) {
+      // Strip the nulls that the predicate returns
+      mapped.filter(p => p != null.asInstanceOf[T])
+    } else {
+      mapped
+    }
+  }
+
+  /**
+   * This method should create a new SequenceDictionary from any parquet file which contains
+   * records that have the requisite reference{Name,Id,Length,Url} fields.
+   *
+   * (If the path is a BAM or SAM file, and the implicit type is an Read, then it just defaults
+   * to reading the SequenceDictionary out of the BAM header in the normal way.)
+   *
+   * @param filePath The path to the input data
+   * @tparam T The type of records to return
+   * @return A sequenceDictionary containing the names and indices of all the sequences to which the records
+   *         in the corresponding file are aligned.
+   */
+  def adamDictionaryLoad[T](filePath: String)(implicit ev1: T => SpecificRecord, ev2: Manifest[T]): SequenceDictionary = {
+
+    // This funkiness is required because (a) ADAMRecords require a different projection from any
+    // other flattened schema, and (b) because the SequenceRecord.fromADAMRecord, below, is going
+    // to be called through a flatMap rather than through a map tranformation on the underlying record RDD.
+    val isADAMRecord = classOf[AlignmentRecord].isAssignableFrom(manifest[T].runtimeClass)
+    val isADAMContig = classOf[NucleotideContigFragment].isAssignableFrom(manifest[T].runtimeClass)
+
+    val projection =
+      if (isADAMRecord) {
+        Projection(
+          AlignmentRecordField.contig,
+          AlignmentRecordField.mateContig,
+          AlignmentRecordField.readPaired,
+          AlignmentRecordField.firstOfPair,
+          AlignmentRecordField.readMapped,
+          AlignmentRecordField.mateMapped)
+      } else if (isADAMContig) {
+        Projection(NucleotideContigFragmentField.contig)
+      } else {
+        Projection(AlignmentRecordField.contig)
+      }
+
+    if (filePath.endsWith(".bam") || filePath.endsWith(".sam")) {
+      if (isADAMRecord)
+        adamBamDictionaryLoad(filePath)
+      else
+        throw new IllegalArgumentException("If you're reading a BAM/SAM file, the record type must be Read")
+
+    } else {
+      val projected: RDD[T] = loadParquet[T](filePath, None, projection = Some(projection))
+
+      val recs: RDD[SequenceRecord] =
+        if (isADAMRecord) {
+          projected.asInstanceOf[RDD[AlignmentRecord]].distinct().flatMap(rec => SequenceRecord.fromADAMRecord(rec))
+        } else if (isADAMContig) {
+          projected.asInstanceOf[RDD[NucleotideContigFragment]].distinct().map(ctg => SequenceRecord.fromADAMContigFragment(ctg))
+        } else {
+          projected.distinct().map(SequenceRecord.fromSpecificRecord(_))
+        }
+
+      val dict = recs.aggregate(SequenceDictionary())(
+        (dict: SequenceDictionary, rec: SequenceRecord) => dict + rec,
+        (dict1: SequenceDictionary, dict2: SequenceDictionary) => dict1 ++ dict2)
+
+      dict
+    }
+  }
+
+  def loadBam(
+    filePath: String): RDD[AlignmentRecord] = {
+
+    val fs = FileSystem.get(sc.hadoopConfiguration)
+    val path = new Path(filePath)
+    val bamFiles = if (fs.isDirectory(path)) fs.listStatus(path) else fs.globStatus(path)
+    val (seqDict, readGroups) = bamFiles
+      .map(fs => fs.getPath)
+      .flatMap(fp => {
+
+        try {
+          // We need to separately read the header, so that we can inject the sequence dictionary
+          // data into each individual Read (see the argument to samRecordConverter.convert,
+          // below).
+          val samHeader = SAMHeaderReader.readSAMHeaderFrom(fp, sc.hadoopConfiguration)
+          log.info("Loaded header from " + fp)
+          val sd = adamBamDictionaryLoad(samHeader)
+          val rg = adamBamLoadReadGroups(samHeader)
+          Some((sd, rg))
+        } catch {
+          case _: Throwable => {
+            log.error("Loading failed for " + fp)
+            None
+          }
+        }
+      }).reduce((kv1, kv2) => {
+        (kv1._1 ++ kv2._1, kv1._2 ++ kv2._2)
+      })
+
+    val job = HadoopUtil.newJob(sc)
+    val records = sc.newAPIHadoopFile(filePath, classOf[AnySAMInputFormat], classOf[LongWritable],
+      classOf[SAMRecordWritable], ContextUtil.getConfiguration(job))
+    if (Metrics.isRecording) records.instrument() else records
+    val samRecordConverter = new SAMRecordConverter
+
+    records.map(p => samRecordConverter.convert(p._2.get, seqDict, readGroups))
+  }
+
+  def loadParquetAlignments(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[AlignmentRecord] = {
+    loadParquet[AlignmentRecord](filePath, predicate, projection)
+  }
+
+  def loadInterleavedFastq(
+    filePath: String): RDD[AlignmentRecord] = {
+
+    val job = HadoopUtil.newJob(sc)
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[InterleavedFastqInputFormat],
+      classOf[Void],
+      classOf[Text],
+      ContextUtil.getConfiguration(job)
+    )
+    if (Metrics.isRecording) records.instrument() else records
+
+    // convert records
+    val fastqRecordConverter = new FastqRecordConverter
+    records.flatMap(fastqRecordConverter.convertPair)
+  }
+
+  def loadUnpairedFastq(
+    filePath: String): RDD[AlignmentRecord] = {
+
+    val job = HadoopUtil.newJob(sc)
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[SingleFastqInputFormat],
+      classOf[Void],
+      classOf[Text],
+      ContextUtil.getConfiguration(job)
+    )
+    if (Metrics.isRecording) records.instrument() else records
+
+    // convert records
+    val fastqRecordConverter = new FastqRecordConverter
+    records.map(fastqRecordConverter.convertRead)
+  }
+
+  def loadVcf(filePath: String, sd: Option[SequenceDictionary]): RDD[VariantContext] = {
+    val job = HadoopUtil.newJob(sc)
+    val vcc = new VariantContextConverter(sd)
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[VCFInputFormat], classOf[LongWritable], classOf[VariantContextWritable],
+      ContextUtil.getConfiguration(job))
+    if (Metrics.isRecording) records.instrument() else records
+
+    records.flatMap(p => vcc.convert(p._2.get))
+  }
+
+  def loadVcf(filePath: String): RDD[htsjdk.variant.variantcontext.VariantContext] = {
+    val job = HadoopUtil.newJob(sc)
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[VCFInputFormat], classOf[LongWritable], classOf[VariantContextWritable],
+      ContextUtil.getConfiguration(job))
+    if (Metrics.isRecording) records.instrument() else records
+
+    records.map(p => p._2.get)
+  }
+
+  def loadParquetGenotypes(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[Genotype] = {
+    loadParquet[Genotype](filePath, predicate, projection)
+  }
+
+  def loadParquetVariants(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[Variant] = {
+    loadParquet[Variant](filePath, predicate, projection)
+  }
+
+  def loadFasta(
+    filePath: String,
+    fragmentLength: Long): RDD[NucleotideContigFragment] = {
+    val fastaData: RDD[(LongWritable, Text)] = sc.newAPIHadoopFile(filePath,
+      classOf[TextInputFormat],
+      classOf[LongWritable],
+      classOf[Text])
+    if (Metrics.isRecording) fastaData.instrument() else fastaData
+
+    val remapData = fastaData.map(kv => (kv._1.get, kv._2.toString))
+
+    FastaConverter(remapData, fragmentLength)
+  }
+
+  def loadParquetFragments(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[NucleotideContigFragment] = {
+    loadParquet[NucleotideContigFragment](filePath, predicate, projection)
+  }
+
+  def loadGTF(filePath: String): RDD[Feature] = {
+    val records = sc.textFile(filePath).flatMap(new GTFParser().parse)
+    if (Metrics.isRecording) records.instrument() else records
+  }
+
+  def loadBED(filePath: String): RDD[Feature] = {
+    val records = sc.textFile(filePath).flatMap(new BEDParser().parse)
+    if (Metrics.isRecording) records.instrument() else records
+  }
+
+  def loadNarrowPeak(filePath: String): RDD[Feature] = {
+    val records = sc.textFile(filePath).flatMap(new NarrowPeakParser().parse)
+    if (Metrics.isRecording) records.instrument() else records
+  }
+
+  def loadIntervalList(filePath: String): RDD[Feature] = {
+    val parsedLines = sc.textFile(filePath).map(new IntervalListParser().parse)
+    val (seqDict, records) = (SequenceDictionary(parsedLines.flatMap(_._1).collect(): _*), parsedLines.flatMap(_._2))
+    val seqDictMap = seqDict.records.map(sr => sr.name -> sr).toMap
+    val recordsWithContigs = for {
+      record <- records
+      seqRecord <- seqDictMap.get(record.getContig.getContigName)
+    } yield Feature.newBuilder(record)
+      .setContig(
+        Contig.newBuilder()
+          .setContigName(seqRecord.name)
+          .setReferenceURL(seqRecord.url.getOrElse(null))
+          .setContigMD5(seqRecord.md5.getOrElse(null))
+          .setContigLength(seqRecord.length)
+          .build()
+      )
+      .build()
+    if (Metrics.isRecording) recordsWithContigs.instrument() else recordsWithContigs
+  }
+
+  def loadParquetFeatures(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[Feature] = {
+    loadParquet[Feature](filePath, predicate, projection)
+  }
+
+  def loadVcfAnnotations(
+    filePath: String,
+    sd: Option[SequenceDictionary] = None): RDD[DatabaseVariantAnnotation] = {
+
+    val job = HadoopUtil.newJob(sc)
+    val vcc = new VariantContextConverter(sd)
+    val records = sc.newAPIHadoopFile(
+      filePath,
+      classOf[VCFInputFormat], classOf[LongWritable], classOf[VariantContextWritable],
+      ContextUtil.getConfiguration(job))
+    if (Metrics.isRecording) records.instrument() else records
+
+    records.map(p => vcc.convertToAnnotation(p._2.get))
+  }
+
+  def loadParquetVariantAnnotations(
+    filePath: String,
+    predicate: Option[FilterPredicate] = None,
+    projection: Option[Schema] = None): RDD[DatabaseVariantAnnotation] = {
+    loadParquet[DatabaseVariantAnnotation](filePath, predicate, projection)
+  }
+
+  def loadVariantAnnotations(
+    filePath: String,
+    projection: Option[Schema] = None,
+    sd: Option[SequenceDictionary] = None): RDD[DatabaseVariantAnnotation] = {
+    if (filePath.endsWith(".vcf")) {
+      log.info("Loading " + filePath + " as VCF, and converting to variant annotations. Projection is ignored.")
+      loadVcfAnnotations(filePath, sd)
+    } else {
+      log.info("Loading " + filePath + " as Parquet containing DatabaseVariantAnnotations.")
+      sd.foreach(sd => log.warn("Sequence dictionary for translation ignored if loading ADAM from Parquet."))
+      loadParquetVariantAnnotations(filePath, None, projection)
+    }
+  }
+
+  def loadFeatures(
+    filePath: String,
+    projection: Option[Schema] = None): RDD[Feature] = {
+
+    if (filePath.endsWith(".bed")) {
+      log.info(s"Loading $filePath as BED and converting to features. Projection is ignored.")
+      loadBED(filePath)
+    } else if (filePath.endsWith(".gtf") ||
+      filePath.endsWith(".gff")) {
+      log.info(s"Loading $filePath as GTF/GFF and converting to features. Projection is ignored.")
+      loadGTF(filePath)
+    } else if (filePath.endsWith(".narrowPeak") ||
+      filePath.endsWith(".narrowpeak")) {
+      log.info(s"Loading $filePath as NarrowPeak and converting to features. Projection is ignored.")
+      loadNarrowPeak(filePath)
+    } else if (filePath.endsWith(".interval_list")) {
+      log.info(s"Loading $filePath as IntervalList and converting to features. Projection is ignored.")
+      loadIntervalList(filePath)
+    } else {
+      log.info(s"Loading $filePath as Parquet containing Features.")
+      loadParquetFeatures(filePath, None, projection)
+    }
+  }
+
+  def loadGenes(filePath: String,
+                projection: Option[Schema] = None): RDD[Gene] = {
+    import ADAMContext._
+    loadFeatures(filePath, projection).asGenes()
+  }
+
+  def loadSequence(
+    filePath: String,
+    projection: Option[Schema] = None,
+    fragmentLength: Long = 10000): RDD[NucleotideContigFragment] = {
+    if (filePath.endsWith(".fa") ||
+      filePath.endsWith(".fasta")) {
+      log.info("Loading " + filePath + " as FASTA and converting to NucleotideContigFragment. Projection is ignored.")
+      loadFasta(filePath,
+        fragmentLength)
+    } else {
+      log.info("Loading " + filePath + " as Parquet containing NucleotideContigFragments.")
+      loadParquetFragments(filePath, None, projection)
+    }
+  }
+
+  def loadGenotypes(
+    filePath: String,
+    projection: Option[Schema] = None,
+    sd: Option[SequenceDictionary] = None): RDD[Genotype] = {
+    if (filePath.endsWith(".vcf")) {
+      log.info("Loading " + filePath + " as VCF, and converting to Genotypes. Projection is ignored.")
+      loadVcf(filePath, sd).flatMap(_.genotypes)
+    } else {
+      log.info("Loading " + filePath + " as Parquet containing Genotypes. Sequence dictionary for translation is ignored.")
+      loadParquetGenotypes(filePath, None, projection)
+    }
+  }
+
+  def loadVariants(
+    filePath: String,
+    projection: Option[Schema] = None,
+    sd: Option[SequenceDictionary] = None): RDD[Variant] = {
+    if (filePath.endsWith(".vcf")) {
+      log.info("Loading " + filePath + " as VCF, and converting to Variants. Projection is ignored.")
+      loadVcf(filePath, sd).map(_.variant.variant)
+    } else {
+      log.info("Loading " + filePath + " as Parquet containing Variants. Sequence dictionary for translation is ignored.")
+      loadParquetVariants(filePath, None, projection)
+    }
+  }
+
+  def loadAlignments(
+    filePath: String,
+    projection: Option[Schema] = None): RDD[AlignmentRecord] = LoadAlignmentRecords.time {
+
+    if (filePath.endsWith(".sam") ||
+      filePath.endsWith(".bam")) {
+      log.info("Loading " + filePath + " as SAM/BAM and converting to AlignmentRecords. Projection is ignored.")
+      loadBam(filePath)
+    } else if (filePath.endsWith(".ifq")) {
+      log.info("Loading " + filePath + " as interleaved FASTQ and converting to AlignmentRecords. Projection is ignored.")
+      loadInterleavedFastq(filePath)
+    } else if (filePath.endsWith(".fq") ||
+      filePath.endsWith(".fastq")) {
+      log.info("Loading " + filePath + " as unpaired FASTQ and converting to AlignmentRecords. Projection is ignored.")
+      loadUnpairedFastq(filePath)
+    } else if (filePath.endsWith(".fa") ||
+      filePath.endsWith(".fasta")) {
+      log.info("Loading " + filePath + " as FASTA and converting to AlignmentRecords. Projection is ignored.")
+      import ADAMContext._
+      loadFasta(filePath, fragmentLength = 10000).toReads
+    } else if (filePath.endsWith("contig.adam")) {
+      log.info("Loading " + filePath + " as Parquet of NucleotideContigFragment and converting to AlignmentRecords. Projection is ignored.")
+      loadParquet[NucleotideContigFragment](filePath).toReads
+    } else {
+      log.info("Loading " + filePath + " as Parquet of AlignmentRecords.")
+      loadParquetAlignments(filePath, None, projection)
+    }
+  }
+
+  /**
+   * Takes a sequence of Path objects (e.g. the return value of findFiles).  Treats each path as
+   * corresponding to a Read set -- loads each Read set, converts each set to use the
+   * same SequenceDictionary, and returns the union of the RDDs.
+   *
+   * @param paths The locations of the parquet files to load
+   * @return a single RDD[Read] that contains the union of the AlignmentRecords in the argument paths.
+   */
+  def loadAlignmentsFromPaths(paths: Seq[Path]): RDD[AlignmentRecord] = {
+    sc.union(paths.map(p => loadAlignments(p.toString)))
+  }
+
+  /**
+   * Searches a path recursively, returning the names of all directories in the tree whose
+   * name matches the given regex.
+   *
+   * @param path The path to begin the search at
+   * @param regex A regular expression
+   * @return A sequence of Path objects corresponding to the identified directories.
+   */
+  def findFiles(path: Path, regex: String): Seq[Path] = {
+    if (regex == null) {
+      Seq(path)
+    } else {
+      val statuses = FileSystem.get(sc.hadoopConfiguration).listStatus(path)
+      val r = Pattern.compile(regex)
+      val (matches, recurse) = statuses.filter(HadoopUtil.isDirectory).map(s => s.getPath).partition(p => r.matcher(p.getName).matches())
+      matches.toSeq ++ recurse.flatMap(p => findFiles(p, regex))
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ADAMRDDFunctions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ADAMRDDFunctions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ADAMRDDFunctions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ADAMRDDFunctions.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,147 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import java.util.logging.Level
+import org.apache.avro.Schema
+import org.apache.avro.generic.IndexedRecord
+import org.apache.hadoop.mapreduce.{ OutputFormat => NewOutputFormat, _ }
+import org.apache.spark.Logging
+import org.apache.spark.rdd.{ InstrumentedOutputFormat, RDD }
+import org.apache.spark.rdd.MetricsContext._
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.util.ParquetLogger
+import org.bdgenomics.utils.cli.SaveArgs
+import org.bdgenomics.utils.misc.HadoopUtil
+import parquet.avro.AvroParquetOutputFormat
+import parquet.hadoop.ParquetOutputFormat
+import parquet.hadoop.metadata.CompressionCodecName
+import parquet.hadoop.util.ContextUtil
+
+trait ADAMSaveAnyArgs extends SaveArgs {
+  var sortFastqOutput: Boolean
+}
+
+class ADAMRDDFunctions[T <% IndexedRecord: Manifest](rdd: RDD[T]) extends Serializable with Logging {
+
+  def adamParquetSave(args: SaveArgs): Unit = {
+    adamParquetSave(
+      args.outputPath,
+      args.blockSize,
+      args.pageSize,
+      args.compressionCodec,
+      args.disableDictionaryEncoding
+    )
+  }
+
+  def adamParquetSave(filePath: String,
+                      blockSize: Int = 128 * 1024 * 1024,
+                      pageSize: Int = 1 * 1024 * 1024,
+                      compressCodec: CompressionCodecName = CompressionCodecName.GZIP,
+                      disableDictionaryEncoding: Boolean = false,
+                      schema: Option[Schema] = None): Unit = SaveAsADAM.time {
+    log.info("Saving data in ADAM format")
+
+    val job = HadoopUtil.newJob(rdd.context)
+    ParquetLogger.hadoopLoggerLevel(Level.SEVERE)
+    ParquetOutputFormat.setCompression(job, compressCodec)
+    ParquetOutputFormat.setEnableDictionary(job, !disableDictionaryEncoding)
+    ParquetOutputFormat.setBlockSize(job, blockSize)
+    ParquetOutputFormat.setPageSize(job, pageSize)
+    AvroParquetOutputFormat.setSchema(job,
+      if (schema.isDefined) schema.get
+      else manifest[T].runtimeClass.asInstanceOf[Class[T]].newInstance().getSchema)
+    // Add the Void Key
+    val recordToSave = rdd.map(p => (null, p))
+    // Save the values to the ADAM/Parquet file
+    recordToSave.saveAsNewAPIHadoopFile(filePath,
+      classOf[java.lang.Void], manifest[T].runtimeClass.asInstanceOf[Class[T]], classOf[InstrumentedADAMAvroParquetOutputFormat],
+      ContextUtil.getConfiguration(job))
+  }
+
+}
+
+/**
+ * A class that provides functions to recover a sequence dictionary from a generic RDD of records.
+ *
+ * @tparam T Type contained in this RDD.
+ * @param rdd RDD over which aggregation is supported.
+ */
+abstract class ADAMSequenceDictionaryRDDAggregator[T](rdd: RDD[T]) extends Serializable with Logging {
+  /**
+   * For a single RDD element, returns 0+ sequence record elements.
+   *
+   * @param elem Element from which to extract sequence records.
+   * @return A seq of sequence records.
+   */
+  def getSequenceRecordsFromElement(elem: T): scala.collection.Set[SequenceRecord]
+
+  /**
+   * Aggregates together a sequence dictionary from the different individual reference sequences
+   * used in this dataset.
+   *
+   * @return A sequence dictionary describing the reference contigs in this dataset.
+   */
+  def adamGetSequenceDictionary(): SequenceDictionary = {
+    def mergeRecords(l: List[SequenceRecord], rec: T): List[SequenceRecord] = {
+      val recs = getSequenceRecordsFromElement(rec)
+
+      recs.foldLeft(l)((li: List[SequenceRecord], r: SequenceRecord) => {
+        if (!li.contains(r)) {
+          r :: li
+        } else {
+          li
+        }
+      })
+    }
+
+    def foldIterator(iter: Iterator[T]): SequenceDictionary = {
+      val recs = iter.foldLeft(List[SequenceRecord]())(mergeRecords)
+      SequenceDictionary(recs: _*)
+    }
+
+    rdd.mapPartitions(iter => Iterator(foldIterator(iter)), preservesPartitioning = true)
+      .reduce(_ ++ _)
+  }
+
+}
+
+/**
+ * A class that provides functions to recover a sequence dictionary from a generic RDD of records
+ * that are defined in Avro. This class assumes that the reference identification fields are
+ * defined inside of the given type.
+ *
+ * @note Avro classes that have specific constraints around sequence dictionary contents should
+ * not use this class. Examples include ADAMRecords and ADAMNucleotideContigs
+ *
+ * @tparam T A type defined in Avro that contains the reference identification fields.
+ * @param rdd RDD over which aggregation is supported.
+ */
+class ADAMSpecificRecordSequenceDictionaryRDDAggregator[T <% IndexedRecord: Manifest](rdd: RDD[T])
+    extends ADAMSequenceDictionaryRDDAggregator[T](rdd) {
+
+  def getSequenceRecordsFromElement(elem: T): Set[SequenceRecord] = {
+    Set(SequenceRecord.fromSpecificRecord(elem))
+  }
+}
+
+class InstrumentedADAMAvroParquetOutputFormat extends InstrumentedOutputFormat[Void, IndexedRecord] {
+  override def outputFormatClass(): Class[_ <: NewOutputFormat[Void, IndexedRecord]] = classOf[AvroParquetOutputFormat]
+  override def timerName(): String = WriteADAMRecord.timerName
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/BroadcastRegionJoin.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/BroadcastRegionJoin.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/BroadcastRegionJoin.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/BroadcastRegionJoin.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,350 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.bdgenomics.adam.models.{ SequenceDictionary, ReferenceRegion }
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext._
+import scala.Predef._
+import org.apache.spark.SparkContext
+import scala.reflect.ClassTag
+
+/**
+ * Contains multiple implementations of a 'region join', an operation that joins two sets of
+ * regions based on the spatial overlap between the regions.
+ *
+ * Different implementations will have different performance characteristics -- and new implementations
+ * will likely be added in the future, see the notes to each individual method for more details.
+ */
+object BroadcastRegionJoin extends RegionJoin {
+
+  /**
+   * Performs a region join between two RDDs (broadcast join).
+   *
+   * This implementation first _collects_ the left-side RDD; therefore, if the left-side RDD is large
+   * or otherwise idiosyncratic in a spatial sense (i.e. contains a set of regions whose unions overlap
+   * a significant fraction of the genome) then the performance of this implementation will likely be
+   * quite bad.
+   *
+   * Once the left-side RDD is collected, its elements are reduced to their distinct unions;
+   * these can then be used to define the partitions over which the region-join will be computed.
+   *
+   * The regions in the left-side are keyed by their corresponding partition (each such region should have
+   * exactly one partition).  The regions in the right-side are also keyed by their corresponding partitions
+   * (here there can be more than one partition for a region, since a region may cross the boundaries of
+   * the partitions defined by the left-side).
+   *
+   * Finally, within each separate partition, we essentially perform a cartesian-product-and-filter
+   * operation.  The result is the region-join.
+   *
+   * @param baseRDD The 'left' side of the join
+   * @param joinedRDD The 'right' side of the join
+   * @param tManifest implicit type of baseRDD
+   * @param uManifest implicit type of joinedRDD
+   * @tparam T type of baseRDD
+   * @tparam U type of joinedRDD
+   * @return An RDD of pairs (x, y), where x is from baseRDD, y is from joinedRDD, and the region
+   *         corresponding to x overlaps the region corresponding to y.
+   */
+  def partitionAndJoin[T, U](baseRDD: RDD[(ReferenceRegion, T)],
+                             joinedRDD: RDD[(ReferenceRegion, U)])(implicit tManifest: ClassTag[T],
+                                                                   uManifest: ClassTag[U]): RDD[(T, U)] = {
+
+    val sc = baseRDD.context
+
+    /**
+     * Original Join Design:
+     *
+     * Parameters:
+     *   (1) f : (Range, Range) => T  // an aggregation function
+     *   (2) a : RDD[Range]
+     *   (3) b : RDD[Range]
+     *
+     * Return type: RDD[(Range,T)]
+     *
+     * Algorithm:
+     *   1. a.collect() (where a is smaller than b)
+     *   2. build a non-overlapping partition on a
+     *   3. ak = a.map( v => (partition(v), v) )
+     *   4. bk = b.flatMap( v => partitions(v).map( i=>(i,v) ) )
+     *   5. joined = ak.join(bk).filter( (i, (r1, r2)) => r1.overlaps(r2) ).map( (i, (r1,r2))=>(r1, r2) )
+     *   6. return: joined.reduceByKey(f)
+     *
+     * Ways in which we've generalized this plan:
+     * - removed the aggregation step altogether
+     * - carry a sequence dictionary through the computation.
+     */
+
+    // First, we group the regions in the left side of the join by their referenceName,
+    // and collect them.
+    val collectedLeft: Seq[(String, Iterable[ReferenceRegion])] =
+      baseRDD
+        .map(kv => (kv._1.referenceName, kv._1)) // RDD[(String,ReferenceRegion)]
+        .groupBy(_._1) // RDD[(String,Seq[(String,ReferenceRegion)])]
+        .mapValues(_.map(_._2)) // RDD[(String,Seq[ReferenceRegion])]
+        .collect() // Iterable[(String,Seq[ReferenceRegion])]
+        .toSeq // Seq[(String,Seq[ReferenceRegion])]
+
+    // Next, we turn that into a data structure that reduces those regions to their non-overlapping
+    // pieces, which we will use as a partition.
+    val multiNonOverlapping = new MultiContigNonoverlappingRegions(collectedLeft)
+
+    // Then, we broadcast those partitions -- this will be the function that allows us to
+    // partition all the regions on the right side of the join.
+    val regions = sc.broadcast(multiNonOverlapping)
+
+    // each element of the left-side RDD should have exactly one partition.
+    val smallerKeyed: RDD[(ReferenceRegion, (ReferenceRegion, T))] =
+      baseRDD.map(t => (regions.value.regionsFor(t).head, t))
+
+    // each element of the right-side RDD may have 0, 1, or more than 1 corresponding partition.
+    val largerKeyed: RDD[(ReferenceRegion, (ReferenceRegion, U))] =
+      joinedRDD.filter(regions.value.filter(_))
+        .flatMap(t => regions.value.regionsFor(t).map((r: ReferenceRegion) => (r, t)))
+
+    // this is (essentially) performing a cartesian product within each partition...
+    val joined: RDD[(ReferenceRegion, ((ReferenceRegion, T), (ReferenceRegion, U)))] =
+      smallerKeyed.join(largerKeyed)
+
+    // ... so we need to filter the final pairs to make sure they're overlapping.
+    val filtered: RDD[(ReferenceRegion, ((ReferenceRegion, T), (ReferenceRegion, U)))] = joined.filter(kv => {
+      val (rr: ReferenceRegion, (t: (ReferenceRegion, T), u: (ReferenceRegion, U))) = kv
+      t._1.overlaps(u._1)
+    })
+
+    // finally, erase the partition key and return the result.
+    filtered.map(rrtu => (rrtu._2._1._2, rrtu._2._2._2))
+  }
+
+  /**
+   * This method does a join between different types which can have a corresponding ReferenceMapping.
+   *
+   * This method does a cartesian product between the two, then removes mismatched regions.
+   *
+   * This is SLOW SLOW SLOW, and shouldn't be used for anything other than correctness-testing on
+   * realistic sized sets.
+   *
+   */
+  def cartesianFilter[T, U](baseRDD: RDD[(ReferenceRegion, T)],
+                            joinedRDD: RDD[(ReferenceRegion, U)])(implicit tManifest: ClassTag[T],
+                                                                  uManifest: ClassTag[U]): RDD[(T, U)] = {
+    baseRDD.cartesian(joinedRDD).filter({
+      case (t: (ReferenceRegion, T), u: (ReferenceRegion, U)) =>
+        t._1.overlaps(u._1)
+    }).map(p => (p._1._2, p._2._2))
+  }
+}
+
+/**
+ * The evaluation of a regionJoin takes place with respect to a complete partition on the total space
+ * of the genome.  NonoverlappingRegions is a class to compute the value of that partition, and to allow
+ * us to assign one or more elements of that partition to a new ReferenceRegion (see the 'regionsFor' method).
+ *
+ * NonoverlappingRegions takes, as input, and 'input-set' of regions.  These are arbitrary ReferenceRegions,
+ * which may be overlapping, identical, disjoint, etc.  The input-set of regions _must_ all be located on
+ * the same reference chromosome (i.e. must all have the same refName); the generalization to reference
+ * regions from multiple chromosomes is in MultiContigNonoverlappingRegions, below.
+ *
+ * NonoverlappingRegions produces, internally, a 'nonoverlapping-set' of regions.  This is basically
+ * the set of _distinct unions_ of the input-set regions.
+ *
+ * @param regions The input-set of regions.
+ */
+private[rdd] class NonoverlappingRegions(regions: Iterable[ReferenceRegion]) extends Serializable {
+
+  assert(regions != null, "regions parameter cannot be null")
+
+  // The regions Seq needs to be of non-zero size, since otherwise we have to add special-case
+  // checks to all the methods below to make sure that 'endpoints' isn't empty.  Also, it shouldn't
+  // make any sense to have a set of non-overlapping regions for ... no regions.
+  // Also, without this check, we can't tell which chromosome this NonoverlappingRegions object is for.
+  assert(regions.size > 0, "regions list must be non-empty")
+  assert(regions.head != null, "regions must have at least one non-null entry")
+
+  val referenceName: String = regions.head.referenceName
+
+  // invariant: all the values in the 'regions' list have the same referenceId
+  assert(regions.forall(_.referenceName == referenceName))
+
+  // We represent the distinct unions, the 'nonoverlapping-set' of regions, as a set of endpoints,
+  // so that we can do reasonably-fast binary searching on them to determine the slice of nonoverlapping-set
+  // regions that are overlapped by a new, query region (see findOverlappingRegions, below).
+  val endpoints: Array[Long] =
+    mergeRegions(regions.toSeq.sortBy(r => r.start)).flatMap(r => Seq(r.start, r.end)).distinct.sorted.toArray
+
+  private def updateListWithRegion(list: List[ReferenceRegion], newRegion: ReferenceRegion): List[ReferenceRegion] = {
+    list match {
+      case head :: tail =>
+
+        // using overlaps || isAdjacent is an important feature!  it means that
+        // we can use the "alternating" optimization, described below, which reduces
+        // the number of regions returned as keys (by a factor of 2) and therefore
+        // the number of partitions that need to be examined during a regionJoin.
+        if (head.overlaps(newRegion) || head.isAdjacent(newRegion)) {
+          head.hull(newRegion) :: tail
+        } else {
+          newRegion :: list
+        }
+      case _ => List(newRegion)
+    }
+  }
+
+  def mergeRegions(regs: Seq[(ReferenceRegion)]): List[ReferenceRegion] =
+    regs.aggregate(List[ReferenceRegion]())(
+      (lst: List[ReferenceRegion], p: (ReferenceRegion)) => updateListWithRegion(lst, p),
+      (a, b) => a ++ b)
+
+  def binaryPointSearch(pos: Long, lessThan: Boolean): Int = {
+    var i = 0
+    var j = endpoints.size - 1
+
+    while (j - i > 1) {
+      val ij2 = (i + j) / 2
+      val mid = endpoints(ij2)
+      if (mid < pos) {
+        i = ij2
+      } else {
+        j = ij2
+      }
+    }
+
+    if (lessThan) i else j
+  }
+
+  def findOverlappingRegions(query: ReferenceRegion): Seq[ReferenceRegion] = {
+
+    assert(query != null, "query region was null")
+    assert(endpoints != null, "endpoints field was null")
+
+    if (query.end <= endpoints.head || query.start >= endpoints.last) {
+      Seq()
+    } else {
+      val firsti = binaryPointSearch(query.start, lessThan = true)
+      val lasti = binaryPointSearch(query.end, lessThan = false)
+
+      // Slice is an inclusive start, exclusive end operation
+      val firstRegionIsHit = firsti % 2 == 0
+
+      val startSlice = endpoints.slice(firsti, lasti)
+      val endSlice = endpoints.slice(firsti + 1, lasti + 1)
+
+      /*
+       * The use of NonoverlappingRegions.alternating is an important optimization --
+       * basically, because we used "overlaps || isAdjacent" as the predicate for 
+       * when to join two regions in the input-set into a single nonoverlapping-region, above,
+       * then we know that the set of nonoverlapping-regions defined by the points in 'endpoints'
+       * are "alternating." In other words, we know that each nonoverlapping-region
+       * defined by an overlapping set of input-regions (in the constructor) is followed by
+       * a nonoverlapping-region which had _no_ input-set regions within it, and vice-versa.
+       *
+       * And _this_ is important because it means that, here, we only need to return
+       * _half_ the regions we otherwise would have -- because this is being used for a
+       * regionJoin, we don't need to return the 'empty' regions since we know a priori
+       * that these don't correspond to any region the input-set, and therefore
+       * will never result in any pairs in the ultimate join result.
+       */
+      NonoverlappingRegions.alternating(startSlice.zip(endSlice).map {
+        case (start, end) =>
+          ReferenceRegion(referenceName, start, end)
+      }.toSeq, firstRegionIsHit)
+    }
+  }
+
+  /**
+   * Given a "regionable" value (corresponds to a ReferencRegion through an implicit ReferenceMapping),
+   * return the set of nonoverlapping-regions to be used as partitions for the input value in a
+   * region-join.  Basically, return the set of any non-empty nonoverlapping-regions that overlap the
+   * region corresponding to this input.
+   *
+   * @param regionable The input, which corresponds to a region
+   * @param mapping The implicit mapping to turn the input into a region
+   * @tparam U The type of the input
+   * @return An Iterable[ReferenceRegion], where each element of the Iterable is a nonoverlapping-region
+   *         defined by 1 or more input-set regions.
+   */
+  def regionsFor[U](regionable: (ReferenceRegion, U)): Iterable[ReferenceRegion] =
+    findOverlappingRegions(regionable._1)
+
+  /**
+   * A quick filter, to find out if we even need to examine a particular input value for keying by
+   * nonoverlapping-regions.  Basically, reject the input value if its corresponding region is
+   * completely outside the hull of all the input-set regions.
+   *
+   * @param regionable The input value
+   * @param mapping an implicity mapping of the input value to a ReferenceRegion
+   * @tparam U
+   * @return a boolean -- the input value should only participate in the regionJoin if the return value
+   *         here is 'true'.
+   */
+  def hasRegionsFor[U](regionable: (ReferenceRegion, U)): Boolean = {
+    !(regionable._1.end <= endpoints.head || regionable._1.start >= endpoints.last)
+  }
+
+  override def toString: String =
+    "%s:%d-%d (%s)".format(referenceName, endpoints.head, endpoints.last, endpoints.mkString(","))
+}
+
+private[rdd] object NonoverlappingRegions {
+
+  def apply[T](values: Seq[(ReferenceRegion, T)]) =
+    new NonoverlappingRegions(values.map(_._1))
+
+  def alternating[T](seq: Seq[T], includeFirst: Boolean): Seq[T] = {
+    val inds = if (includeFirst) { 0 until seq.size } else { 1 until seq.size + 1 }
+    seq.zip(inds).filter(p => p._2 % 2 == 0).map(_._1)
+  }
+
+}
+
+/**
+ * Creates a multi-reference-region collection of NonoverlappingRegions -- see
+ * the scaladocs to NonoverlappingRegions.
+ *
+ * @param regions A Seq of ReferencRegions, pre-partitioned by their
+ *                referenceNames.  So, for a given pair (x, regs) in
+ *                this Seq, all regions R in regs must satisfy
+ *                R.referenceName == x.  Furthermore, all the x's must
+ *                be valid reference names with respect to the sequence
+ *                dictionary.
+ */
+private[rdd] class MultiContigNonoverlappingRegions(
+    regions: Seq[(String, Iterable[ReferenceRegion])]) extends Serializable {
+
+  assert(regions != null,
+    "Regions was set to null")
+
+  val regionMap: Map[String, NonoverlappingRegions] =
+    Map(regions.map(r => (r._1, new NonoverlappingRegions(r._2))): _*)
+
+  def regionsFor[U](regionable: (ReferenceRegion, U)): Iterable[ReferenceRegion] =
+    regionMap.get(regionable._1.referenceName).fold(Iterable[ReferenceRegion]())(_.regionsFor(regionable))
+
+  def filter[U](value: (ReferenceRegion, U)): Boolean =
+    regionMap.get(value._1.referenceName).fold(false)(_.hasRegionsFor(value))
+}
+
+private[rdd] object MultiContigNonoverlappingRegions {
+  def apply[T](values: Seq[(ReferenceRegion, T)]): MultiContigNonoverlappingRegions = {
+    new MultiContigNonoverlappingRegions(
+      values.map(kv => (kv._1.referenceName, kv._1))
+        .groupBy(t => t._1)
+        .map(t => (t._1, t._2.map(k => k._2)))
+        .toSeq)
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/contig/FlankReferenceFragments.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/contig/FlankReferenceFragments.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/contig/FlankReferenceFragments.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/contig/FlankReferenceFragments.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,72 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.contig
+
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.{ ReferenceRegion, SequenceDictionary }
+import org.bdgenomics.adam.rdd.ReferencePartitioner
+import org.bdgenomics.formats.avro.NucleotideContigFragment
+
+private[contig] object FlankReferenceFragments extends Serializable {
+
+  def apply(rdd: RDD[NucleotideContigFragment],
+            sd: SequenceDictionary,
+            flankSize: Int): RDD[NucleotideContigFragment] = {
+    rdd.keyBy(ctg => ReferenceRegion(ctg).get)
+      .repartitionAndSortWithinPartitions(ReferencePartitioner(sd))
+      .mapPartitions(flank(_, flankSize))
+  }
+
+  def flank(iter: Iterator[(ReferenceRegion, NucleotideContigFragment)],
+            flankSize: Int): Iterator[NucleotideContigFragment] = {
+    // we need to have at least one element in the iterator
+    if (iter.hasNext) {
+      // now, we apply a window and flank adjacent segments
+      var lastFragment = iter.next
+      iter.map(f => {
+        // grab temp copy; we will overwrite later
+        val copyLastFragment = lastFragment
+
+        // are the two fragments adjacent? if so, we must add the flanking sequences
+        if (copyLastFragment._1.isAdjacent(f._1)) {
+          val lastSequence = copyLastFragment._2.getFragmentSequence
+          val currSequence = f._2.getFragmentSequence
+
+          // update fragments with flanking sequences
+          copyLastFragment._2.setFragmentSequence(lastSequence + currSequence.take(flankSize))
+          copyLastFragment._2.setDescription(Option(copyLastFragment._2.getDescription)
+            .fold("rr")(_ + "rr"))
+          f._2.setFragmentSequence(lastSequence.takeRight(flankSize) + currSequence)
+          f._2.setDescription("f")
+
+          // we must change the start position of the fragment we are appending in front of
+          f._2.setFragmentStartPosition(f._2.getFragmentStartPosition - flankSize.toLong)
+        }
+
+        // overwrite last fragment
+        lastFragment = f
+
+        // emit updated last fragment
+        copyLastFragment._2
+      }) ++ Iterator(lastFragment._2)
+    } else {
+      Iterator()
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/contig/NucleotideContigFragmentRDDFunctions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/contig/NucleotideContigFragmentRDDFunctions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/contig/NucleotideContigFragmentRDDFunctions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/contig/NucleotideContigFragmentRDDFunctions.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,151 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.contig
+
+import java.util.logging.Level
+import org.apache.avro.specific.SpecificRecord
+import org.apache.spark.Logging
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.converters.FragmentConverter
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rdd.ADAMSequenceDictionaryRDDAggregator
+import org.bdgenomics.adam.util.ParquetLogger
+import org.bdgenomics.formats.avro._
+import org.bdgenomics.utils.misc.HadoopUtil
+import parquet.avro.AvroParquetOutputFormat
+import parquet.hadoop.ParquetOutputFormat
+import parquet.hadoop.metadata.CompressionCodecName
+import parquet.hadoop.util.ContextUtil
+import scala.math.max
+import scala.Some
+
+class NucleotideContigFragmentRDDFunctions(rdd: RDD[NucleotideContigFragment]) extends ADAMSequenceDictionaryRDDAggregator[NucleotideContigFragment](rdd) {
+
+  /**
+   * Converts an RDD of nucleotide contig fragments into reads. Adjacent contig fragments are
+   * combined.
+   *
+   * @return Returns an RDD of reads.
+   */
+  def toReads(): RDD[AlignmentRecord] = {
+    FragmentConverter.convertRdd(rdd)
+  }
+
+  /**
+   * Converts an RDD of nucleotide contig fragments into Rich nucleotide contig fragments
+   * @return Returns an RDD of Rich nucleotide contig fragments
+   */
+  /*
+  def toRichNucleotideContigFragment(records: RDD[AlignmentRecord], fragmentLength: Long): RDD[RichNucleotideContigFragment] = {
+    FragmentConverter.convertRich(rdd, records, fragmentLength)
+  }
+  */
+
+  /**
+   * From a set of contigs, returns the base sequence that corresponds to a region of the reference.
+   *
+   * @throws UnsupportedOperationException Throws exception if query region is not found.
+   *
+   * @param region Reference region over which to get sequence.
+   * @return String of bases corresponding to reference sequence.
+   */
+  def adamGetReferenceString(region: ReferenceRegion): String = {
+    def getString(fragment: (ReferenceRegion, NucleotideContigFragment)): (ReferenceRegion, String) = {
+      val trimStart = max(0, region.start - fragment._1.start).toInt
+      val trimEnd = max(0, fragment._1.end - region.end).toInt
+
+      val fragmentSequence: String = fragment._2.getFragmentSequence
+
+      val str = fragmentSequence.drop(trimStart)
+        .dropRight(trimEnd)
+      val reg = new ReferenceRegion(fragment._1.referenceName,
+        fragment._1.start + trimStart,
+        fragment._1.end - trimEnd)
+      (reg, str)
+    }
+
+    def reducePairs(kv1: (ReferenceRegion, String),
+                    kv2: (ReferenceRegion, String)): (ReferenceRegion, String) = {
+      assert(kv1._1.isAdjacent(kv2._1), "Regions being joined must be adjacent. For: " +
+        kv1 + ", " + kv2)
+
+      (kv1._1.merge(kv2._1), if (kv1._1.compareTo(kv2._1) <= 0) {
+        kv1._2 + kv2._2
+      } else {
+        kv2._2 + kv1._2
+      })
+    }
+
+    try {
+      val pair: (ReferenceRegion, String) = rdd.keyBy(ReferenceRegion(_))
+        .filter(kv => kv._1.isDefined)
+        .map(kv => (kv._1.get, kv._2))
+        .filter(kv => kv._1.overlaps(region))
+        .sortByKey()
+        .map(kv => getString(kv))
+        .reduce(reducePairs)
+
+      assert(pair._1.compareTo(region) == 0,
+        "Merging fragments returned a different region than requested.")
+
+      pair._2
+    } catch {
+      case (uoe: UnsupportedOperationException) =>
+        throw new UnsupportedOperationException("Could not find " + region + "in reference RDD.")
+    }
+  }
+
+  def getSequenceRecordsFromElement(elem: NucleotideContigFragment): Set[SequenceRecord] = {
+    // variant context contains a single locus
+    Set(SequenceRecord.fromADAMContigFragment(elem))
+  }
+
+  /**
+   * For all adjacent records in the RDD, we extend the records so that the adjacent
+   * records now overlap by _n_ bases, where _n_ is the flank length.
+   *
+   * @param flankLength The length to extend adjacent records by.
+   * @param optSd An optional sequence dictionary. If none is provided, we recompute the
+   *              sequence dictionary on the fly. Default is None.
+   * @return Returns the RDD, with all adjacent fragments extended with flanking sequence.
+   */
+  def flankAdjacentFragments(flankLength: Int,
+                             optSd: Option[SequenceDictionary] = None): RDD[NucleotideContigFragment] = {
+    FlankReferenceFragments(rdd, optSd.getOrElse(adamGetSequenceDictionary), flankLength)
+  }
+
+  /**
+   * Counts the k-mers contained in a FASTA contig.
+   *
+   * @param kmerLength The length of k-mers to count.
+   * @param optSd An optional sequence dictionary. If none is provided, we recompute the
+   *              sequence dictionary on the fly. Default is None.
+   * @return Returns an RDD containing k-mer/count pairs.
+   */
+  def countKmers(kmerLength: Int,
+                 optSd: Option[SequenceDictionary] = None): RDD[(String, Long)] = {
+    flankAdjacentFragments(kmerLength, optSd).flatMap(r => {
+      // cut each read into k-mers, and attach a count of 1L
+      r.getFragmentSequence
+        .sliding(kmerLength)
+        .map(k => (k, 1L))
+    }).reduceByKey((k1: Long, k2: Long) => k1 + k2)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/Coverage.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/Coverage.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/Coverage.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/Coverage.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,232 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.apache.spark.SparkContext
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import scala.math._
+import org.bdgenomics.adam.models.{ SequenceDictionary, ReferenceRegion }
+import PairingRDD._
+
+/**
+ * A base is 'covered' by a region set if any region in the set contains the base itself.
+ *
+ * The 'coverage regions' of a region set are the unique, disjoint, non-adjacent,
+ * minimal set of regions which contain every covered base, and no bases which are not covered.
+ *
+ * The Coverage class calculates the coverage regions for a given region set.
+ *
+ * @param window A parameter (which should be a positive number) that determines the parallelism
+ *               which Coverage uses to calculate the coverage regions -- larger window sizes
+ *               indicate less parallelism, but also fewer subsequent passes.
+ */
+class Coverage(val window: Long) extends Serializable {
+
+  require(window > 0)
+
+  type Region = ReferenceRegion
+
+  /**
+   * Calling findCoverageRegions calculates (as an RDD) the coverage regions for a given
+   * RDD of input regions.
+   *
+   * The primary method.
+   *
+   * @param coveringRegions The input regions whose coverage regions are to be calculated
+   * @return an RDD containing the ReferenceRegions corresponding to the coverage regions
+   *         of the input set 'coveringRegions'
+   */
+  def findCoverageRegions(coveringRegions: RDD[ReferenceRegion]): RDD[ReferenceRegion] = {
+
+    // First, map each input region to a window
+    val windowKeyedRegions: RDD[(Region, Region)] = coveringRegions.flatMap(regionToWindows)
+
+    // Then, within each window, calculate the coverage regions.  This complete list
+    // might contain pairs of regions that are adjacent (i.e. adjacent at the window
+    // boundaries), therefore we ...
+    val possiblyAdjacent: RDD[Region] = windowKeyedRegions.groupByKey().flatMap {
+      case (window: Region, cRegions: Iterable[Region]) =>
+        calculateCoverageRegions(cRegions)
+    }
+
+    // ... collapse the adjacent regions down into single contiguous regions.
+    collapseAdjacent(possiblyAdjacent)
+  }
+
+  /**
+   * Uses the fixed window-width to key each Region by the corresponding window Region
+   * to which it belongs (through overlap).  Since a Region can overlap several windows,
+   * there may be >1 value in the resulting Seq.
+   *
+   * @param region An input Region which is to be keyed to 1 or more windows.
+   * @return A Seq of Region pairs, where the first element of each pair is one of the windows
+   *         (of fixed-width) and the second element is the input Region
+   */
+  def regionToWindows(region: ReferenceRegion): Seq[(Region, Region)] = {
+    val windowStart = region.start / window
+    val windowEnd = region.end / window
+
+    (windowStart to windowEnd).map {
+      case (widx: Long) =>
+        val wstart = widx * window
+        val wend = wstart + window
+        val wRegion = ReferenceRegion(region.referenceName, wstart, wend)
+        val clippedRegion = ReferenceRegion(region.referenceName, max(wstart, region.start), min(wend, region.end))
+        (wRegion, clippedRegion)
+    }
+  }
+
+  def optionOrdering(or1: Option[Region], or2: Option[Region]): Int =
+    (or1, or2) match {
+      case (None, None)         => 0
+      case (None, Some(r2))     => -1
+      case (Some(r1), None)     => 1
+      case (Some(r1), Some(r2)) => r1.compareTo(r2)
+    }
+
+  case class OrientedPoint(chrom: String, pos: Long, polarity: Boolean) extends Ordered[OrientedPoint] with Serializable {
+    override def compare(that: OrientedPoint): Int = {
+      if (chrom != that.chrom) {
+        chrom.compare(that.chrom)
+      } else {
+        val c1 = pos.compare(that.pos)
+        if (c1 != 0) {
+          c1
+        } else {
+          // we actually want the *reverse* ordering from the Java Boolean.compareTo
+          // function!
+          // c.f. https://docs.oracle.com/javase/7/docs/api/java/lang/Boolean.html#compareTo(java.lang.Boolean)
+          -polarity.compare(that.polarity)
+        }
+      }
+    }
+  }
+
+  /**
+   * This is a helper function for findCoverageRegions -- basically, it takes a set
+   * of input ReferenceRegions, it finds all pairs of regions that are adjacent to each
+   * other (i.e. pairs (r1, r2) where r1.end == r2.start and r1.referenceName == r2.referenceName),
+   * and it collapses all such adjacent regions into single contiguous regions.
+   *
+   * @param regions The input regions set; we assume that this input set is non-overlapping
+   *                (that no two regions in the input set overlap each other)
+   * @return The collapsed set of regions -- no two regions in the returned RDD should be
+   *         adjacent, all should be at least one base-pair apart (or on separate
+   *         chromosomes).
+   */
+  def collapseAdjacent(regions: RDD[Region]): RDD[Region] = {
+
+    val pairs = regions.sortBy(p => p).pairWithEnds()
+
+    val points: RDD[OrientedPoint] = pairs.flatMap {
+      case (None, Some(region)) =>
+        Seq(OrientedPoint(region.referenceName, region.start, true))
+      case (Some(region), None) =>
+        Seq(OrientedPoint(region.referenceName, region.end, false))
+      case (Some(r1), Some(r2)) =>
+        if (r1.isAdjacent(r2)) {
+          Seq()
+        } else {
+          Seq(
+            OrientedPoint(r1.referenceName, r1.end, false),
+            OrientedPoint(r2.referenceName, r2.start, true))
+        }
+      case _ => Seq()
+    }
+    val paired = points.pair()
+    val pairedAndFiltered = paired.filter(p =>
+      p._1.chrom == p._2.chrom && p._1.polarity && p._2.pos - p._1.pos >= 0)
+
+    pairedAndFiltered.map {
+      case (p1: OrientedPoint, p2: OrientedPoint) => ReferenceRegion(p1.chrom, p1.pos, p2.pos)
+    }
+  }
+
+  def getAllWindows(sc: SparkContext, dict: SequenceDictionary): RDD[ReferenceRegion] = {
+
+    val chromRegions: RDD[ReferenceRegion] = sc.parallelize(
+      dict.records.toSeq.map {
+        case seqRecord =>
+          ReferenceRegion(seqRecord.name, 0, seqRecord.length)
+      })
+
+    val windowRegions: RDD[ReferenceRegion] = chromRegions.flatMap {
+      case chromRegion =>
+        (0 until chromRegion.length().toInt by window.toInt).map { start =>
+          ReferenceRegion(chromRegion.referenceName, start, start + window)
+        }
+    }
+
+    windowRegions
+  }
+
+  def calculateCoverageRegions(regions: Iterable[ReferenceRegion]): Iterator[ReferenceRegion] =
+    calculateCoverageRegions(regions.iterator)
+
+  /**
+   * Calculates the coverage regions for an input set -- note that this input set is an
+   * Iterable, not an RDD.  This is the method which we call on each individual partition
+   * of the RDD, in order to calculate an initial set of disjoint-but-possibly-adjacent
+   * regions within the partition.
+   *
+   * @param regions The input set of ReferenceRegion objects
+   * @return The 'coverage regions' of the input set
+   */
+  def calculateCoverageRegions(regions: Iterator[ReferenceRegion]): Iterator[ReferenceRegion] = {
+    if (regions.isEmpty) {
+      Iterator()
+
+    } else {
+      val sregions = regions.toArray.sorted
+      if (sregions.size == 1) {
+        return sregions.iterator
+      }
+
+      // We're calculating the 'coverage regions' here.
+      // We do this in a few steps:
+      // 1. sort the regions in lexicographic (seq-start-end) order -- this happened above.
+      //    let the conceptual variables STARTS and ENDS be two arrays, each of len(regions),
+      //    which contain the .start and .end fields of the (ordered) regions.
+      // 2. Next, we calculate an array of length len(regions), called MAXENDS, where
+      //    MAXENDS(i) = max(ENDS[0:i-1])
+      // 3. Now, for any index i, if STARTS(i) > MAXENDS(i), then we call region i a
+      //    'split' region -- a region that doesn't overlap any region that came before it,
+      //    and which _starts_ a 'coverage region.' We calculate the set
+      //       SPLITS = { i : STARTS(i) > MAXENDS(i) }
+      // 4. Finally, we pair the splits -- each pair of splits corresponds to a single,
+      //    contiguous coverage region.
+
+      // TODO:
+      // Calculating the MAXENDS and SPLITS sets in two passes here, although we could probably
+      // do it in one if we really thought about it...
+      val maxEnds: Array[Long] = sregions.map(_.end).scanLeft(0L)(max)
+      val splitIndices: Seq[Int] =
+        0 +:
+          (1 until sregions.size).filter(i => sregions(i).start > maxEnds(i)) :+
+          sregions.size
+
+      //
+      splitIndices.sliding(2).map {
+        case Vector(i1, i2) =>
+          ReferenceRegion(sregions(i1).referenceName, sregions(i1).start, maxEnds(i2))
+      }.toIterator
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/features/FeatureParser.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/features/FeatureParser.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/features/FeatureParser.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/features/FeatureParser.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,306 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.features
+
+import java.io.File
+import java.util.UUID
+import org.bdgenomics.adam.models.SequenceRecord
+import org.bdgenomics.formats.avro.{ Dbxref, Contig, Strand, Feature }
+import scala.collection.JavaConversions._
+import scala.collection.mutable.ArrayBuffer
+
+import scala.io.Source
+
+trait FeatureParser extends Serializable {
+  def parse(line: String): Seq[Feature]
+}
+
+class FeatureFile(parser: FeatureParser) extends Serializable {
+  def parse(file: File): Iterator[Feature] =
+    Source.fromFile(file).getLines().flatMap { line =>
+      parser.parse(line)
+    }
+}
+
+object GTFParser {
+
+  private val attr_regex = "\\s*([^\\s]+)\\s\"([^\"]+)\"".r
+
+  /**
+   * Parses a string of format
+   *   token; token; token ...
+   *
+   * where each 'token' is of the form
+   *   key "value"
+   *
+   * and turns it into a Map
+   *
+   * @param attributeField The original string of tokens
+   * @return The Map of attributes
+   */
+  def parseAttrs(attributeField: String): Map[String, String] =
+    attributeField.split(";").flatMap {
+      case token: String =>
+        attr_regex.findFirstMatchIn(token).map(m => (m.group(1), m.group(2)))
+    }.toMap
+}
+
+/**
+ * GTF is a line-based GFF variant.
+ *
+ * Details of the GTF/GFF format here:
+ * http://www.ensembl.org/info/website/upload/gff.html
+ */
+class GTFParser extends FeatureParser {
+
+  override def parse(line: String): Seq[Feature] = {
+    // Just skip the '#' prefixed lines, these are comments in the
+    // GTF file format.
+    if (line.startsWith("#")) {
+      return Seq()
+    }
+
+    val fields = line.split("\t")
+
+    val (seqname, source, feature, start, end, score, strand, frame, attribute) =
+      (fields(0), fields(1), fields(2), fields(3), fields(4), fields(5), fields(6), fields(7), fields(8))
+
+    lazy val attrs = GTFParser.parseAttrs(attribute)
+
+    val contig = Contig.newBuilder().setContigName(seqname).build()
+    val f = Feature.newBuilder()
+      .setContig(contig)
+      .setStart(start.toLong - 1) // GTF/GFF ranges are 1-based
+      .setEnd(end.toLong) // GTF/GFF ranges are closed
+      .setFeatureType(feature)
+      .setSource(source)
+
+    val _strand = strand match {
+      case "+" => Strand.Forward
+      case "-" => Strand.Reverse
+      case _   => Strand.Independent
+    }
+    f.setStrand(_strand)
+
+    val exonId: Option[String] = attrs.get("exon_id").orElse {
+      attrs.get("transcript_id").flatMap(t => attrs.get("exon_number").map(e => t + "_" + e))
+    }
+
+    val (_id, _parentId) =
+      feature match {
+        case "gene"       => (attrs.get("gene_id"), None)
+        case "transcript" => (attrs.get("transcript_id"), attrs.get("gene_id"))
+        case "exon"       => (exonId, attrs.get("transcript_id"))
+        case "CDS"        => (attrs.get("id"), attrs.get("transcript_id"))
+        case "UTR"        => (attrs.get("id"), attrs.get("transcript_id"))
+        case _            => (attrs.get("id"), None)
+      }
+    _id.foreach(f.setFeatureId)
+    _parentId.foreach(parentId => f.setParentIds(List[String](parentId)))
+
+    f.setAttributes(attrs)
+
+    Seq(f.build())
+  }
+}
+
+class IntervalListParser extends Serializable {
+  def parse(line: String): (Option[SequenceRecord], Option[Feature]) = {
+    val fields = line.split("[ \t]+")
+    if (fields.length < 2) {
+      (None, None)
+    } else {
+      if (fields(0).startsWith("@")) {
+        if (fields(0).startsWith("@SQ")) {
+          val (name, length, url, md5) = {
+            val attrs = fields.drop(1).map(field => field.split(":", 2) match {
+              case Array(key, value) => key -> value
+              case x                 => throw new Exception(s"Expected fields of the form 'key:value' in field $field but got: $x. Line:\n$line")
+            }).toMap
+
+            // Require that all @SQ lines have name, length, url, md5.
+            (attrs("SN"), attrs("LN").toLong, attrs("UR"), attrs("M5"))
+          }
+
+          (Some(SequenceRecord(name, length, md5, url)), None)
+        } else {
+          (None, None)
+        }
+      } else {
+        if (fields.length < 4) {
+          throw new Exception(s"Invalid line: $line")
+        }
+
+        val (dbxrfs, attrs: Map[String, String]) =
+          (if (fields.length < 5 || fields(4) == "." || fields(4) == "-") {
+            (Nil, Map())
+          } else {
+            val a = fields(4).split(Array(';', ',')).map(field => field.split('|') match {
+              case Array(key, value) =>
+                key match {
+                  case "gn" | "ens" | "vega" | "ccds" =>
+                    (
+                      Some(Dbxref.newBuilder().setDb(key).setAccession(value).build()),
+                      None
+                    )
+                  case _ => (None, Some(key -> value))
+                }
+              case x => throw new Exception(s"Expected fields of the form 'key|value;' but got: $field. Line:\n$line")
+            })
+
+            (a.flatMap(_._1).toList, a.flatMap(_._2).toMap)
+          })
+
+        (
+          None,
+          Some(
+            Feature.newBuilder()
+              .setContig(Contig.newBuilder().setContigName(fields(0)).build())
+              .setStart(fields(1).toLong)
+              .setEnd(fields(2).toLong)
+              .setStrand(fields(3) match {
+                case "+" => Strand.Forward
+                case "-" => Strand.Reverse
+                case _   => Strand.Independent
+              })
+              .setAttributes(attrs)
+              .setDbxrefs(dbxrfs)
+              .build()
+          )
+        )
+      }
+    }
+  }
+
+}
+
+class BEDParser extends FeatureParser {
+
+  override def parse(line: String): Seq[Feature] = {
+
+    val fields = line.split("\t")
+    if (fields.length < 3) {
+      return Seq()
+    }
+    val fb = Feature.newBuilder()
+    val cb = Contig.newBuilder()
+    cb.setContigName(fields(0))
+    fb.setContig(cb.build())
+    fb.setFeatureId(UUID.randomUUID().toString)
+
+    // BED files are 0-based space-coordinates, so conversion to
+    // our coordinate space should mean that the values are unchanged.
+    fb.setStart(fields(1).toLong)
+    fb.setEnd(fields(2).toLong)
+
+    if (fields.length > 3) {
+      fb.setFeatureType(fields(3))
+    }
+    if (fields.length > 4) {
+      fb.setValue(fields(4) match {
+        case "." => null
+        case _   => fields(4).toDouble
+      })
+    }
+    if (fields.length > 5) {
+      fb.setStrand(fields(5) match {
+        case "+" => Strand.Forward
+        case "-" => Strand.Reverse
+        case _   => Strand.Independent
+      })
+    }
+    val attributes = new ArrayBuffer[(String, String)]()
+    if (fields.length > 6) {
+      attributes += ("thickStart" -> fields(6))
+    }
+    if (fields.length > 7) {
+      attributes += ("thickEnd" -> fields(7))
+    }
+    if (fields.length > 8) {
+      attributes += ("itemRgb" -> fields(8))
+    }
+    if (fields.length > 9) {
+      attributes += ("blockCount" -> fields(9))
+    }
+    if (fields.length > 10) {
+      attributes += ("blockSizes" -> fields(10))
+    }
+    if (fields.length > 11) {
+      attributes += ("blockStarts" -> fields(11))
+    }
+    val attrMap = attributes.toMap
+    fb.setAttributes(attrMap)
+
+    val feature: Feature = fb.build()
+    Seq(feature)
+  }
+}
+
+class NarrowPeakParser extends FeatureParser {
+
+  override def parse(line: String): Seq[Feature] = {
+    val fields = line.split("\t")
+    if (fields.length < 3) {
+      return Seq()
+    }
+    val fb = Feature.newBuilder()
+    val cb = Contig.newBuilder()
+    cb.setContigName(fields(0))
+    fb.setContig(cb.build())
+    fb.setFeatureId(UUID.randomUUID().toString)
+
+    // Peak files are 0-based space-coordinates, so conversion to
+    // our coordinate space should mean that the values are unchanged.
+    fb.setStart(fields(1).toLong)
+    fb.setEnd(fields(2).toLong)
+
+    if (fields.length > 3) {
+      fb.setFeatureType(fields(3))
+    }
+    if (fields.length > 4) {
+      fb.setValue(fields(4) match {
+        case "." => null
+        case _   => fields(4).toDouble
+      })
+    }
+    if (fields.length > 5) {
+      fb.setStrand(fields(5) match {
+        case "+" => Strand.Forward
+        case "-" => Strand.Reverse
+        case _   => Strand.Independent
+      })
+    }
+    val attributes = new ArrayBuffer[(String, String)]()
+    if (fields.length > 6) {
+      attributes += ("signalValue" -> fields(6))
+    }
+    if (fields.length > 7) {
+      attributes += ("pValue" -> fields(7))
+    }
+    if (fields.length > 8) {
+      attributes += ("qValue" -> fields(8))
+    }
+    if (fields.length > 9) {
+      attributes += ("peak" -> fields(9))
+    }
+    val attrMap = attributes.toMap
+    fb.setAttributes(attrMap)
+    Seq(fb.build())
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/features/GeneFeatureRDDFunctions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/features/GeneFeatureRDDFunctions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/features/GeneFeatureRDDFunctions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/features/GeneFeatureRDDFunctions.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,134 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.features
+
+import org.apache.spark.Logging
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.formats.avro.{ Strand, Feature }
+import scala.collection.JavaConversions._
+
+class GeneFeatureRDDFunctions(featureRDD: RDD[Feature]) extends Serializable with Logging {
+
+  private def strand(str: Strand): Boolean = str match {
+    case Strand.Forward     => true
+    case Strand.Reverse     => false
+    case Strand.Independent => true
+  }
+
+  def asGenes(): RDD[Gene] = {
+
+    /*
+    Creating a set of gene models works in four steps:
+
+    1. Key each GTFFeature by its featureType (which should be either 'gene',
+       'transcript', or 'exon').
+
+    2. Take all the values of type 'exon', and create Exon objects from them.
+       Key this RDD by transcriptId of each exon, and group all exons of the same
+       transcript together. Also, do the same for the 'cds' features.
+
+    3. Take all the values of type 'transcript', key them by their transcriptId,
+       and join them with the cds and exons from step #2.  This should give each us
+       enough information to create each Transcript, which we do, key each Transcript
+       by its geneId, and group the transcripts which share a common gene together.
+
+    4. Finally, find each 'gene'-typed GTFFeature, key it by its geneId, and join with
+       the transcripts in #3.  Use these joined values to create the final set of
+       Gene values.
+
+    The three groupBys and two joins are necessary for creating the two-level hierarchical
+    tree-structured Gene models, under the assumption that the rows of the GTF file itself
+    aren't ordered.
+     */
+
+    // Step #1
+    val typePartitioned: RDD[(String, Feature)] =
+      featureRDD.keyBy(_.getFeatureType.toString).cache()
+
+    // Step #2
+    val exonsByTranscript: RDD[(String, Iterable[Exon])] =
+      typePartitioned.filter(_._1 == "exon").flatMap {
+        // There really only should be _one_ parent listed in this flatMap, but since
+        // getParentIds is modeled as returning a List[], we'll write it this way.
+        case ("exon", ftr: Feature) =>
+          val ids: Seq[String] = ftr.getParentIds.map(_.toString)
+          ids.map(transcriptId => (transcriptId,
+            Exon(ftr.getFeatureId.toString, transcriptId, strand(ftr.getStrand), ReferenceRegion(ftr))))
+      }.groupByKey()
+
+    val cdsByTranscript: RDD[(String, Iterable[CDS])] =
+      typePartitioned.filter(_._1 == "CDS").flatMap {
+        case ("CDS", ftr: Feature) =>
+          val ids: Seq[String] = ftr.getParentIds.map(_.toString)
+          ids.map(transcriptId => (transcriptId,
+            CDS(transcriptId, strand(ftr.getStrand), ReferenceRegion(ftr))))
+      }.groupByKey()
+
+    val utrsByTranscript: RDD[(String, Iterable[UTR])] =
+      typePartitioned.filter(_._1 == "UTR").flatMap {
+        case ("UTR", ftr: Feature) =>
+          val ids: Seq[String] = ftr.getParentIds.map(_.toString)
+          ids.map(transcriptId => (transcriptId,
+            UTR(transcriptId, strand(ftr.getStrand), ReferenceRegion(ftr))))
+      }.groupByKey()
+
+    // Step #3
+    val transcriptsByGene: RDD[(String, Iterable[Transcript])] =
+      typePartitioned.filter(_._1 == "transcript").map {
+        case ("transcript", ftr: Feature) => (ftr.getFeatureId.toString, ftr)
+      }.join(exonsByTranscript)
+        .leftOuterJoin(utrsByTranscript)
+        .leftOuterJoin(cdsByTranscript)
+
+        .flatMap {
+          // There really only should be _one_ parent listed in this flatMap, but since
+          // getParentIds is modeled as returning a List[], we'll write it this way.
+          case (transcriptId: String, (((tgtf: Feature, exons: Iterable[Exon]),
+            utrs: Option[Iterable[UTR]]),
+            cds: Option[Iterable[CDS]])) =>
+            val geneIds: Seq[String] = tgtf.getParentIds.map(_.toString) // should be length 1
+            geneIds.map(geneId => (geneId,
+              Transcript(transcriptId, Seq(transcriptId), geneId,
+                strand(tgtf.getStrand),
+                exons, cds.getOrElse(Seq()), utrs.getOrElse(Seq()))))
+        }.groupByKey()
+
+    // Step #4
+    val genes = typePartitioned.filter(_._1 == "gene").map {
+      case ("gene", ftr: Feature) => (ftr.getFeatureId.toString, ftr)
+    }.leftOuterJoin(transcriptsByGene).map {
+      case (geneId: String, (ggtf: Feature, transcripts: Option[Iterable[Transcript]])) =>
+        Gene(geneId, Seq(geneId),
+          strand(ggtf.getStrand),
+          transcripts.getOrElse(Seq()))
+    }
+
+    genes
+  }
+
+  def filterByOverlappingRegion(query: ReferenceRegion): RDD[Feature] = {
+    def overlapsQuery(rec: Feature): Boolean =
+      rec.getContig.getContigName.toString == query.referenceName &&
+        rec.getStart < query.end &&
+        rec.getEnd > query.start
+    featureRDD.filter(overlapsQuery)
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/GenomicPartitioners.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/GenomicPartitioners.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/GenomicPartitioners.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/GenomicPartitioners.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,136 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.bdgenomics.adam.models.{ ReferenceRegion, ReferencePosition, SequenceDictionary }
+import org.apache.spark.{ Logging, Partitioner }
+import scala.math._
+
+/**
+ * GenomicPositionPartitioner partitions ReferencePosition objects into separate, spatially-coherent
+ * regions of the genome.
+ *
+ * This can be used to organize genomic data for computation that is spatially distributed (e.g. GATK and Queue's
+ * "scatter-and-gather" for locus-parallelizable walkers).
+ *
+ * @param numParts The number of equally-sized regions into which the total genomic space is partitioned;
+ *                 the total number of partitions is numParts + 1, with the "+1" resulting from one
+ *                 extra partition that is used to capture null or UNMAPPED values of the ReferencePosition
+ *                 type.
+ * @param seqLengths a map relating sequence-name to length and indicating the set and length of all extant
+ *                   sequences in the genome.
+ */
+case class GenomicPositionPartitioner(numParts: Int, seqLengths: Map[String, Long]) extends Partitioner with Logging {
+
+  log.info("Have genomic position partitioner with " + numParts + " partitions, and sequences:")
+  seqLengths.foreach(kv => log.info("Contig " + kv._1 + " with length " + kv._2))
+
+  val names: Seq[String] = seqLengths.keys.toSeq.sortWith(_ < _)
+  val lengths: Seq[Long] = names.map(seqLengths(_))
+  private val cumuls: Seq[Long] = lengths.scan(0L)(_ + _)
+
+  // total # of bases in the sequence dictionary
+  val totalLength: Long = lengths.reduce(_ + _)
+
+  // referenceName -> cumulative length before this sequence (using seqDict.records as the implicit ordering)
+  val cumulativeLengths: Map[String, Long] = Map(
+    names.zip(cumuls): _*)
+
+  /**
+   * 'parts' is the total number of partitions for non-UNMAPPED ReferencePositions --
+   * the total number of partitions (see numPartitions, below) is parts+1, with the
+   * extra partition being included for handling ReferencePosition.UNMAPPED
+   */
+  private val parts = min(numParts, totalLength).toInt
+
+  override def numPartitions: Int = parts + 1
+
+  override def getPartition(key: Any): Int = {
+
+    // This allows partitions that cross chromosome boundaries.
+    // The computation is slightly more complicated if you want to avoid this.
+    def getPart(referenceName: String, pos: Long): Int = {
+      val totalOffset = cumulativeLengths(referenceName) + pos
+      val totalFraction: Double = totalOffset.toDouble / totalLength
+      // Need to use 'parts' here, rather than 'numPartitions' -- see the note
+      // on 'parts', above.
+      min(floor(totalFraction * parts.toDouble).toInt, numPartitions)
+    }
+
+    key match {
+      // "unmapped" positions get put in the "top" or last bucket
+      case ReferencePosition.UNMAPPED => parts
+
+      // everything else gets assigned normally.
+      case refpos: ReferencePosition => {
+        require(seqLengths.contains(refpos.referenceName),
+          "Received key (%s) that did not map to a known contig. Contigs are:\n%s".format(refpos,
+            seqLengths.keys.mkString("\n")))
+        getPart(refpos.referenceName, refpos.pos)
+      }
+
+      // only ReferencePosition values are partitioned using this partitioner
+      case _ => throw new IllegalArgumentException("Only ReferencePosition values can be partitioned by GenomicPositionPartitioner")
+    }
+  }
+
+  override def toString(): String = {
+    return "%d parts, %d partitions, %s" format (parts, numPartitions, cumulativeLengths.toString)
+  }
+
+}
+
+object GenomicPositionPartitioner {
+
+  def apply(numParts: Int, seqDict: SequenceDictionary): GenomicPositionPartitioner =
+    GenomicPositionPartitioner(numParts, extractLengthMap(seqDict))
+
+  def extractLengthMap(seqDict: SequenceDictionary): Map[String, Long] =
+    Map(seqDict.records.toSeq.map(rec => (rec.name.toString, rec.length)): _*)
+}
+
+case class GenomicRegionPartitioner(partitionSize: Long, seqLengths: Map[String, Long], start: Boolean = true) extends Partitioner with Logging {
+  private val names: Seq[String] = seqLengths.keys.toSeq.sortWith(_ < _)
+  private val lengths: Seq[Long] = names.map(seqLengths(_))
+  private val parts: Seq[Int] = lengths.map(v => round(ceil(v.toDouble / partitionSize)).toInt)
+  private val cumulParts: Map[String, Int] = Map(names.zip(parts.scan(0)(_ + _)): _*)
+
+  private def computePartition(refReg: ReferenceRegion): Int = {
+    val pos = if (start) refReg.start else (refReg.end - 1)
+    (cumulParts(refReg.referenceName) + pos / partitionSize).toInt
+  }
+
+  override def numPartitions: Int = parts.sum
+
+  override def getPartition(key: Any): Int = {
+    key match {
+      case region: ReferenceRegion => {
+        require(seqLengths.contains(region.referenceName),
+          "Received key (%s) that did not map to a known contig. Contigs are:\n%s".format(region,
+            seqLengths.keys.mkString("\n")))
+        computePartition(region)
+      }
+      case _ => throw new IllegalArgumentException("Only ReferenceMappable values can be partitioned by GenomicRegionPartitioner")
+    }
+  }
+}
+
+object GenomicRegionPartitioner {
+  def apply(partitionSize: Long, seqDict: SequenceDictionary): GenomicRegionPartitioner =
+    GenomicRegionPartitioner(partitionSize, GenomicPositionPartitioner.extractLengthMap(seqDict))
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/PairingRDD.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/PairingRDD.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/PairingRDD.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/PairingRDD.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,130 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext._
+import org.bdgenomics.adam.rdd.ADAMContext._
+import scala.math._
+
+import scala.reflect.ClassTag
+
+/**
+ * PairingRDD provides some simple helper methods, allowing us take an RDD (presumably an
+ * RDD whose values are in some reasonable or intelligible order within and across partitions)
+ * and get paired or windowed views on that list of items.
+ *
+ * @param rdd The RDD of ordered values
+ * @param kt The type of the values in the RDD must be manifest
+ * @tparam T The type of the values in the RDD
+ */
+class PairingRDD[T](rdd: RDD[T])(implicit kt: ClassTag[T], ordering: Ordering[T]) extends Serializable {
+
+  private val sorted: RDD[T] = rdd.sortBy(k => k, ascending = true)
+
+  /**
+   * Replicates the Seq.sliding(int) method, where we turn an RDD[T] into an RDD[Seq[T]],
+   * where each internal Seq contains exactly 'width' values taken (in order) from the original
+   * RDD, and where all such windows are presented 'in order' in the output set.
+   *
+   * E.g. the result of 'sliding(3)' on an RDD of the elements
+   *    1, 2, 3, 4, 5
+   *
+   * Should be an RDD of
+   *    Seq(1, 2, 3), Seq(2, 3, 4), Seq(3, 4, 5)
+   *
+   * @param width The 'width' of the sliding window to calculate
+   * @return An RDD of the sliding window values
+   */
+  def sliding(width: Int): RDD[Seq[T]] = {
+    val base: RDD[(Long, T)] = sorted.zipWithIndex().map(p => (p._2, p._1))
+
+    val allOffsets: RDD[(Long, (Long, T))] = base.flatMap(
+      (p: (Long, T)) =>
+        (0 until min(width, p._1.toInt + 1)).map(w => (p._1 - w, (p._1, p._2)))
+    )
+
+    val grouped: RDD[Seq[T]] = allOffsets.groupByKey().map {
+      case (index: Long, values: Iterable[(Long, T)]) =>
+        values.toSeq.sortBy(_._1).map(_._2)
+    }
+
+    grouped.filter(_.length == width)
+  }
+
+  /**
+   * The 'pair' method is a simplified version of .sliding(2), returning just pairs
+   * of (T, T) values for every consecutive pair of T values in the input RDD.
+   *
+   * For example, calling .pair() on a (sorted) RDD of
+   *   1, 2, 3, 4
+   *
+   * should return the following pairs
+   *   (1, 2), (2, 3), (3, 4)
+   *
+   * @return an RDD[(T, T)] of all consecutive pairs of values
+   */
+  def pair(): RDD[(T, T)] = {
+    val indexed: RDD[(Long, T)] = sorted.zipWithIndex().map(p => (p._2, p._1)).sortByKey()
+    val indexMinusOne: RDD[(Long, T)] = indexed.map(p => (p._1 - 1, p._2))
+
+    indexed.join(indexMinusOne).map(_._2)
+  }
+
+  /**
+   * The 'pairWithEnds' method is a variation on 'pairs', except that it returns two
+   * _extra_ pairs (relative to 'pairs') corresponding to the first and last elements
+   * of the original RDD.  Every (t1, t2) from .pair() now becomes a (Some(t1), Some(t2))
+   * with .pairWithEnds().  The first element is a (None, Some(t0)) and the last element
+   * is a (Some(tN), None).
+   *
+   * For example, calling .pairWithEnds() on a (sorted) RDD of
+   *   1, 2, 3
+   *
+   * should return the following pairs
+   *   (None, Some(1)), (Some(1), Some(2)), (Some(2), Some(3)), (Some(3), None)
+   *
+   * (This is immediately useful as a helper method inside the Coverage class, but also
+   * might be useful to other applications as well, that rely on a total ordering of the
+   * elements within a single RDD.)
+   *
+   * @return an RDD[(T, T)] of all consecutive pairs of values
+   */
+  def pairWithEnds(): RDD[(Option[T], Option[T])] = {
+    val indexed: RDD[(Long, Option[T])] = sorted.zipWithIndex().map(p => (p._2, Some(p._1)))
+    val indexMinusOne: RDD[(Long, Option[T])] = indexed.map(p => (p._1 - 1, p._2))
+    val max: Long = indexed.map(_._1).count()
+
+    if (max == 0) { return rdd.sparkContext.emptyRDD }
+
+    val initial: RDD[(Long, Option[T])] = indexed.sparkContext.parallelize(Seq(-1L -> None))
+    val terminal: RDD[(Long, Option[T])] = indexed.sparkContext.parallelize(Seq((max - 1) -> None))
+
+    val initialed = indexed.union(initial)
+    val terminated = indexMinusOne.union(terminal)
+    val joined = initialed.join(terminated).sortByKey(ascending = true)
+
+    joined.map(_._2)
+  }
+
+}
+
+object PairingRDD extends Serializable {
+  implicit def rddToPairingRDD[T](rdd: RDD[T])(implicit kt: ClassTag[T], ordering: Ordering[T]): PairingRDD[T] =
+    new PairingRDD[T](rdd)
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMBAMOutputFormat.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMBAMOutputFormat.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMBAMOutputFormat.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMBAMOutputFormat.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,75 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import org.seqdoop.hadoop_bam.{ SAMRecordWritable, KeyIgnoringBAMOutputFormat }
+import htsjdk.samtools.SAMFileHeader
+import org.apache.spark.rdd.InstrumentedOutputFormat
+import org.apache.hadoop.mapreduce.OutputFormat
+import org.bdgenomics.adam.instrumentation.Timers
+
+object ADAMBAMOutputFormat extends Serializable {
+
+  private[read] var header: Option[SAMFileHeader] = None
+
+  /**
+   * Attaches a header to the ADAMSAMOutputFormat Hadoop writer. If a header has previously
+   * been attached, the header must be cleared first.
+   *
+   * @throws Exception Exception thrown if a SAM header has previously been attached, and not cleared.
+   *
+   * @param samHeader Header to attach.
+   *
+   * @see clearHeader
+   */
+  def addHeader(samHeader: SAMFileHeader) {
+    assert(header.isEmpty, "Cannot attach a new SAM header without first clearing the header.")
+    header = Some(samHeader)
+  }
+
+  /**
+   * Clears the attached header.
+   *
+   * @see addHeader
+   */
+  def clearHeader() {
+    header = None
+  }
+
+  /**
+   * Returns the current header.
+   *
+   * @return Current SAM header.
+   */
+  private[read] def getHeader: SAMFileHeader = {
+    assert(header.isDefined, "Cannot return header if not attached.")
+    header.get
+  }
+}
+
+class ADAMBAMOutputFormat[K]
+    extends KeyIgnoringBAMOutputFormat[K] with Serializable {
+
+  setSAMHeader(ADAMBAMOutputFormat.getHeader)
+}
+
+class InstrumentedADAMBAMOutputFormat[K] extends InstrumentedOutputFormat[K, org.seqdoop.hadoop_bam.SAMRecordWritable] {
+  override def timerName(): String = Timers.WriteBAMRecord.timerName
+  override def outputFormatClass(): Class[_ <: OutputFormat[K, SAMRecordWritable]] = classOf[ADAMBAMOutputFormat[K]]
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMSAMOutputFormat.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMSAMOutputFormat.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMSAMOutputFormat.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/ADAMSAMOutputFormat.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,74 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import org.seqdoop.hadoop_bam.{ SAMRecordWritable, KeyIgnoringAnySAMOutputFormat, SAMFormat }
+import htsjdk.samtools.SAMFileHeader
+import org.apache.spark.rdd.InstrumentedOutputFormat
+import org.bdgenomics.adam.instrumentation.Timers
+import org.apache.hadoop.mapreduce.OutputFormat
+
+object ADAMSAMOutputFormat extends Serializable {
+
+  private[read] var header: Option[SAMFileHeader] = None
+
+  /**
+   * Attaches a header to the ADAMSAMOutputFormat Hadoop writer. If a header has previously
+   * been attached, the header must be cleared first.
+   *
+   * @throws Exception Exception thrown if a SAM header has previously been attached, and not cleared.
+   *
+   * @param samHeader Header to attach.
+   *
+   * @see clearHeader
+   */
+  def addHeader(samHeader: SAMFileHeader) {
+    assert(header.isEmpty, "Cannot attach a new SAM header without first clearing the header.")
+    header = Some(samHeader)
+  }
+
+  /**
+   * Clears the attached header.
+   *
+   * @see addHeader
+   */
+  def clearHeader() {
+    header = None
+  }
+
+  /**
+   * Returns the current header.
+   *
+   * @return Current SAM header.
+   */
+  private[read] def getHeader: SAMFileHeader = {
+    assert(header.isDefined, "Cannot return header if not attached.")
+    header.get
+  }
+}
+
+class ADAMSAMOutputFormat[K]
+    extends KeyIgnoringAnySAMOutputFormat[K](SAMFormat.valueOf("SAM")) with Serializable {
+
+  setSAMHeader(ADAMSAMOutputFormat.getHeader)
+}
+
+class InstrumentedADAMSAMOutputFormat[K] extends InstrumentedOutputFormat[K, org.seqdoop.hadoop_bam.SAMRecordWritable] {
+  override def timerName(): String = Timers.WriteSAMRecord.timerName
+  override def outputFormatClass(): Class[_ <: OutputFormat[K, SAMRecordWritable]] = classOf[ADAMSAMOutputFormat[K]]
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/AlignmentRecordRDDFunctions.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,580 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import java.io.StringWriter
+import htsjdk.samtools.{ ValidationStringency, SAMTextHeaderCodec, SAMTextWriter, SAMFileHeader }
+import org.apache.spark.storage.StorageLevel
+import org.seqdoop.hadoop_bam.SAMRecordWritable
+import org.apache.hadoop.io.LongWritable
+import org.apache.spark.broadcast.Broadcast
+import org.apache.spark.rdd.MetricsContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.algorithms.consensus.{
+  ConsensusGenerator,
+  ConsensusGeneratorFromReads
+}
+import org.bdgenomics.adam.converters.AlignmentRecordConverter
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rdd.{ ADAMSaveAnyArgs, ADAMSequenceDictionaryRDDAggregator }
+import org.bdgenomics.adam.rdd.read.realignment.RealignIndels
+import org.bdgenomics.adam.rdd.read.recalibration.BaseQualityRecalibration
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.util.MapTools
+import org.bdgenomics.formats.avro._
+import org.bdgenomics.utils.cli.SaveArgs
+
+
+/**
+ * Modified by lhy, to link new duplicate method when call markDuplicate on read rdds.
+ */
+class AlignmentRecordRDDFunctions(rdd: RDD[AlignmentRecord])
+    extends ADAMSequenceDictionaryRDDAggregator[AlignmentRecord](rdd) {
+
+  /**
+   * Calculates the subset of the RDD whose AlignmentRecords overlap the corresponding
+   * query ReferenceRegion.  Equality of the reference sequence (to which these are aligned)
+   * is tested by string equality of the names.  AlignmentRecords whose 'getReadMapped' method
+   * return 'false' are ignored.
+   *
+   * The end of the record against the reference sequence is calculated from the cigar string
+   * using the ADAMContext.referenceLengthFromCigar method.
+   *
+   * @param query The query region, only records which overlap this region are returned.
+   * @return The subset of AlignmentRecords (corresponding to either primary or secondary alignments) that
+   *         overlap the query region.
+   */
+  def filterByOverlappingRegion(query: ReferenceRegion): RDD[AlignmentRecord] = {
+    def overlapsQuery(rec: AlignmentRecord): Boolean =
+      rec.getReadMapped &&
+        rec.getContig.getContigName.toString == query.referenceName &&
+        rec.getStart < query.end &&
+        rec.getEnd > query.start
+    rdd.filter(overlapsQuery)
+  }
+
+  def maybeSaveBam(args: SaveArgs): Boolean = {
+    if (args.outputPath.endsWith(".sam")) {
+      log.info("Saving data in SAM format")
+      rdd.adamSAMSave(args.outputPath)
+      true
+    } else if (args.outputPath.endsWith(".bam")) {
+      log.info("Saving data in BAM format")
+      rdd.adamSAMSave(args.outputPath, asSam = false)
+      true
+    } else
+      false
+  }
+
+  def maybeSaveFastq(args: ADAMSaveAnyArgs): Boolean = {
+    if (args.outputPath.endsWith(".fq") || args.outputPath.endsWith(".fastq") ||
+      args.outputPath.endsWith(".ifq")) {
+      rdd.adamSaveAsFastq(args.outputPath, sort = args.sortFastqOutput)
+      true
+    } else
+      false
+  }
+
+  def adamAlignedRecordSave(args: SaveArgs) = {
+    maybeSaveBam(args) || { rdd.adamParquetSave(args); true }
+  }
+
+  def adamSave(args: ADAMSaveAnyArgs) = {
+    maybeSaveBam(args) || maybeSaveFastq(args) || { rdd.adamParquetSave(args); true }
+  }
+
+  def adamSAMString: String = {
+    // convert the records
+    val (convertRecords: RDD[SAMRecordWritable], header: SAMFileHeader) = rdd.adamConvertToSAM()
+
+    val records = convertRecords.coalesce(1, shuffle = true).collect()
+
+    val samHeaderCodec = new SAMTextHeaderCodec
+    samHeaderCodec.setValidationStringency(ValidationStringency.SILENT)
+
+    val samStringWriter = new StringWriter()
+    samHeaderCodec.encode(samStringWriter, header);
+
+    val samWriter: SAMTextWriter = new SAMTextWriter(samStringWriter)
+    //samWriter.writeHeader(stringHeaderWriter.toString)
+
+    records.foreach(record => samWriter.writeAlignment(record.get))
+
+    samStringWriter.toString
+  }
+
+  /**
+   * Saves an RDD of ADAM read data into the SAM/BAM format.
+   *
+   * @param filePath Path to save files to.
+   * @param asSam Selects whether to save as SAM or BAM. The default value is true (save in SAM format).
+   */
+  def adamSAMSave(filePath: String, asSam: Boolean = true) = SAMSave.time {
+
+    // convert the records
+    val (convertRecords: RDD[SAMRecordWritable], header: SAMFileHeader) = rdd.adamConvertToSAM()
+
+    // add keys to our records
+    val withKey = convertRecords.keyBy(v => new LongWritable(v.get.getAlignmentStart))
+
+    val bcastHeader = rdd.context.broadcast(header)
+    val mp = rdd.mapPartitionsWithIndex((idx, iter) => {
+      log.warn(s"Setting ${if (asSam) "SAM" else "BAM"} header for partition $idx")
+      val header = bcastHeader.value
+      synchronized {
+        // perform map partition call to ensure that the VCF header is set on all
+        // nodes in the cluster; see:
+        // https://github.com/bigdatagenomics/adam/issues/353
+
+        asSam match {
+          case true =>
+            ADAMSAMOutputFormat.clearHeader()
+            ADAMSAMOutputFormat.addHeader(header)
+            log.warn(s"Set SAM header for partition $idx")
+          case false =>
+            ADAMBAMOutputFormat.clearHeader()
+            ADAMBAMOutputFormat.addHeader(header)
+            log.warn(s"Set BAM header for partition $idx")
+        }
+      }
+      Iterator[Int]()
+    }).count()
+
+    // force value check, ensure that computation happens
+    if (mp != 0) {
+      log.warn("Had more than 0 elements after map partitions call to set VCF header across cluster.")
+    }
+
+    // attach header to output format
+    asSam match {
+      case true =>
+        ADAMSAMOutputFormat.clearHeader()
+        ADAMSAMOutputFormat.addHeader(header)
+      case false =>
+        ADAMBAMOutputFormat.clearHeader()
+        ADAMBAMOutputFormat.addHeader(header)
+    }
+
+    // write file to disk
+    val conf = rdd.context.hadoopConfiguration
+    asSam match {
+      case true =>
+        withKey.saveAsNewAPIHadoopFile(
+          filePath,
+          classOf[LongWritable],
+          classOf[SAMRecordWritable],
+          classOf[InstrumentedADAMSAMOutputFormat[LongWritable]],
+          conf
+        )
+      case false =>
+        withKey.saveAsNewAPIHadoopFile(
+          filePath,
+          classOf[LongWritable],
+          classOf[SAMRecordWritable],
+          classOf[InstrumentedADAMBAMOutputFormat[LongWritable]],
+          conf
+        )
+    }
+  }
+
+  def getSequenceRecordsFromElement(elem: AlignmentRecord): scala.collection.Set[SequenceRecord] = {
+    SequenceRecord.fromADAMRecord(elem)
+  }
+
+  /**
+   * Collects a dictionary summarizing the read groups in an RDD of ADAMRecords.
+   *
+   * @return A dictionary describing the read groups in this RDD.
+   */
+  def adamGetReadGroupDictionary(): RecordGroupDictionary = {
+    val rgNames = rdd.flatMap(RecordGroup(_))
+      .distinct()
+      .collect()
+      .toSeq
+
+    new RecordGroupDictionary(rgNames)
+  }
+
+  /**
+   * Converts an RDD of ADAM read records into SAM records.
+   *
+   * @return Returns a SAM/BAM formatted RDD of reads, as well as the file header.
+   */
+  def adamConvertToSAM(): (RDD[SAMRecordWritable], SAMFileHeader) = ConvertToSAM.time {
+    // collect global summary data
+    val sd = rdd.adamGetSequenceDictionary()
+    val rgd = rdd.adamGetReadGroupDictionary()
+
+    // create conversion object
+    val adamRecordConverter = new AlignmentRecordConverter
+
+    // create header
+    val header = adamRecordConverter.createSAMHeader(sd, rgd)
+
+    // broadcast for efficiency
+    val hdrBcast = rdd.context.broadcast(SAMFileHeaderWritable(header))
+
+    // map across RDD to perform conversion
+    val convertedRDD: RDD[SAMRecordWritable] = rdd.map(r => {
+      // must wrap record for serializability
+      val srw = new SAMRecordWritable()
+      srw.set(adamRecordConverter.convert(r, hdrBcast.value))
+      srw
+    })
+
+    (convertedRDD, header)
+  }
+
+  /**
+   * Cuts reads into _k_-mers, and then counts the number of occurrences of each _k_-mer.
+   *
+   * @param kmerLength The value of _k_ to use for cutting _k_-mers.
+   * @return Returns an RDD containing k-mer/count pairs.
+   *
+   * @see adamCountQmers
+   */
+  def adamCountKmers(kmerLength: Int): RDD[(String, Long)] = {
+    rdd.flatMap(r => {
+      // cut each read into k-mers, and attach a count of 1L
+      r.getSequence
+        .toString
+        .sliding(kmerLength)
+        .map(k => (k, 1L))
+    }).reduceByKey((k1: Long, k2: Long) => k1 + k2)
+  }
+
+  def adamSortReadsByReferencePosition(): RDD[AlignmentRecord] = SortReads.time {
+    log.info("Sorting reads by reference position")
+
+    // NOTE: In order to keep unmapped reads from swamping a single partition
+    // we sort the unmapped reads by read name. we prefix with "ZZZ" to ensure
+    // that the read name is lexicographically "after" the contig names
+    rdd.keyBy(r => {
+      if (r.getReadMapped) {
+        ReferencePosition(r)
+      } else {
+        ReferencePosition("ZZZ%s".format(r.getReadName), 0)
+      }
+    }).sortByKey().map(p => p._2)
+  }
+
+  def adamMarkDuplicates(): RDD[AlignmentRecord] = MarkDuplicatesInDriver.time {
+    GATKSparkMarkDuplicates(rdd)
+  }
+
+  /**
+   * Runs base quality score recalibration on a set of reads. Uses a table of
+   * known SNPs to mask true variation during the recalibration process.
+   *
+   * @param knownSnps A table of known SNPs to mask valid variants.
+   * @param observationDumpFile An optional local path to dump recalibration
+   *                            observations to.
+   * @return Returns an RDD of recalibrated reads.
+   */
+  def adamBQSR(knownSnps: Broadcast[SnpTable],
+               observationDumpFile: Option[String] = None): RDD[AlignmentRecord] = BQSRInDriver.time {
+    BaseQualityRecalibration(rdd, knownSnps, observationDumpFile)
+  }
+
+  /**
+   * Realigns indels using a concensus-based heuristic.
+   *
+   * @see RealignIndels
+   *
+   * @param isSorted If the input data is sorted, setting this parameter to true avoids a second sort.
+   * @param maxIndelSize The size of the largest indel to use for realignment.
+   * @param maxConsensusNumber The maximum number of consensus sequences to realign against per
+   * target region.
+   * @param lodThreshold Log-odds threhold to use when realigning; realignments are only finalized
+   * if the log-odds threshold is exceeded.
+   * @param maxTargetSize The maximum width of a single target region for realignment.
+   *
+   * @return Returns an RDD of mapped reads which have been realigned.
+   */
+  def adamRealignIndels(consensusModel: ConsensusGenerator = new ConsensusGeneratorFromReads,
+                        isSorted: Boolean = false,
+                        maxIndelSize: Int = 500,
+                        maxConsensusNumber: Int = 30,
+                        lodThreshold: Double = 5.0,
+                        maxTargetSize: Int = 3000): RDD[AlignmentRecord] = RealignIndelsInDriver.time {
+    RealignIndels(rdd, consensusModel, isSorted, maxIndelSize, maxConsensusNumber, lodThreshold)
+  }
+
+  // Returns a tuple of (failedQualityMetrics, passedQualityMetrics)
+  def adamFlagStat(): (FlagStatMetrics, FlagStatMetrics) = {
+    FlagStat(rdd)
+  }
+
+  /**
+   * Groups all reads by record group and read name
+   * @return SingleReadBuckets with primary, secondary and unmapped reads
+   */
+  def adamSingleReadBuckets(): RDD[SingleReadBucket] = {
+    SingleReadBucket(rdd)
+  }
+
+  /**
+   * Converts a set of records into an RDD containing the pairs of all unique tagStrings
+   * within the records, along with the count (number of records) which have that particular
+   * attribute.
+   *
+   * @return An RDD of attribute name / count pairs.
+   */
+  def adamCharacterizeTags(): RDD[(String, Long)] = {
+    rdd.flatMap(RichAlignmentRecord(_).tags.map(attr => (attr.tag, 1L))).reduceByKey(_ + _)
+  }
+
+  /**
+   * Calculates the set of unique attribute <i>values</i> that occur for the given
+   * tag, and the number of time each value occurs.
+   *
+   * @param tag The name of the optional field whose values are to be counted.
+   * @return A Map whose keys are the values of the tag, and whose values are the number of time each tag-value occurs.
+   */
+  def adamCharacterizeTagValues(tag: String): Map[Any, Long] = {
+    adamFilterRecordsWithTag(tag).flatMap(RichAlignmentRecord(_).tags.find(_.tag == tag)).map(
+      attr => Map(attr.value -> 1L)).reduce {
+        (map1: Map[Any, Long], map2: Map[Any, Long]) =>
+          MapTools.add(map1, map2)
+      }
+  }
+
+  /**
+   * Returns the subset of the ADAMRecords which have an attribute with the given name.
+   * @param tagName The name of the attribute to filter on (should be length 2)
+   * @return An RDD[Read] containing the subset of records with a tag that matches the given name.
+   */
+  def adamFilterRecordsWithTag(tagName: String): RDD[AlignmentRecord] = {
+    assert(tagName.length == 2,
+      "withAttribute takes a tagName argument of length 2; tagName=\"%s\"".format(tagName))
+    rdd.filter(RichAlignmentRecord(_).tags.exists(_.tag == tagName))
+  }
+
+  /**
+   * Saves these AlignmentRecords to two FASTQ files: one for the first mate in each pair, and the other for the second.
+   *
+   * @param fileName1 Path at which to save a FASTQ file containing the first mate of each pair.
+   * @param fileName2 Path at which to save a FASTQ file containing the second mate of each pair.
+   * @param validationStringency Iff strict, throw an exception if any read in this RDD is not accompanied by its mate.
+   */
+  def adamSaveAsPairedFastq(fileName1: String,
+                            fileName2: String,
+                            validationStringency: ValidationStringency = ValidationStringency.LENIENT,
+                            persistLevel: Option[StorageLevel] = None): Unit = {
+    def maybePersist[T](r: RDD[T]): Unit = {
+      persistLevel.foreach(r.persist(_))
+    }
+    def maybeUnpersist[T](r: RDD[T]): Unit = {
+      persistLevel.foreach(_ => r.unpersist())
+    }
+
+    maybePersist(rdd)
+    val numRecords = rdd.count()
+
+    val readsByID: RDD[(String, Iterable[AlignmentRecord])] =
+      rdd.groupBy(record => {
+        if (!AlignmentRecordConverter.readNameHasPairedSuffix(record))
+          record.getReadName.toString
+        else
+          record.getReadName.toString.dropRight(2)
+      })
+
+    if (validationStringency == ValidationStringency.STRICT) {
+      val readIDsWithCounts: RDD[(String, Int)] = readsByID.mapValues(_.size)
+      val unpairedReadIDsWithCounts: RDD[(String, Int)] = readIDsWithCounts.filter(_._2 != 2)
+      maybePersist(unpairedReadIDsWithCounts)
+
+      val numUnpairedReadIDsWithCounts: Long = unpairedReadIDsWithCounts.count()
+      if (numUnpairedReadIDsWithCounts != 0) {
+        val readNameOccurrencesMap: collection.Map[Int, Long] = unpairedReadIDsWithCounts.map(_._2).countByValue()
+        throw new Exception(
+          "Found %d read names that don't occur exactly twice:\n%s\n\nSamples:\n%s".format(
+            numUnpairedReadIDsWithCounts,
+            readNameOccurrencesMap.map(p => "%dx:\t%d".format(p._1, p._2)).mkString("\t", "\n\t", ""),
+            unpairedReadIDsWithCounts.take(100).map(_._1).mkString("\t", "\n\t", "")
+          )
+        )
+      }
+    }
+
+    val pairedRecords: RDD[AlignmentRecord] = readsByID.filter(_._2.size == 2).map(_._2).flatMap(x => x)
+    maybePersist(pairedRecords)
+    val numPairedRecords = pairedRecords.count()
+
+    maybeUnpersist(rdd.unpersist())
+
+    val firstInPairRecords: RDD[AlignmentRecord] = pairedRecords.filter(_.getFirstOfPair)
+    maybePersist(firstInPairRecords)
+    val numFirstInPairRecords = firstInPairRecords.count()
+
+    val secondInPairRecords: RDD[AlignmentRecord] = pairedRecords.filter(_.getSecondOfPair)
+    maybePersist(secondInPairRecords)
+    val numSecondInPairRecords = secondInPairRecords.count()
+
+    maybeUnpersist(pairedRecords)
+
+    log.info(
+      "%d/%d records are properly paired: %d firsts, %d seconds".format(
+        numPairedRecords,
+        numRecords,
+        numFirstInPairRecords,
+        numSecondInPairRecords
+      )
+    )
+
+    if (validationStringency == ValidationStringency.STRICT) {
+      firstInPairRecords.foreach(read =>
+        if (read.getSecondOfPair)
+          throw new Exception("Read %s found with first- and second-of-pair set".format(read.getReadName))
+      )
+      secondInPairRecords.foreach(read =>
+        if (read.getFirstOfPair)
+          throw new Exception("Read %s found with first- and second-of-pair set".format(read.getReadName))
+      )
+    }
+
+    assert(
+      numFirstInPairRecords == numSecondInPairRecords,
+      "Different numbers of first- and second-reads: %d vs. %d".format(numFirstInPairRecords, numSecondInPairRecords)
+    )
+
+    val arc = new AlignmentRecordConverter
+
+    firstInPairRecords
+      .sortBy(_.getReadName.toString)
+      .map(record => arc.convertToFastq(record, maybeAddSuffix = true))
+      .saveAsTextFile(fileName1)
+
+    secondInPairRecords
+      .sortBy(_.getReadName.toString)
+      .map(record => arc.convertToFastq(record, maybeAddSuffix = true))
+      .saveAsTextFile(fileName2)
+
+    maybeUnpersist(firstInPairRecords)
+    maybeUnpersist(secondInPairRecords)
+  }
+
+  /**
+   * Saves reads in FASTQ format.
+   *
+   * @param fileName Path to save files at.
+   * @param sort Whether to sort the FASTQ files by read name or not. Defaults
+   *             to false. Sorting the output will recover pair order, if desired.
+   */
+  def adamSaveAsFastq(fileName: String,
+                      fileName2Opt: Option[String] = None,
+                      sort: Boolean = false,
+                      validationStringency: ValidationStringency = ValidationStringency.LENIENT,
+                      persistLevel: Option[StorageLevel] = None) {
+    log.info("Saving data in FASTQ format.")
+    fileName2Opt match {
+      case Some(fileName2) =>
+        adamSaveAsPairedFastq(
+          fileName,
+          fileName2,
+          validationStringency = validationStringency,
+          persistLevel = persistLevel
+        )
+      case _ =>
+        val arc = new AlignmentRecordConverter
+
+        // sort the rdd if desired
+        val outputRdd = if (sort || fileName2Opt.isDefined) {
+          rdd.sortBy(_.getReadName.toString)
+        } else {
+          rdd
+        }
+
+        // convert the rdd and save as a text file
+        outputRdd
+          .map(record => arc.convertToFastq(record))
+          .saveAsTextFile(fileName)
+    }
+  }
+
+  /**
+   * Reassembles read pairs from two sets of unpaired reads. The assumption is that the two sets
+   * were _originally_ paired together.
+   *
+   * @note The RDD that this is called on should be the RDD with the first read from the pair.
+   *
+   * @param secondPairRdd The rdd containing the second read from the pairs.
+   * @param validationStringency How stringently to validate the reads.
+   * @return Returns an RDD with the pair information recomputed.
+   */
+  def adamRePairReads(secondPairRdd: RDD[AlignmentRecord],
+                      validationStringency: ValidationStringency = ValidationStringency.LENIENT): RDD[AlignmentRecord] = {
+    // cache rdds
+    val firstPairRdd = rdd.cache()
+    secondPairRdd.cache()
+
+    val firstRDDKeyedByReadName = firstPairRdd.keyBy(_.getReadName.toString.dropRight(2))
+    val secondRDDKeyedByReadName = secondPairRdd.keyBy(_.getReadName.toString.dropRight(2))
+
+    // all paired end reads should have the same name, except for the last two
+    // characters, which will be _1/_2
+    val joinedRDD: RDD[(String, (AlignmentRecord, AlignmentRecord))] =
+      if (validationStringency == ValidationStringency.STRICT) {
+        firstRDDKeyedByReadName.cogroup(secondRDDKeyedByReadName).map {
+          case (readName, (firstReads, secondReads)) =>
+            (firstReads.toList, secondReads.toList) match {
+              case (firstRead :: Nil, secondRead :: Nil) =>
+                (readName, (firstRead, secondRead))
+              case _ =>
+                throw new Exception(
+                  "Expected %d first reads and %d second reads for name %s; expected exactly one of each:\n%s\n%s".format(
+                    firstReads.size,
+                    secondReads.size,
+                    readName,
+                    firstReads.map(_.getReadName.toString).mkString("\t", "\n\t", ""),
+                    secondReads.map(_.getReadName.toString).mkString("\t", "\n\t", "")
+                  )
+                )
+            }
+        }
+
+      } else {
+        firstRDDKeyedByReadName.join(secondRDDKeyedByReadName)
+      }
+
+    val finalRdd = joinedRDD
+      .flatMap(kv => Seq(
+        AlignmentRecord.newBuilder(kv._2._1)
+          .setReadPaired(true)
+          .setProperPair(true)
+          .setFirstOfPair(true)
+          .setSecondOfPair(false)
+          .build(),
+        AlignmentRecord.newBuilder(kv._2._2)
+          .setReadPaired(true)
+          .setProperPair(true)
+          .setFirstOfPair(false)
+          .setSecondOfPair(true)
+          .build()
+      ))
+
+    // uncache temp rdds
+    firstPairRdd.unpersist()
+    secondPairRdd.unpersist()
+
+    // return
+    finalRdd
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/FlagStat.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/FlagStat.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/FlagStat.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/FlagStat.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,120 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.util.Util._
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+object FlagStatMetrics {
+  val emptyFailedQuality = new FlagStatMetrics(0, DuplicateMetrics.empty, DuplicateMetrics.empty, 0, 0, 0, 0, 0, 0, 0, 0, 0, true)
+  val emptyPassedQuality = new FlagStatMetrics(0, DuplicateMetrics.empty, DuplicateMetrics.empty, 0, 0, 0, 0, 0, 0, 0, 0, 0, false)
+}
+
+object DuplicateMetrics {
+  val empty = new DuplicateMetrics(0, 0, 0, 0)
+
+  def apply(record: AlignmentRecord): (DuplicateMetrics, DuplicateMetrics) = {
+    import FlagStat.b2i
+
+    def isPrimary(record: AlignmentRecord): Boolean = {
+      record.getDuplicateRead && record.getPrimaryAlignment
+    }
+    def isSecondary(record: AlignmentRecord): Boolean = {
+      record.getDuplicateRead && !record.getPrimaryAlignment
+    }
+
+    def duplicateMetrics(f: (AlignmentRecord) => Boolean) = {
+      new DuplicateMetrics(b2i(f(record)),
+        b2i(f(record) && record.getReadMapped && record.getMateMapped),
+        b2i(f(record) && record.getReadMapped && !record.getMateMapped),
+        b2i(f(record) && (!isSameContig(record.getContig, record.getMateContig))))
+    }
+    (duplicateMetrics(isPrimary), duplicateMetrics(isSecondary))
+  }
+}
+
+case class DuplicateMetrics(total: Long, bothMapped: Long, onlyReadMapped: Long, crossChromosome: Long) {
+  def +(that: DuplicateMetrics): DuplicateMetrics = {
+    new DuplicateMetrics(total + that.total,
+      bothMapped + that.bothMapped,
+      onlyReadMapped + that.onlyReadMapped,
+      crossChromosome + that.crossChromosome)
+  }
+}
+
+case class FlagStatMetrics(total: Long, duplicatesPrimary: DuplicateMetrics, duplicatesSecondary: DuplicateMetrics,
+                           mapped: Long, pairedInSequencing: Long,
+                           read1: Long, read2: Long, properlyPaired: Long, withSelfAndMateMapped: Long,
+                           singleton: Long, withMateMappedToDiffChromosome: Long,
+                           withMateMappedToDiffChromosomeMapQ5: Long, failedQuality: Boolean) {
+  def +(that: FlagStatMetrics): FlagStatMetrics = {
+    assert(failedQuality == that.failedQuality, "Can't reduce passedVendorQuality with different failedQuality values")
+    new FlagStatMetrics(total + that.total,
+      duplicatesPrimary + that.duplicatesPrimary,
+      duplicatesSecondary + that.duplicatesSecondary,
+      mapped + that.mapped,
+      pairedInSequencing + that.pairedInSequencing,
+      read1 + that.read1,
+      read2 + that.read2,
+      properlyPaired + that.properlyPaired,
+      withSelfAndMateMapped + that.withSelfAndMateMapped,
+      singleton + that.singleton,
+      withMateMappedToDiffChromosome + that.withMateMappedToDiffChromosome,
+      withMateMappedToDiffChromosomeMapQ5 + that.withMateMappedToDiffChromosomeMapQ5,
+      failedQuality)
+  }
+}
+
+object FlagStat {
+
+  def b2i(boolean: Boolean) = if (boolean) 1 else 0
+
+  def apply(rdd: RDD[AlignmentRecord]) = {
+    rdd.map {
+      p =>
+        val mateMappedToDiffChromosome =
+          p.getReadPaired && p.getReadMapped && p.getMateMapped && !isSameContig(p.getContig, p.getMateContig)
+        val (primaryDuplicates, secondaryDuplicates) = DuplicateMetrics(p)
+        new FlagStatMetrics(1,
+          primaryDuplicates, secondaryDuplicates,
+          b2i(p.getReadMapped),
+          b2i(p.getReadPaired),
+          b2i(p.getReadPaired && p.getFirstOfPair),
+          b2i(p.getReadPaired && p.getSecondOfPair),
+          b2i(p.getReadPaired && p.getProperPair),
+          b2i(p.getReadPaired && p.getReadMapped && p.getMateMapped),
+          b2i(p.getReadPaired && p.getReadMapped && !p.getMateMapped),
+          b2i(mateMappedToDiffChromosome),
+          b2i(mateMappedToDiffChromosome && p.getMapq >= 5),
+          p.getFailedVendorQualityChecks)
+    }.aggregate((FlagStatMetrics.emptyFailedQuality, FlagStatMetrics.emptyPassedQuality))(
+      seqOp = {
+        (a, b) =>
+          if (b.failedQuality) {
+            (a._1 + b, a._2)
+          } else {
+            (a._1, a._2 + b)
+          }
+      },
+      combOp = {
+        (a, b) =>
+          (a._1 + b._1, a._2 + b._2)
+      })
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/GATKSparkMarkDuplicates.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/GATKSparkMarkDuplicates.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/GATKSparkMarkDuplicates.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/GATKSparkMarkDuplicates.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,118 @@
+/*
+ * Copyright (c) 2016 NCIC, Institute of Computing Technology, Chinese Academy of Sciences
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * New implement of picard mark duplicates
+ * Author: lhy
+ * Modified: wbc 4/30
+ */
+private[rdd] object GATKSparkMarkDuplicates extends Serializable {
+
+  // Calculates the sum of the phred scores that are greater than or equal to 15
+  def score(record: AlignmentRecord): Int = {
+    record.qualityScores.filter(15 <=).sum
+  }
+
+  def apply(rdd: RDD[AlignmentRecord]): RDD[AlignmentRecord] = {
+    // keep all unmapped reads, and append it at tail. But I don't think it can be used in the future.
+    val unmapped = rdd.filter(r => !r.getReadMapped || r.getSecondaryAlignment)
+    val mapped = rdd.filter(r => (r.getReadMapped && !r.getSecondaryAlignment) )
+
+    // paired reads. If both reads in paire-end reads has same coordiname and orientation,
+    // the pair with lower map score will be marked as duplicate reads.
+    val pairReads = mapped.filter(r => r.getMateMapped).groupBy(r => r.getReadName)
+      .map(r => r._2).groupBy(MyReferencePositionPair(_)).flatMap(kv => {
+      val pairs = kv._2.toArray
+      var maxScorePos = 0
+      var maxScore = pairs(0).map(score).sum
+      for(i <- 1 to (pairs.length-1)) {
+        val nscore = pairs(i).map(score).sum
+        if(nscore > maxScore) {
+          maxScore = nscore
+          maxScorePos = i
+        }
+      }
+      var reads = List[AlignmentRecord]()
+      for(i <- 0 to (pairs.length-1)) {
+        if(i != maxScorePos) {
+          val nscore = pairs(i).map(score).sum
+          pairs(i).foreach(r => {
+            r.setDuplicateRead(true)
+            reads ::= r
+          })
+        }
+      }
+      pairs(maxScorePos).foreach(r => {
+        r.setDuplicateRead(false)
+        reads ::= r
+      })
+      reads
+    })
+
+    // If a fragment read has same coordinate and orientation with a read which is member of a pair-ends, mark it as duplicate no matter how much it scores.
+    // If two fragments reads has same coordinate and orientation, mark the one with lower mapped score as duplicate.
+    val fragReads = mapped.groupBy(r => MyReferencePosition(r))
+      .flatMap(kv => {
+      val frags = kv._2.toArray
+      val containPair = (frags.filter(r => r.getMateMapped).size) > 0
+      var reads = List[AlignmentRecord]()
+      if(containPair) {
+        frags.foreach(r => {
+          if(!r.getMateMapped) {
+            r.setDuplicateRead(true)
+            reads ::= r
+          }
+        })
+      } else {
+        var maxScorePos = 0
+        var maxScore = score(frags(0))
+        for(i <- 1 to (frags.length-1)) {
+          val nscore = score(frags(i))
+          if(nscore > maxScore) {
+            maxScore = nscore
+            maxScorePos = i
+          }
+        }
+        for(i <- 0 to (frags.length-1)) {
+          if(i != maxScorePos) {
+            frags(i).setDuplicateRead(true)
+            reads ::= frags(i)
+          }
+        }
+        frags(maxScorePos).setDuplicateRead(false)
+        reads ::= frags(maxScorePos)
+      }
+      reads
+    })
+    pairReads ++ fragReads ++ unmapped
+  }
+
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/MarkDuplicates.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/MarkDuplicates.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/MarkDuplicates.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/MarkDuplicates.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,140 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read
+
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.models.{
+  ReferencePosition,
+  ReferencePositionPair,
+  SingleReadBucket
+}
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+private[rdd] object MarkDuplicates extends Serializable {
+
+  private def markReadsInBucket(bucket: SingleReadBucket, primaryAreDups: Boolean, secondaryAreDups: Boolean) {
+    bucket.primaryMapped.foreach(read => {
+      read.setDuplicateRead(primaryAreDups)
+    })
+    bucket.secondaryMapped.foreach(read => {
+      read.setDuplicateRead(secondaryAreDups)
+    })
+    bucket.unmapped.foreach(read => {
+      read.setDuplicateRead(false)
+    })
+  }
+
+  // Calculates the sum of the phred scores that are greater than or equal to 15
+  def score(record: AlignmentRecord): Int = {
+    record.qualityScores.filter(15 <=).sum
+  }
+
+  private def scoreBucket(bucket: SingleReadBucket): Int = {
+    bucket.primaryMapped.map(score).sum
+  }
+
+  private def markReads(reads: Iterable[(ReferencePositionPair, SingleReadBucket)], areDups: Boolean) {
+    markReads(reads, primaryAreDups = areDups, secondaryAreDups = areDups, ignore = None)
+  }
+
+  private def markReads(reads: Iterable[(ReferencePositionPair, SingleReadBucket)], primaryAreDups: Boolean, secondaryAreDups: Boolean,
+                        ignore: Option[(ReferencePositionPair, SingleReadBucket)] = None) = MarkReads.time {
+    reads.foreach(read => {
+      if (ignore.isEmpty || read != ignore.get) {
+        markReadsInBucket(read._2, primaryAreDups, secondaryAreDups)
+      }
+    })
+  }
+
+  def apply(rdd: RDD[AlignmentRecord]): RDD[AlignmentRecord] = {
+
+    // Group by library and left position
+    def leftPositionAndLibrary(p: (ReferencePositionPair, SingleReadBucket)): (Option[ReferencePosition], String) = {
+      (p._1.read1refPos, p._2.allReads.head.getRecordGroupLibrary)
+    }
+
+    // Group by right position
+    def rightPosition(p: (ReferencePositionPair, SingleReadBucket)): Option[ReferencePosition] = {
+      p._1.read2refPos
+    }
+
+    //rdd.adamSingleReadBuckets().keyBy(ReferencePositionPair(_)).saveAsTextFile("mytest.txt")
+    rdd.adamSingleReadBuckets()
+      .keyBy(ReferencePositionPair(_))
+      .groupBy(leftPositionAndLibrary)
+      .flatMap(kv => PerformDuplicateMarking.time {
+
+        val leftPos: Option[ReferencePosition] = kv._1._1
+        val readsAtLeftPos: Iterable[(ReferencePositionPair, SingleReadBucket)] = kv._2
+
+
+        leftPos match {
+
+          // These are all unmapped reads. There is no way to determine if they are duplicates
+          case None =>
+            markReads(readsAtLeftPos, areDups = false)
+
+          // These reads have their left position mapped
+          case Some(leftPosWithOrientation) =>
+
+            val readsByRightPos = readsAtLeftPos.groupBy(rightPosition)
+
+            val groupCount = readsByRightPos.size
+
+            readsByRightPos.foreach(e => {
+
+              val rightPos = e._1
+              val reads = e._2
+
+              val groupIsFragments = rightPos.isEmpty
+
+              // We have no pairs (only fragments) if the current group is a group of fragments
+              // and there is only one group in total
+              val onlyFragments = groupIsFragments && groupCount == 1
+
+              // If there are only fragments then score the fragments. Otherwise, if there are not only
+              // fragments (there are pairs as well) mark all fragments as duplicates.
+              // If the group does not contain fragments (it contains pairs) then always score it.
+              if (onlyFragments || !groupIsFragments) {
+                // Find the highest-scoring read and mark it as not a duplicate. Mark all the other reads in this group as duplicates.
+                val highestScoringRead = reads.max(ScoreOrdering)
+                markReadsInBucket(highestScoringRead._2, primaryAreDups = false, secondaryAreDups = true)
+                markReads(reads, primaryAreDups = true, secondaryAreDups = true, ignore = Some(highestScoringRead))
+              } else {
+                markReads(reads, areDups = true)
+              }
+            })
+        }
+
+        readsAtLeftPos.flatMap(read => { read._2.allReads })
+
+      })
+
+  }
+
+  private object ScoreOrdering extends Ordering[(ReferencePositionPair, SingleReadBucket)] {
+    override def compare(x: (ReferencePositionPair, SingleReadBucket), y: (ReferencePositionPair, SingleReadBucket)): Int = {
+      // This is safe because scores are Ints
+      scoreBucket(x._2) - scoreBucket(y._2)
+    }
+  }
+
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/IndelRealignmentTarget.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/IndelRealignmentTarget.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/IndelRealignmentTarget.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/IndelRealignmentTarget.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,217 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.realignment
+
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import htsjdk.samtools.CigarOperator
+import org.apache.spark.Logging
+import org.bdgenomics.adam.models.ReferenceRegion
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.formats.avro.AlignmentRecord
+import org.bdgenomics.adam.instrumentation.Timers._
+import scala.collection.immutable.TreeSet
+
+object ZippedTargetOrdering extends Ordering[(IndelRealignmentTarget, Int)] {
+
+  /**
+   * Order two indel realignment targets by earlier starting position.
+   *
+   * @param a Indel realignment target to compare.
+   * @param b Indel realignment target to compare.
+   * @return Comparison done by starting position.
+   */
+  def compare(a: (IndelRealignmentTarget, Int), b: (IndelRealignmentTarget, Int)): Int = {
+    TargetOrdering.compare(a._1, b._1)
+  }
+}
+
+object TargetOrdering extends Ordering[IndelRealignmentTarget] {
+
+  /**
+   * Order two indel realignment targets by earlier starting position.
+   *
+   * @param a Indel realignment target to compare.
+   * @param b Indel realignment target to compare.
+   * @return Comparison done by starting position.
+   */
+  def compare(a: IndelRealignmentTarget, b: IndelRealignmentTarget): Int = a.readRange compareTo b.readRange
+
+  /**
+   * Check to see if an indel realignment target contains the given read.
+   *
+   * @param target Realignment target to compare.
+   * @param read Read to compare.
+   * @return True if read alignment is contained in target span.
+   */
+  def contains(target: IndelRealignmentTarget, read: AlignmentRecord): Boolean = {
+    if (read.getReadMapped) {
+      target.readRange.overlaps(ReferenceRegion(read))
+    } else {
+      false
+    }
+  }
+
+  /**
+   * Compares a read to an indel realignment target to see if it starts before the start of the indel realignment target.
+   *
+   * @param target Realignment target to compare.
+   * @param read Read to compare.
+   * @return True if start of read is before the start of the indel alignment target.
+   */
+  def lt(target: IndelRealignmentTarget, read: RichAlignmentRecord): Boolean = {
+    if (read.getReadMapped) {
+      target.readRange.compareTo(ReferenceRegion(read.record)) < 0
+    } else {
+      false
+    }
+  }
+
+  /**
+   * Compares two indel realignment targets to see if they overlap.
+   *
+   * @param a Indel realignment target to compare.
+   * @param b Indel realignment target to compare.
+   * @return True if two targets overlap.
+   */
+  def overlap(a: IndelRealignmentTarget, b: IndelRealignmentTarget): Boolean = {
+    (a.variation.isDefined && a.variation.forall(_.overlaps(b.readRange))) ||
+      (b.variation.isDefined && b.variation.forall(_.overlaps(a.readRange)))
+  }
+}
+
+object IndelRealignmentTarget {
+
+  /**
+   * Generates 1+ indel realignment targets from a single read.
+   *
+   * @param read Read to use for generation.
+   * @param maxIndelSize Maximum allowable size of an indel.
+   * @return Set of generated realignment targets.
+   */
+  def apply(read: RichAlignmentRecord,
+            maxIndelSize: Int): Seq[IndelRealignmentTarget] = CreateIndelRealignmentTargets.time {
+
+    val region = ReferenceRegion(read.record)
+    val refId = read.record.getContig.getContigName
+    var pos = List[ReferenceRegion]()
+    var referencePos = read.record.getStart
+    val cigar = read.samtoolsCigar
+
+    cigar.getCigarElements.foreach(cigarElement =>
+      cigarElement.getOperator match {
+        // INSERT
+        case CigarOperator.I =>
+          if (cigarElement.getLength <= maxIndelSize) {
+            pos ::= ReferenceRegion(refId, referencePos, referencePos + 1)
+          }
+        // DELETE
+        case CigarOperator.D =>
+          if (cigarElement.getLength <= maxIndelSize) {
+            pos ::= ReferenceRegion(refId, referencePos, referencePos + cigarElement.getLength)
+          }
+          referencePos += cigarElement.getLength
+        case _ =>
+          if (cigarElement.getOperator.consumesReferenceBases()) {
+            referencePos += cigarElement.getLength
+          }
+      })
+
+    // if we have indels, emit those targets, else emit a target for this read
+    if (pos.length == 0) {
+      Seq(new IndelRealignmentTarget(None, region))
+    } else {
+      pos.map(ir => new IndelRealignmentTarget(Some(ir), region))
+        .toSeq
+    }
+  }
+}
+
+class IndelRealignmentTarget(val variation: Option[ReferenceRegion],
+                             val readRange: ReferenceRegion) extends Logging {
+
+  override def toString(): String = {
+    variation + " over " + readRange
+  }
+
+  /**
+   * Merges two indel realignment targets.
+   *
+   * @param target Target to merge in.
+   * @return Merged target.
+   */
+  def merge(target: IndelRealignmentTarget): IndelRealignmentTarget = {
+    assert(readRange.isAdjacent(target.readRange) || readRange.overlaps(target.readRange),
+      "Targets do not overlap, and therefore cannot be merged.")
+
+    val newVar = if (variation.isDefined && target.variation.isDefined) {
+      Some(variation.get.hull(target.variation.get))
+    } else if (variation.isDefined) {
+      variation
+    } else if (target.variation.isDefined) {
+      target.variation
+    } else {
+      None
+    }
+
+    new IndelRealignmentTarget(newVar, readRange.merge(target.readRange))
+  }
+
+  def isEmpty: Boolean = {
+    variation.isEmpty
+  }
+}
+
+class TargetSetSerializer extends Serializer[TargetSet] {
+
+  def write(kryo: Kryo, output: Output, obj: TargetSet) = {
+    kryo.writeClassAndObject(output, obj.set.toList)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[TargetSet]): TargetSet = {
+    new TargetSet(new TreeSet()(TargetOrdering)
+      .union(kryo.readClassAndObject(input).asInstanceOf[List[IndelRealignmentTarget]].toSet))
+  }
+}
+
+class ZippedTargetSetSerializer extends Serializer[ZippedTargetSet] {
+
+  def write(kryo: Kryo, output: Output, obj: ZippedTargetSet) = {
+    kryo.writeClassAndObject(output, obj.set.toList)
+  }
+
+  def read(kryo: Kryo, input: Input, klazz: Class[ZippedTargetSet]): ZippedTargetSet = {
+    new ZippedTargetSet(new TreeSet()(ZippedTargetOrdering)
+      .union(kryo.readClassAndObject(input).asInstanceOf[List[(IndelRealignmentTarget, Int)]].toSet))
+  }
+}
+
+object TargetSet {
+  def apply(): TargetSet = {
+    new TargetSet(TreeSet[IndelRealignmentTarget]()(TargetOrdering))
+  }
+}
+
+// These two case classes are needed to get around some serialization issues
+case class TargetSet(set: TreeSet[IndelRealignmentTarget]) extends Serializable {
+}
+
+case class ZippedTargetSet(set: TreeSet[(IndelRealignmentTarget, Int)]) extends Serializable {
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignIndels.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignIndels.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignIndels.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignIndels.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,499 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.realignment
+
+import htsjdk.samtools.{ Cigar, CigarElement, CigarOperator }
+import org.apache.spark.Logging
+import org.apache.spark.rdd.MetricsContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.algorithms.consensus.{ ConsensusGenerator, ConsensusGeneratorFromReads }
+import org.bdgenomics.adam.models.ReferenceRegion._
+import org.bdgenomics.adam.models.{ Consensus, ReferencePosition, ReferenceRegion }
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.util.ImplicitJavaConversions._
+import org.bdgenomics.adam.util.MdTag
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.formats.avro.AlignmentRecord
+import scala.annotation.tailrec
+import scala.collection.immutable.{ NumericRange, TreeSet }
+import scala.collection.mutable
+import scala.util.Random
+
+private[rdd] object RealignIndels extends Serializable with Logging {
+
+  /**
+   * Realigns an RDD of reads.
+   *
+   * @param rdd RDD of reads to realign.
+   * @return RDD of realigned reads.
+   */
+  def apply(rdd: RDD[AlignmentRecord],
+            consensusModel: ConsensusGenerator = new ConsensusGeneratorFromReads,
+            dataIsSorted: Boolean = false,
+            maxIndelSize: Int = 500,
+            maxConsensusNumber: Int = 30,
+            lodThreshold: Double = 5.0,
+            maxTargetSize: Int = 3000): RDD[AlignmentRecord] = {
+    new RealignIndels(consensusModel,
+      dataIsSorted,
+      maxIndelSize,
+      maxConsensusNumber,
+      lodThreshold,
+      maxTargetSize).realignIndels(rdd)
+  }
+
+  /**
+   * Method to map a record to an indel realignment target. Returns the index of the target to align to if the read has a
+   * target and should be realigned, else returns the "empty" target (denoted by a negative index).
+   *
+   * @note Generally, this function shouldn't be called directly---for most cases, prefer mapTargets.
+   * @param read Read to check.
+   * @param targets Sorted set of realignment targets.
+   * @return If overlapping target is found, returns that target. Else, returns the "empty" target.
+   *
+   * @see mapTargets
+   */
+  @tailrec final def mapToTarget(read: RichAlignmentRecord,
+                                 targets: TreeSet[(IndelRealignmentTarget, Int)]): Int = {
+    // Perform tail call recursive binary search
+    if (targets.size == 1) {
+      if (TargetOrdering.contains(targets.head._1, read)) {
+        // if there is overlap, return the overlapping target
+        targets.head._2
+      } else {
+        // else, return an empty target (negative index)
+        // to prevent key skew, split up by max indel alignment length
+        (-1 - (read.record.getStart / 3000L)).toInt
+      }
+    } else {
+      // split the set and recurse
+      val (head, tail) = targets.splitAt(targets.size / 2)
+      val reducedSet = if (TargetOrdering.lt(tail.head._1, read)) {
+        head
+      } else {
+        tail
+      }
+      mapToTarget(read, reducedSet)
+    }
+  }
+
+  /**
+   * This method wraps mapToTarget(RichADAMRecord, TreeSet[Tuple2[IndelRealignmentTarget, Int]]) for
+   * serialization purposes.
+   *
+   * @param read Read to check.
+   * @param targets Wrapped zipped indel realignment target.
+   * @return Target if an overlapping target is found, else the empty target.
+   *
+   * @see mapTargets
+   */
+  def mapToTarget(read: RichAlignmentRecord,
+                  targets: ZippedTargetSet): Int = {
+    mapToTarget(read, targets.set)
+  }
+
+  /**
+   * Method to map a target index to an indel realignment target.
+   *
+   * @note Generally, this function shouldn't be called directly---for most cases, prefer mapTargets.
+   * @note This function should not be called in a context where target set serialization is needed.
+   * Instead, call mapToTarget(Int, ZippedTargetSet), which wraps this function.
+   *
+   * @param targetIndex Index of target.
+   * @param targets Set of realignment targets.
+   * @return Indel realignment target.
+   *
+   * @see mapTargets
+   */
+  def mapToTargetUnpacked(targetIndex: Int,
+                          targets: TreeSet[(IndelRealignmentTarget, Int)]): Option[IndelRealignmentTarget] = {
+    if (targetIndex < 0) {
+      None
+    } else {
+      Some(targets.filter(p => p._2 == targetIndex).head._1)
+    }
+  }
+
+  /**
+   * Wrapper for mapToTarget(Int, TreeSet[Tuple2[IndelRealignmentTarget, Int]]) for contexts where
+   * serialization is needed.
+   *
+   * @param targetIndex Index of target.
+   * @param targets Set of realignment targets.
+   * @return Indel realignment target.
+   *
+   * @see mapTargets
+   */
+  def mapToTarget(targetIndex: Int, targets: ZippedTargetSet): Option[IndelRealignmentTarget] = {
+    mapToTargetUnpacked(targetIndex, targets.set)
+  }
+
+  /**
+   * Maps reads to targets. Wraps both mapToTarget functions together and handles target index creation and broadcast.
+   *
+   * @note This function may return multiple sets of reads mapped to empty targets. This is intentional. For typical workloads, there
+   * will be many more reads that map to the empty target (reads that do not need to be realigned) than reads that need to be realigned.
+   * Thus, we must spread the reads that do not need to be realigned across multiple empty targets to reduce imbalance.
+   *
+   * @param rich_rdd RDD containing RichADAMRecords which are to be mapped to a realignment target.
+   * @param targets Set of targets that are to be mapped against.
+   *
+   * @return A key-value pair RDD with realignment targets matched with sets of reads.
+   *
+   * @see mapToTarget
+   */
+  def mapTargets(rich_rdd: RDD[RichAlignmentRecord], targets: TreeSet[IndelRealignmentTarget]): RDD[(Option[IndelRealignmentTarget], Iterable[RichAlignmentRecord])] = MapTargets.time {
+    val tmpZippedTargets = targets.zip(0 until targets.count(t => true))
+    var tmpZippedTargets2 = new TreeSet[(IndelRealignmentTarget, Int)]()(ZippedTargetOrdering)
+    tmpZippedTargets.foreach(t => tmpZippedTargets2 = tmpZippedTargets2 + t)
+
+    val zippedTargets = new ZippedTargetSet(tmpZippedTargets2)
+
+    // group reads by target
+    val broadcastTargets = rich_rdd.context.broadcast(zippedTargets)
+    val readsMappedToTarget = rich_rdd.groupBy(mapToTarget(_, broadcastTargets.value))
+      .map(kv => {
+      val (k, v) = kv
+
+      val target = mapToTarget(k, broadcastTargets.value)
+
+      (target, v)
+    })
+
+    readsMappedToTarget
+  }
+
+  /**
+   * From a set of reads, returns the reference sequence that they overlap.
+   */
+  def getReferenceFromReads(reads: Iterable[RichAlignmentRecord]): (String, Long, Long) = GetReferenceFromReads.time {
+    var tossedReads = 0
+
+    // get reference and range from a single read
+    val readRefs = reads.flatMap((r: RichAlignmentRecord) => {
+      if (r.mdTag.isDefined) {
+        Some((r.mdTag.get.getReference(r), r.getStart.toLong to r.getEnd))
+      } else {
+        log.warn("Discarding read " + r.record.getReadName + " during reference re-creation.")
+        tossedReads += 1
+        None
+      }
+    })
+      .toSeq
+      .sortBy(_._2.head)
+
+    // fold over sequences and append - sequence is sorted at start
+    val ref = readRefs.reverse.foldRight[(String, Long)](("", readRefs.head._2.head))((refReads: (String, NumericRange[Long]), reference: (String, Long)) => {
+      if (refReads._2.end < reference._2) {
+        reference
+      } else if (reference._2 >= refReads._2.head) {
+        (reference._1 + refReads._1.substring((reference._2 - refReads._2.head).toInt), refReads._2.end)
+      } else {
+        // there is a gap in the sequence
+        throw new IllegalArgumentException("Current sequence has a gap at " + reference._2 + "with " + refReads._2.head + "," + refReads._2.end +
+          ". Discarded " + tossedReads + " in region when reconstructing region; reads may not have MD tag attached.")
+      }
+    })
+
+    (ref._1, readRefs.head._2.head, ref._2)
+  }
+}
+
+import org.bdgenomics.adam.rdd.read.realignment.RealignIndels._
+
+private[rdd] class RealignIndels(val consensusModel: ConsensusGenerator = new ConsensusGeneratorFromReads,
+                                 val dataIsSorted: Boolean = false,
+                                 val maxIndelSize: Int = 500,
+                                 val maxConsensusNumber: Int = 30,
+                                 val lodThreshold: Double = 5.0,
+                                 val maxTargetSize: Int = 3000) extends Serializable with Logging {
+
+  /**
+   * Given a target group with an indel realignment target and a group of reads to realign, this method
+   * generates read consensuses and realigns reads if a consensus leads to a sufficient improvement.
+   *
+   * @param targetGroup A tuple consisting of an indel realignment target and a seq of reads
+   * @return A sequence of reads which have either been realigned if there is a sufficiently good alternative
+   * consensus, or not realigned if there is not a sufficiently good consensus.
+   */
+  def realignTargetGroup(targetGroup: (Option[IndelRealignmentTarget], Iterable[RichAlignmentRecord])): Iterable[RichAlignmentRecord] = RealignTargetGroup.time {
+    val (target, reads) = targetGroup
+
+    if (target.isEmpty) {
+      // if the indel realignment target is empty, do not realign
+      reads
+    } else {
+      // bootstrap realigned read set with the reads that need to be realigned
+      var realignedReads = reads.filter(r => r.mdTag.isDefined && !r.mdTag.get.hasMismatches)
+
+      // get reference from reads
+      val (reference, refStart, refEnd) = getReferenceFromReads(reads.map(r => new RichAlignmentRecord(r)))
+      val refRegion = ReferenceRegion(reads.head.record.getContig.getContigName, refStart, refEnd)
+
+      // preprocess reads and get consensus
+      val readsToClean = consensusModel.preprocessReadsForRealignment(reads.filter(r => !r.mdTag.isDefined || r.mdTag.get.hasMismatches),
+        reference,
+        refRegion)
+      var consensus = consensusModel.findConsensus(readsToClean)
+
+      // reduce count of consensus sequences
+      if (consensus.size > maxConsensusNumber) {
+        val r = new Random()
+        consensus = r.shuffle(consensus).take(maxConsensusNumber)
+      }
+
+      if (readsToClean.size > 0 && consensus.size > 0) {
+
+        // do not check realigned reads - they must match
+        val totalMismatchSumPreCleaning = readsToClean.map(sumMismatchQuality(_)).reduce(_ + _)
+
+        /* list to log the outcome of all consensus trials. stores:
+         *  - mismatch quality of reads against new consensus sequence
+         *  - the consensus sequence itself
+         *  - a map containing each realigned read and it's offset into the new sequence
+         */
+        var consensusOutcomes = List[(Int, Consensus, mutable.Map[RichAlignmentRecord, Int])]()
+
+        // loop over all consensuses and evaluate
+        consensus.foreach(c => {
+          // generate a reference sequence from the consensus
+          val consensusSequence = c.insertIntoReference(reference, refStart, refEnd)
+
+          // evaluate all reads against the new consensus
+          val sweptValues = readsToClean.map(r => {
+            val (qual, pos) = sweepReadOverReferenceForQuality(r.getSequence, consensusSequence, r.qualityScores)
+            val originalQual = sumMismatchQuality(r)
+
+            // if the read's mismatch quality improves over the original alignment, save
+            // its alignment in the consensus sequence, else store -1
+            if (qual < originalQual) {
+              (r, (qual, pos))
+            } else {
+              (r, (originalQual, -1))
+            }
+          })
+
+          // sum all mismatch qualities to get the total mismatch quality for this alignment
+          val totalQuality = sweptValues.map(_._2._1).reduce(_ + _)
+
+          // package data
+          var readMappings = mutable.Map[RichAlignmentRecord, Int]()
+          sweptValues.map(kv => (kv._1, kv._2._2)).foreach(m => {
+            readMappings += (m._1 -> m._2)
+          })
+
+          // add to outcome list
+          consensusOutcomes = (totalQuality, c, readMappings) :: consensusOutcomes
+        })
+
+        // perform reduction to pick the consensus with the lowest aggregated mismatch score
+        val bestConsensusTuple = consensusOutcomes.reduce((c1: (Int, Consensus, mutable.Map[RichAlignmentRecord, Int]), c2: (Int, Consensus, mutable.Map[RichAlignmentRecord, Int])) => {
+          if (c1._1 <= c2._1) {
+            c1
+          } else {
+            c2
+          }
+        })
+
+        val (bestConsensusMismatchSum, bestConsensus, bestMappings) = bestConsensusTuple
+
+        // check for a sufficient improvement in mismatch quality versus threshold
+        log.info("On " + refRegion + ", before realignment, sum was " + totalMismatchSumPreCleaning +
+          ", best realignment has " + bestConsensusMismatchSum)
+        val lodImprovement = (totalMismatchSumPreCleaning - bestConsensusMismatchSum).toDouble / 10.0
+        if (lodImprovement > lodThreshold) {
+          var realignedReadCount = 0
+
+          // if we see a sufficient improvement, realign the reads
+          val cleanedReads: Iterable[RichAlignmentRecord] = readsToClean.map(r => {
+
+            val builder: AlignmentRecord.Builder = AlignmentRecord.newBuilder(r)
+            val remapping = bestMappings(r)
+
+            // if read alignment is improved by aligning against new consensus, realign
+            if (remapping != -1) {
+
+              realignedReadCount += 1
+
+              // bump up mapping quality by 10
+              builder.setMapq(r.getMapq + 10)
+
+              // set new start to consider offset
+              builder.setStart(refStart + remapping)
+
+              // recompute cigar
+              val newCigar: Cigar = {
+                // if element overlaps with consensus indel, modify cigar with indel
+                val (idElement, endLength, endPenalty) = if (bestConsensus.index.start == bestConsensus.index.end - 1) {
+                  (new CigarElement(bestConsensus.consensus.length, CigarOperator.I),
+                    r.getSequence.length - bestConsensus.consensus.length - (bestConsensus.index.start - (refStart + remapping)),
+                    -bestConsensus.consensus.length)
+                } else {
+                  (new CigarElement((bestConsensus.index.end - 1 - bestConsensus.index.start).toInt, CigarOperator.D),
+                    r.getSequence.length - (bestConsensus.index.start - (refStart + remapping)),
+                    bestConsensus.consensus.length)
+                }
+
+                // compensate the end
+                builder.setEnd(refStart + remapping + r.getSequence.length + endPenalty)
+
+                val cigarElements = List[CigarElement](new CigarElement((bestConsensus.index.start - (refStart + remapping)).toInt, CigarOperator.M),
+                  idElement,
+                  new CigarElement(endLength.toInt, CigarOperator.M))
+
+                new Cigar(cigarElements)
+              }
+
+              // update mdtag and cigar
+              builder.setMismatchingPositions(MdTag.moveAlignment(r, newCigar, reference.drop(remapping), refStart + remapping).toString())
+              builder.setOldPosition(r.getStart())
+              builder.setOldCigar(r.getCigar())
+              builder.setCigar(newCigar.toString)
+              new RichAlignmentRecord(builder.build())
+            } else
+              new RichAlignmentRecord(builder.build())
+          })
+
+          log.info("On " + refRegion + ", realigned " + realignedReadCount + " reads to " +
+            bestConsensus + " due to LOD improvement of " + lodImprovement)
+
+          realignedReads = cleanedReads ++ realignedReads
+        } else {
+          log.info("On " + refRegion + ", skipping realignment due to insufficient LOD improvement (" +
+            lodImprovement + "for consensus " + bestConsensus)
+          realignedReads = readsToClean ++ realignedReads
+        }
+      }
+
+      // return all reads that we cleaned and all reads that were initially realigned
+      realignedReads
+    }
+  }
+
+  /**
+   * Sweeps the read across a reference sequence and evaluates the mismatch quality at each position. Returns
+   * the alignment offset that leads to the lowest mismatch quality score. Invariant: reference sequence must be
+   * longer than the read sequence.
+   *
+   * @param read Read to test.
+   * @param reference Reference sequence to sweep across.
+   * @param qualities Integer sequence of phred scaled base quality scores.
+   * @return Tuple of (mismatch quality score, alignment offset).
+   */
+  def sweepReadOverReferenceForQuality(read: String, reference: String, qualities: Seq[Int]): (Int, Int) = SweepReadOverReferenceForQuality.time {
+
+    var qualityScores = List[(Int, Int)]()
+
+    // calculate mismatch quality score for all admissable alignment offsets
+    for (i <- 0 until (reference.length - read.length)) {
+      val qualityScore = sumMismatchQualityIgnoreCigar(read, reference.substring(i, i + read.length), qualities)
+      qualityScores = (qualityScore, i) :: qualityScores
+    }
+
+    // perform reduction to get best quality offset
+    qualityScores.reduce((p1: (Int, Int), p2: (Int, Int)) => {
+      if (p1._1 < p2._1) {
+        p1
+      } else {
+        p2
+      }
+    })
+  }
+
+  /**
+   * Sums the mismatch quality of a read against a reference. Mismatch quality is defined as the sum
+   * of the base quality for all bases in the read that do not match the reference. This method
+   * ignores the cigar string, which treats indels as causing mismatches.
+   *
+   * @param read Read to evaluate.
+   * @param reference Reference sequence to look for mismatches against.
+   * @param qualities Sequence of base quality scores.
+   * @return Mismatch quality sum.
+   */
+  def sumMismatchQualityIgnoreCigar(read: String, reference: String, qualities: Seq[Int]): Int = {
+    val mismatchQualities = read.zip(reference)
+      .zip(qualities)
+      .filter(r => r._1._1 != r._1._2)
+      .map(_._2)
+
+    if (mismatchQualities.length > 0) {
+      mismatchQualities.reduce(_ + _)
+    } else {
+      0
+    }
+  }
+
+  /**
+   * Given a read, sums the mismatch quality against it's current alignment position.
+   * Does NOT ignore cigar.
+   *
+   * @param read Read over which to sum mismatch quality.
+   * @return Mismatch quality of read for current alignment.
+   */
+  def sumMismatchQuality(read: AlignmentRecord): Int = {
+    sumMismatchQualityIgnoreCigar(read.getSequence,
+      read.mdTag.get.getReference(read),
+      read.qualityScores)
+  }
+
+  /**
+   * Performs realignment for an RDD of reads. This includes target generation, read/target
+   * classification, and read realignment.
+   *
+   * @param rdd Reads to realign.
+   * @return Realigned read.
+   */
+  def realignIndels(rdd: RDD[AlignmentRecord]): RDD[AlignmentRecord] = {
+    val sortedRdd = if (dataIsSorted) {
+      rdd.filter(r => r.getReadMapped)
+    } else {
+      val sr = rdd.filter(r => r.getReadMapped)
+        .keyBy(ReferencePosition(_))
+        .sortByKey()
+      sr.map(kv => kv._2)
+    }
+
+    // we only want to convert once so let's get it over with
+    val richRdd = sortedRdd.map(new RichAlignmentRecord(_))
+    richRdd.cache()
+
+    // find realignment targets
+    log.info("Generating realignment targets...")
+    val targets: TreeSet[IndelRealignmentTarget] = RealignmentTargetFinder(richRdd,
+      maxIndelSize,
+      maxTargetSize)
+
+    // we should only attempt realignment if the target set isn't empty
+    if (targets.isEmpty) {
+      val readRdd = richRdd.map(r => r.record)
+      richRdd.unpersist()
+      readRdd
+    } else {
+      // map reads to targets
+      log.info("Grouping reads by target...")
+      val readsMappedToTarget = RealignIndels.mapTargets(richRdd, targets)
+      richRdd.unpersist()
+
+      // realign target groups
+      log.info("Sorting reads by reference in ADAM RDD")
+      readsMappedToTarget.flatMap(realignTargetGroup).map(r => r.record)
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignmentTargetFinder.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignmentTargetFinder.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignmentTargetFinder.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/realignment/RealignmentTargetFinder.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,121 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.realignment
+
+import org.apache.spark.Logging
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.instrumentation.Timers._
+import scala.annotation.tailrec
+import scala.collection.immutable.TreeSet
+
+object RealignmentTargetFinder {
+
+  /**
+   * Generates realignment targets from a set of reads.
+   *
+   * @param rdd RDD of reads to use in generating realignment targets.
+   * @return Sorted set of realignment targets.
+   */
+  def apply(rdd: RDD[RichAlignmentRecord],
+            maxIndelSize: Int = 500,
+            maxTargetSize: Int = 3000): TreeSet[IndelRealignmentTarget] = {
+    new RealignmentTargetFinder().findTargets(rdd, maxIndelSize, maxTargetSize).set
+  }
+}
+
+class RealignmentTargetFinder extends Serializable with Logging {
+
+  /**
+   * Joins two sorted sets of targets together. Is tail call recursive.
+   *
+   * @note This function should not be called in a context where target set serialization is needed.
+   * Instead, call joinTargets(TargetSet, TargetSet), which wraps this function.
+   *
+   * @param first A sorted set of realignment targets. This set must be ordered ahead of the
+   * second set.
+   * @param second A sorted set of realignment targets.
+   * @return A merged set of targets.
+   */
+  @tailrec protected final def joinTargets(
+    first: TreeSet[IndelRealignmentTarget],
+    second: TreeSet[IndelRealignmentTarget]): TreeSet[IndelRealignmentTarget] = {
+
+    if (first.isEmpty && second.isEmpty) {
+      TreeSet[IndelRealignmentTarget]()(TargetOrdering)
+    } else if (second.isEmpty) {
+      first
+    } else if (first.isEmpty) {
+      second
+    } else {
+      // if the two sets overlap, we must merge their head and tail elements,
+      // else we can just blindly append
+      if (!TargetOrdering.overlap(first.last, second.head)) {
+        first.union(second)
+      } else {
+        // merge the tail of the first set and the head of the second set and retry the merge
+        joinTargets(first - first.last + first.last.merge(second.head), second - second.head)
+      }
+    }
+  }
+
+  /**
+   * Wrapper for joinTargets(TreeSet[IndelRealignmentTarget], TreeSet[IndelRealignmentTarget])
+   * for contexts where serialization is needed.
+   *
+   * @param first A sorted set of realignment targets. This set must be ordered ahead of the
+   * second set.
+   * @param second A sorted set of realignment targets.
+   * @return A merged set of targets.
+   */
+  def joinTargets(first: TargetSet,
+                  second: TargetSet): TargetSet = JoinTargets.time {
+    new TargetSet(joinTargets(first.set, second.set))
+  }
+
+  /**
+   * Finds indel targets over a set of reads.
+   *
+   * @param reads An RDD containing reads to generate indel realignment targets from.
+   * @return An ordered set of indel realignment targets.
+   */
+  def findTargets(reads: RDD[RichAlignmentRecord],
+                  maxIndelSize: Int = 500,
+                  maxTargetSize: Int = 3000): TargetSet = FindTargets.time {
+
+    def createTargetSet(target: IndelRealignmentTarget): TargetSet = {
+      val tmp = new TreeSet()(TargetOrdering)
+      new TargetSet(tmp + target)
+    }
+
+    /* for each rod, generate an indel realignment target. we then filter out all "empty" targets: these
+     * are targets which do not show snp/indel evidence. we order these targets by reference position, and
+     * merge targets who have overlapping positions
+     */
+    val targets = reads.flatMap(IndelRealignmentTarget(_, maxIndelSize))
+      .filter(t => !t.isEmpty)
+
+    val targetSet: TargetSet = TargetSet(targets.mapPartitions(iter => SortTargets.time { iter.toArray.sorted(TargetOrdering).toIterator })
+      .map(createTargetSet)
+      .fold(TargetSet())((t1: TargetSet, t2: TargetSet) => joinTargets(t1, t2))
+      .set.filter(_.readRange.length <= maxTargetSize))
+
+    targetSet
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/BaseQualityRecalibration.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/BaseQualityRecalibration.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/BaseQualityRecalibration.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/BaseQualityRecalibration.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,130 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import java.io._
+import org.apache.spark.SparkContext._
+import org.apache.spark.Logging
+import org.apache.spark.broadcast.Broadcast
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.SnpTable
+import org.bdgenomics.adam.rich.DecadentRead
+import org.bdgenomics.adam.rich.DecadentRead._
+import org.bdgenomics.adam.util.QualityScore
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+/**
+ * The algorithm proceeds in two phases. First, we make a pass over the reads
+ * to collect statistics and build the recalibration tables. Then, we perform
+ * a second pass over the reads to apply the recalibration and assign adjusted
+ * quality scores.
+ */
+class BaseQualityRecalibration(
+  val input: RDD[DecadentRead],
+  val knownSnps: Broadcast[SnpTable],
+  val dumpObservationTableFile: Option[String] = None)
+    extends Serializable with Logging {
+
+  // Additional covariates to use when computing the correction
+  // TODO: parameterize
+  val covariates = CovariateSpace(new CycleCovariate, new DinucCovariate)
+
+  // Bases with quality less than this will be skipped and left alone
+  // TODO: parameterize
+  val minAcceptableQuality = QualityScore(5)
+
+  // Debug: Log the visited/skipped residues to bqsr-visits.dump
+  val enableVisitLogging = false
+
+  val dataset: RDD[(CovariateKey, Residue)] = {
+    def shouldIncludeRead(read: DecadentRead) =
+      read.isCanonicalRecord &&
+        read.record.record.getQual != null &&
+        read.alignmentQuality.exists(_ > QualityScore.zero) &&
+        read.passedQualityChecks
+
+    def shouldIncludeResidue(residue: Residue) =
+      residue.quality > QualityScore.zero &&
+        residue.isRegularBase &&
+        !residue.isInsertion &&
+        !knownSnps.value.isMasked(residue)
+
+    def observe(read: DecadentRead): Seq[(CovariateKey, Residue)] =
+      covariates(read).zip(read.residues).
+        filter { case (key, residue) => shouldIncludeResidue(residue) }
+
+    input.filter(shouldIncludeRead).flatMap(observe)
+  }
+
+  if (enableVisitLogging) {
+    input.cache()
+    dataset.cache()
+    dumpVisits("bqsr-visits.dump")
+  }
+
+  val observed: ObservationTable = {
+    dataset.
+      map { case (key, residue) => (key, Observation(residue.isSNP)) }.
+      aggregate(ObservationAccumulator(covariates))(_ += _, _ ++= _).result
+  }
+
+  dumpObservationTableFile.foreach(p => {
+    val writer = new PrintWriter(new File(p))
+
+    writer.write(observed.toCSV)
+    writer.close()
+  })
+
+  val result: RDD[AlignmentRecord] = {
+    val recalibrator = Recalibrator(observed, minAcceptableQuality)
+    input.map(recalibrator)
+  }
+
+  private def dumpVisits(filename: String) = {
+    def readId(read: DecadentRead): String =
+      read.name +
+        (if (read.isNegativeRead) "-" else "+") +
+        (if (read.record.getFirstOfPair) "1" else "") +
+        (if (read.record.getSecondOfPair) "2" else "")
+
+    val readLengths =
+      input.map(read => (readId(read), read.residues.length)).collectAsMap()
+
+    val visited = dataset.
+      map { case (key, residue) => (readId(residue.read), Seq(residue.offset)) }.
+      reduceByKeyLocally((left, right) => left ++ right)
+
+    val outf = new java.io.File(filename)
+    val writer = new java.io.PrintWriter(outf)
+    visited.foreach {
+      case (readName, visited) =>
+        val length = readLengths(readName)
+        val buf = Array.fill[Char](length)('O')
+        visited.foreach { idx => buf(idx) = 'X' }
+        writer.println(readName + "\t" + String.valueOf(buf))
+    }
+    writer.close()
+  }
+}
+
+object BaseQualityRecalibration {
+  def apply(rdd: RDD[AlignmentRecord],
+            knownSnps: Broadcast[SnpTable],
+            observationDumpFile: Option[String] = None): RDD[AlignmentRecord] =
+    new BaseQualityRecalibration(cloy(rdd), knownSnps).result
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Covariate.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Covariate.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Covariate.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Covariate.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,140 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import org.bdgenomics.adam.rich.DecadentRead
+import org.bdgenomics.adam.util.QualityScore
+import org.bdgenomics.adam.util.Util
+
+/**
+ * A Covariate represents a predictor, also known as a "feature" or
+ * "independent variable".
+ *
+ * @note Concrete implementations of Covariate should inherit from
+ * AbstractCovariate, not Covariate.
+ */
+trait Covariate {
+  type Value
+
+  /**
+   * Given a read, computes the value of this covariate for each residue in the
+   * read.
+   *
+   * The returned values must be in the same order as the residues. A value
+   * of None means this covariate does not apply to the corresponding residue.
+   *
+   * Example: The DinucCovariate returns a pair of bases for each residue,
+   * except for bases at the start of a read, for which it returns None.
+   */
+  def compute(read: DecadentRead): Seq[Option[Value]]
+
+  def apply(read: DecadentRead): Seq[Option[Value]] = compute(read)
+
+  // Format the provided Value to be compatible with GATK's CSV output
+  def toCSV(option: Option[Value]): String = option match {
+    case None        => "(none)"
+    case Some(value) => value.toString
+  }
+
+  // A short name for this covariate, used in CSV output header
+  def csvFieldName: String
+}
+
+abstract class AbstractCovariate[ValueT] extends Covariate with Serializable {
+  override type Value = ValueT
+}
+
+/**
+ * Represents a tuple containing a value for each covariate.
+ *
+ * The values for mandatory covariates are stored in member fields and optional
+ * covariate values are in `extras`.
+ */
+class CovariateKey(
+    val readGroup: String,
+    val quality: QualityScore,
+    val extras: Seq[Option[Covariate#Value]]) extends Serializable {
+
+  def containsNone: Boolean = extras.exists(_.isEmpty)
+
+  override def toString: String = {
+    def parts: Seq[Any] = Seq(readGroup, quality) ++ extras
+    "[" + parts.mkString(", ") + "]"
+  }
+
+  override def equals(other: Any) = other match {
+    case that: CovariateKey =>
+      this.readGroup == that.readGroup && this.quality == that.quality && this.extras == that.extras
+    case _ => false
+  }
+
+  override val hashCode: Int = {
+    41 * (
+      41 * (
+        41 + readGroup.hashCode
+      ) + quality.hashCode
+    ) + extras.hashCode
+  }
+}
+
+/**
+ * Represents the abstract space of all possible CovariateKeys for the given set
+ * of Covariates.
+ */
+class CovariateSpace(val extras: IndexedSeq[Covariate]) extends Serializable {
+  // Computes the covariate values for all residues in this read
+  def apply(read: DecadentRead): Seq[CovariateKey] = {
+    // Ask each 'extra' covariate to compute its values for this read
+    val extraVals = extras.map(cov => {
+      val result = cov(read)
+      // Each covariate must return a value per Residue
+      assert(result.size == read.residues.size)
+      result
+    })
+
+    // Construct the CovariateKeys
+    read.residues.zipWithIndex.map {
+      case (residue, residueIdx) =>
+        val residueExtras = extraVals.map(_(residueIdx))
+        new CovariateKey(read.readGroup, residue.quality, residueExtras)
+    }
+  }
+
+  // Format the provided key to be compatible with GATK's CSV output
+  def toCSV(key: CovariateKey): Seq[String] = {
+    val extraFields: Seq[String] = extras.zip(key.extras).map {
+      case (cov, value) => cov.toCSV(value.asInstanceOf[Option[cov.Value]])
+    }
+    Seq(key.readGroup, key.quality.phred.toString) ++ extraFields
+  }
+
+  def csvHeader: Seq[String] = Seq("ReadGroup", "ReportedQ") ++ extras.map(_.csvFieldName)
+
+  override def equals(other: Any): Boolean = other match {
+    case that: CovariateSpace => this.extras == that.extras
+    case _                    => false
+  }
+
+  override def hashCode = extras.hashCode
+
+}
+
+object CovariateSpace {
+  def apply(extras: Covariate*): CovariateSpace =
+    new CovariateSpace(extras.toIndexedSeq)
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/CycleCovariate.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/CycleCovariate.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/CycleCovariate.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/CycleCovariate.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,55 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import org.bdgenomics.adam.rich.DecadentRead
+
+// This is based on the CycleCovariate in GATK 1.6.
+class CycleCovariate extends AbstractCovariate[Int] {
+  def compute(read: DecadentRead): Seq[Option[Int]] = {
+    val (initial, increment) = initialization(read)
+    Range(0, read.residues.length).map(pos => Some(initial + increment * pos))
+  }
+
+  // Returns (initialValue, increment)
+  private def initialization(read: DecadentRead): (Int, Int) = {
+    if (!read.isNegativeRead) {
+      if (read.isSecondOfPair) {
+        (-1, -1)
+      } else {
+        (1, 1)
+      }
+    } else {
+      if (read.isSecondOfPair) {
+        (-read.residues.length, 1)
+      } else {
+        (read.residues.length, -1)
+      }
+    }
+  }
+
+  override def csvFieldName: String = "Cycle"
+
+  override def equals(other: Any) = other match {
+    case that: CycleCovariate => true
+    case _                    => false
+  }
+
+  override def hashCode = 0x83EFAB61
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/DinucCovariate.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/DinucCovariate.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/DinucCovariate.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/DinucCovariate.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,76 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import org.bdgenomics.adam.rich.DecadentRead
+
+// TODO: should inherit from something like AbstractCovariate[(DNABase, DNABase)]
+class DinucCovariate extends AbstractCovariate[(Char, Char)] {
+  def compute(read: DecadentRead): Seq[Option[(Char, Char)]] = {
+    val sequence = read.residues.map(_.base)
+    if (read.isNegativeRead) {
+      /* Use the reverse-complement of the sequence to get back the original
+       * sequence as it was read by the sequencing machine. The sequencer
+       * always reads from the 5' to the 3' end of each strand, but the output
+       * from the aligner is always in the same sense as the reference, so we
+       * use the reverse-complement if this read was originally from the
+       * complementary strand.
+       */
+      dinucs(complement(sequence.reverse)).reverse
+    } else {
+      dinucs(sequence)
+    }
+  }
+
+  private def dinucs(sequence: Seq[Char]): Seq[Option[(Char, Char)]] = {
+    sequence.zipWithIndex.map {
+      case (current, index) =>
+        assert(Seq('A', 'C', 'T', 'G', 'N').contains(current))
+        def previous = sequence(index - 1)
+        if (index > 0 && previous != 'N' && current != 'N') {
+          Some((previous, current))
+        } else {
+          None
+        }
+    }
+  }
+
+  private def complement(sequence: Seq[Char]): Seq[Char] = {
+    sequence.map {
+      case 'A' => 'T'
+      case 'T' => 'A'
+      case 'C' => 'G'
+      case 'G' => 'C'
+      case 'N' => 'N'
+    }
+  }
+
+  override def toCSV(option: Option[Value]): String = option match {
+    case None        => "NN"
+    case Some(value) => "%s%s".format(value._1, value._2)
+  }
+
+  override def csvFieldName: String = "Dinuc"
+
+  override def equals(other: Any) = other match {
+    case that: DinucCovariate => true
+    case _                    => false
+  }
+
+  override def hashCode = 0x9EAC50CB
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/ObservationTable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/ObservationTable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/ObservationTable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/ObservationTable.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,155 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import org.bdgenomics.adam.instrumentation.Timers._
+import org.bdgenomics.adam.util.QualityScore
+import org.bdgenomics.adam.util.Util
+import scala.collection.mutable
+
+/**
+ * An empirical frequency count of mismatches from the reference.
+ *
+ * This is used in ObservationTable, which maps from CovariateKey to Observation.
+ */
+class Observation(val total: Long, val mismatches: Long) extends Serializable {
+  require(mismatches >= 0 && mismatches <= total)
+
+  def this(that: Observation) = this(that.total, that.mismatches)
+
+  def +(that: Observation) =
+    new Observation(this.total + that.total, this.mismatches + that.mismatches)
+
+  /**
+   * Empirically estimated probability of a mismatch.
+   */
+  def empiricalErrorProbability: Double =
+    bayesianErrorProbability
+
+  /**
+   * Empirically estimated probability of a mismatch, as a QualityScore.
+   */
+  def empiricalQuality: QualityScore =
+    QualityScore.fromErrorProbability(empiricalErrorProbability)
+
+  /**
+   * Estimates the probability of a mismatch under a Bayesian model with
+   * Binomial likelihood and Beta(a, b) prior. When a = b = 1, this is also
+   * known as "Laplace's rule of succession".
+   *
+   * TODO: Beta(1, 1) is the safest choice, but maybe Beta(1/2, 1/2) is more
+   * accurate?
+   */
+  def bayesianErrorProbability: Double = bayesianErrorProbability(1, 1)
+  def bayesianErrorProbability(a: Double, b: Double): Double = (a + mismatches) / (a + b + total)
+
+  // Format as string compatible with GATK's CSV output
+  def toCSV: Seq[String] = Seq(total.toString, mismatches.toString, empiricalQuality.phred.toString)
+
+  override def toString: String =
+    "%s / %s (%s)".format(mismatches, total, empiricalQuality)
+
+  override def equals(other: Any): Boolean = other match {
+    case that: Observation => this.total == that.total && this.mismatches == that.mismatches
+    case _                 => false
+  }
+
+  override def hashCode: Int = {
+    41 * (
+      41 + total.hashCode
+    ) + mismatches.hashCode
+  }
+
+}
+
+object Observation {
+  val empty = new Observation(0, 0)
+
+  def apply(isMismatch: Boolean) = new Observation(1, if (isMismatch) 1 else 0)
+}
+
+class Aggregate private (
+    total: Long, // number of total observations
+    mismatches: Long, // number of mismatches observed
+    val expectedMismatches: Double // expected number of mismatches based on reported quality scores
+    ) extends Observation(total, mismatches) {
+
+  require(expectedMismatches <= total)
+
+  def reportedErrorProbability: Double = expectedMismatches / total.toDouble
+
+  def +(that: Aggregate): Aggregate =
+    new Aggregate(
+      this.total + that.total,
+      this.mismatches + that.mismatches,
+      this.expectedMismatches + that.expectedMismatches)
+}
+
+object Aggregate {
+  val empty: Aggregate = new Aggregate(0, 0, 0)
+
+  def apply(key: CovariateKey, value: Observation) =
+    new Aggregate(value.total, value.mismatches, key.quality.errorProbability * value.total)
+}
+
+/**
+ * Table containing the empirical frequency of mismatches for each set of covariate values.
+ */
+class ObservationTable(
+    val space: CovariateSpace,
+    val entries: Map[CovariateKey, Observation]) extends Serializable {
+
+  override def toString = entries.map { case (k, v) => "%s\t%s".format(k, v) }.mkString("\n")
+
+  // Format as CSV compatible with GATK's output
+  def toCSV: String = {
+    val rows = entries.map {
+      case (key, obs) =>
+        space.toCSV(key) ++ obs.toCSV ++ (if (key.containsNone) Seq("**") else Seq())
+    }
+    (Seq(csvHeader) ++ rows).map(_.mkString(",")).mkString("\n")
+  }
+
+  def csvHeader: Seq[String] = space.csvHeader ++ Seq("TotalCount", "MismatchCount", "EmpiricalQ", "IsSkipped")
+}
+
+class ObservationAccumulator(val space: CovariateSpace) extends Serializable {
+  private val entries = mutable.HashMap[CovariateKey, Observation]()
+
+  def +=(that: (CovariateKey, Observation)): ObservationAccumulator = ObservationAccumulatorSeq.time {
+    accum(that._1, that._2)
+  }
+
+  def ++=(that: ObservationAccumulator): ObservationAccumulator = ObservationAccumulatorComb.time {
+    if (this.space != that.space)
+      throw new IllegalArgumentException("Can only combine observations with matching CovariateSpaces")
+    that.entries.foreach { case (k, v) => accum(k, v) }
+    this
+  }
+
+  def accum(key: CovariateKey, value: Observation): ObservationAccumulator = {
+    entries(key) = value + entries.getOrElse(key, Observation.empty)
+    this
+  }
+
+  def result: ObservationTable = new ObservationTable(space, entries.toMap)
+}
+
+object ObservationAccumulator {
+  def apply(space: CovariateSpace) = new ObservationAccumulator(space)
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Recalibrator.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Recalibrator.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Recalibrator.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/read/recalibration/Recalibrator.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,169 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.read.recalibration
+
+import org.bdgenomics.adam.rich.DecadentRead.Residue
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.rich.DecadentRead
+import org.bdgenomics.adam.util.QualityScore
+import org.bdgenomics.formats.avro.AlignmentRecord
+import org.bdgenomics.adam.instrumentation.Timers._
+import scala.math.{ exp, log }
+
+class Recalibrator(val table: RecalibrationTable, val minAcceptableQuality: QualityScore)
+    extends (DecadentRead => AlignmentRecord) with Serializable {
+
+  def apply(read: DecadentRead): AlignmentRecord = RecalibrateRead.time {
+    val record: AlignmentRecord = read.record
+    if (record.getQual != null) {
+      AlignmentRecord.newBuilder(record)
+        .setQual(QualityScore.toString(computeQual(read)))
+        .setOrigQual(record.getQual)
+        .build()
+    } else {
+      record
+    }
+  }
+
+  def computeQual(read: DecadentRead): Seq[QualityScore] = ComputeQualityScore.time {
+    val origQuals = read.residues.map(_.quality)
+    val newQuals = table(read)
+    origQuals.zip(newQuals).map {
+      case (origQ, newQ) =>
+        // Keep original quality score if below recalibration threshold
+        if (origQ >= minAcceptableQuality) newQ else origQ
+    }
+  }
+}
+
+object Recalibrator {
+  def apply(observed: ObservationTable, minAcceptableQuality: QualityScore): Recalibrator = {
+    new Recalibrator(RecalibrationTable(observed), minAcceptableQuality)
+  }
+}
+
+class RecalibrationTable(
+  // covariates for this recalibration
+  val covariates: CovariateSpace,
+  // marginal and quality scores by read group,
+  val globalTable: Map[String, (Aggregate, QualityTable)])
+    extends (DecadentRead => Seq[QualityScore]) with Serializable {
+
+  // TODO: parameterize?
+  val maxQualScore = QualityScore(50)
+
+  val maxLogP = log(maxQualScore.errorProbability)
+
+  def apply(read: DecadentRead): Seq[QualityScore] = {
+    val globalEntry: Option[(Aggregate, QualityTable)] = globalTable.get(read.readGroup)
+    val globalDelta = computeGlobalDelta(globalEntry)
+    val extraValues: IndexedSeq[Seq[Option[Covariate#Value]]] = getExtraValues(read)
+    read.residues.zipWithIndex.map(lookup(_, globalEntry, globalDelta, extraValues))
+  }
+
+  def lookup(residueWithIndex: (Residue, Int), globalEntry: Option[(Aggregate, QualityTable)], globalDelta: Double,
+             extraValues: IndexedSeq[Seq[Option[Covariate#Value]]]): QualityScore = {
+    val (residue, index) = residueWithIndex
+    val residueLogP = log(residue.quality.errorProbability)
+    val qualityEntry: Option[(Aggregate, ExtrasTables)] = getQualityEntry(residue.quality, globalEntry)
+    val qualityDelta = computeQualityDelta(qualityEntry, residueLogP + globalDelta)
+    val extrasDelta = computeExtrasDelta(qualityEntry, index, extraValues, residueLogP + globalDelta + qualityDelta)
+    val correctedLogP = residueLogP + globalDelta + qualityDelta + extrasDelta
+    qualityFromLogP(correctedLogP)
+  }
+
+  def qualityFromLogP(logP: Double): QualityScore = {
+    val boundedLogP = math.min(0.0, math.max(maxLogP, logP))
+    QualityScore.fromErrorProbability(exp(boundedLogP))
+  }
+
+  def computeGlobalDelta(globalEntry: Option[(Aggregate, QualityTable)]): Double = {
+    globalEntry.map(bucket => log(bucket._1.empiricalErrorProbability) - log(bucket._1.reportedErrorProbability)).
+      getOrElse(0.0)
+  }
+
+  def getQualityEntry(quality: QualityScore,
+                      globalEntry: Option[(Aggregate, QualityTable)]): Option[(Aggregate, ExtrasTables)] = {
+    globalEntry.flatMap(_._2.table.get(quality))
+  }
+
+  def computeQualityDelta(qualityEntry: Option[(Aggregate, ExtrasTables)], offset: Double): Double = {
+    qualityEntry.map(bucket => log(bucket._1.empiricalErrorProbability) - offset).
+      getOrElse(0.0)
+  }
+
+  def computeExtrasDelta(maybeQualityEntry: Option[(Aggregate, ExtrasTables)], residueIndex: Int,
+                         extraValues: IndexedSeq[Seq[Option[Covariate#Value]]], offset: Double): Double = {
+    // Returns sum(delta for each extra covariate)
+    maybeQualityEntry.map(qualityEntry => {
+      val extrasTables = qualityEntry._2.extrasTables
+      assert(extrasTables.size == extraValues.size)
+      var extrasDelta = 0.0
+      var index = 0
+      extraValues.foreach(residueValues => {
+        val table = extrasTables(index)
+        extrasDelta += table.get(residueValues(residueIndex)).
+          map(aggregate => log(aggregate.empiricalErrorProbability) - offset).
+          getOrElse(0.0)
+        index += 1
+      })
+      extrasDelta
+    }).getOrElse(0.0)
+  }
+
+  def getExtraValues(read: DecadentRead): IndexedSeq[Seq[Option[Covariate#Value]]] = GetExtraValues.time {
+    covariates.extras.map(extra => extra(read))
+  }
+
+}
+
+object RecalibrationTable {
+
+  def apply(observed: ObservationTable): RecalibrationTable = {
+    // The ".map(identity)" calls are needed to force the result to be serializable.
+    val globalTable: Map[String, (Aggregate, QualityTable)] = observed.entries.groupBy(_._1.readGroup).map(globalEntry => {
+      (globalEntry._1, (aggregateObservations(globalEntry._2), new QualityTable(computeQualityTable(globalEntry, observed.space))))
+    }).map(identity)
+    new RecalibrationTable(observed.space, globalTable)
+  }
+
+  def computeQualityTable(globalEntry: (String, Map[CovariateKey, Observation]),
+                          space: CovariateSpace): Map[QualityScore, (Aggregate, ExtrasTables)] = {
+    globalEntry._2.groupBy(_._1.quality).map(qualityEntry => {
+      (qualityEntry._1, (aggregateObservations(qualityEntry._2), new ExtrasTables(computeExtrasTables(qualityEntry._2, space))))
+    }).map(identity)
+  }
+
+  def computeExtrasTables(table: Map[CovariateKey, Observation],
+                          space: CovariateSpace): IndexedSeq[Map[Option[Covariate#Value], Aggregate]] = {
+    Range(0, space.extras.length).map(index => {
+      table.groupBy(_._1.extras(index)).map(extraEntry => {
+        (extraEntry._1, aggregateObservations(extraEntry._2))
+      }).map(identity)
+    })
+  }
+
+  def aggregateObservations[K](observations: Map[CovariateKey, Observation]): Aggregate = {
+    observations.map { case (oldKey, obs) => Aggregate(oldKey, obs) }.fold(Aggregate.empty)(_ + _)
+  }
+
+}
+
+class QualityTable(val table: Map[QualityScore, (Aggregate, ExtrasTables)]) extends Serializable
+
+class ExtrasTables(val extrasTables: IndexedSeq[Map[Option[Covariate#Value], Aggregate]]) extends Serializable
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ReferencePartitioner.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ReferencePartitioner.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ReferencePartitioner.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ReferencePartitioner.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,57 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.apache.spark.Partitioner
+import org.bdgenomics.adam.models.{
+  ReferencePosition,
+  ReferenceRegion,
+  SequenceDictionary
+}
+
+/**
+ * Repartitions objects that are keyed by a ReferencePosition or ReferenceRegion
+ * into a single partition per contig.
+ */
+case class ReferencePartitioner(sd: SequenceDictionary) extends Partitioner {
+
+  // extract just the reference names
+  private val referenceNames = sd.records.map(_.name)
+
+  override def numPartitions: Int = referenceNames.length
+
+  private def partitionFromName(name: String): Int = {
+    // which reference is this in?
+    val pIdx = referenceNames.indexOf(name)
+
+    // provide debug info to user if key is bad
+    assert(pIdx != -1, "Reference not found in " + sd + " for key " + name)
+
+    pIdx
+  }
+
+  override def getPartition(key: Any): Int = key match {
+    case rp: ReferencePosition => {
+      partitionFromName(rp.referenceName)
+    }
+    case rr: ReferenceRegion => {
+      partitionFromName(rr.referenceName)
+    }
+    case _ => throw new IllegalArgumentException("Only ReferencePositions or ReferenceRegions can be used as a key.")
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/RegionJoin.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/RegionJoin.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/RegionJoin.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/RegionJoin.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,43 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.bdgenomics.adam.models.{ SequenceDictionary, ReferenceRegion }
+import org.apache.spark.rdd.RDD
+import org.apache.spark.SparkContext._
+import scala.Predef._
+import org.apache.spark.SparkContext
+import scala.reflect.ClassTag
+
+trait RegionJoin {
+  /**
+   * Performs a region join between two RDDs.
+   *
+   * @param baseRDD The 'left' side of the join
+   * @param joinedRDD The 'right' side of the join
+   * @param tManifest implicit type of baseRDD
+   * @param uManifest implicit type of joinedRDD
+   * @tparam T type of baseRDD
+   * @tparam U type of joinedRDD
+   * @return An RDD of pairs (x, y), where x is from baseRDD, y is from joinedRDD, and the region
+   *         corresponding to x overlaps the region corresponding to y.
+   */
+  def partitionAndJoin[T, U](baseRDD: RDD[(ReferenceRegion, T)],
+                             joinedRDD: RDD[(ReferenceRegion, U)])(implicit tManifest: ClassTag[T],
+                                                                   uManifest: ClassTag[U]): RDD[(T, U)]
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ShuffleRegionJoin.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ShuffleRegionJoin.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/ShuffleRegionJoin.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/ShuffleRegionJoin.scala	2016-05-06 09:27:54.012365912 +0800
@@ -0,0 +1,290 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd
+
+import org.apache.spark.{ Logging, Partitioner, SparkContext }
+import org.apache.spark.SparkContext._
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.ReferenceRegion._
+import org.bdgenomics.adam.models.{ SequenceDictionary, SequenceRecord, ReferenceRegion }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import scala.collection.mutable.ListBuffer
+import scala.math._
+import scala.reflect.ClassTag
+
+object ShuffleRegionJoin extends RegionJoin {
+
+  private var sd = new SequenceDictionary(Vector[SequenceRecord]())
+
+  def setSequenceDictionary(_sd: SequenceDictionary) {
+    sd = _sd
+  }
+
+  def partitionAndJoin[T, U](leftRDD: RDD[(ReferenceRegion, T)],
+                             rightRDD: RDD[(ReferenceRegion, U)])(implicit tManifest: ClassTag[T],
+                                                                  uManifest: ClassTag[U]): RDD[(T, U)] = {
+
+    // we will compute these parameters from the datasets we've got
+    val maxPartitions = max(leftRDD.partitions.length.toLong, rightRDD.partitions.length.toLong)
+    val partitionSize = sd.records.map(_.length).sum / maxPartitions
+
+    partitionAndJoin(leftRDD, rightRDD, sd, partitionSize)
+  }
+
+  /**
+   * Performs a region join between two RDDs (shuffle join).
+   *
+   * This implementation is shuffle-based, so does not require collecting one side into memory
+   * like BroadcastRegionJoin.  It basically performs a global sort of each RDD by genome position
+   * and then does a sort-merge join, similar to the chromsweep implementation in bedtools.  More
+   * specifically, it first defines a set of bins across the genome, then assigns each object in the
+   * RDDs to each bin that they overlap (replicating if necessary), performs the shuffle, and sorts
+   * the object in each bin.  Finally, each bin independently performs a chromsweep sort-merge join.
+   *
+   * @param leftRDD The 'left' side of the join
+   * @param rightRDD The 'right' side of the join
+   * @param seqDict A SequenceDictionary -- every region corresponding to either the leftRDD or rightRDD
+   *                values must be mapped to a chromosome with an entry in this dictionary.
+   * @param partitionSize The size of the genome bin in nucleotides.  Controls the parallelism of the join.
+   * @param tManifest implicit type of leftRDD
+   * @param uManifest implicit type of rightRDD
+   * @tparam T type of leftRDD
+   * @tparam U type of rightRDD
+   * @return An RDD of pairs (x, y), where x is from leftRDD, y is from rightRDD, and the region
+   *         corresponding to x overlaps the region corresponding to y.
+   */
+  def partitionAndJoin[T, U](leftRDD: RDD[(ReferenceRegion, T)],
+                             rightRDD: RDD[(ReferenceRegion, U)],
+                             seqDict: SequenceDictionary,
+                             partitionSize: Long)(implicit tManifest: ClassTag[T],
+                                                  uManifest: ClassTag[U]): RDD[(T, U)] = {
+    val sc = leftRDD.context
+
+    // Create the set of bins across the genome for parallel processing
+    val seqLengths = Map(seqDict.records.toSeq.map(rec => (rec.name.toString, rec.length)): _*)
+    val bins = sc.broadcast(GenomeBins(partitionSize, seqLengths))
+
+    // Key each RDD element to its corresponding bin
+    // Elements may be replicated if they overlap multiple bins
+    val keyedLeft: RDD[((ReferenceRegion, Int), T)] =
+      leftRDD.flatMap(kv => {
+        val (region, x) = kv
+        val lo = bins.value.getStartBin(region)
+        val hi = bins.value.getEndBin(region)
+        (lo to hi).map(i => ((region, i), x))
+      })
+    val keyedRight: RDD[((ReferenceRegion, Int), U)] =
+      rightRDD.flatMap(kv => {
+        val (region, y) = kv
+        val lo = bins.value.getStartBin(region)
+        val hi = bins.value.getEndBin(region)
+        (lo to hi).map(i => ((region, i), y))
+      })
+
+    // Sort each RDD by shuffling the data into the corresponding genome bin
+    // and then sorting within each bin by the key, which sorts by ReferenceRegion.
+    // This should be the most expensive operation. At the end, each genome bin
+    // corresponds to a Spark partition.  The ManualRegionPartitioner pulls out the
+    // bin number for each elt.
+    val sortedLeft: RDD[((ReferenceRegion, Int), T)] =
+      keyedLeft.repartitionAndSortWithinPartitions(ManualRegionPartitioner(bins.value.numBins))
+    val sortedRight: RDD[((ReferenceRegion, Int), U)] =
+      keyedRight.repartitionAndSortWithinPartitions(ManualRegionPartitioner(bins.value.numBins))
+
+    // this function carries out the sort-merge join inside each Spark partition.
+    // It assumes the iterators are sorted.
+    def sweep(leftIter: Iterator[((ReferenceRegion, Int), T)], rightIter: Iterator[((ReferenceRegion, Int), U)]) = {
+      if (leftIter.isEmpty || rightIter.isEmpty) {
+        Seq.empty[(T, U)].toIterator
+      } else {
+        val bufferedLeft = leftIter.buffered
+        val currentBin = bufferedLeft.head._1._2
+        val region = bins.value.invert(currentBin)
+        // return an Iterator[(T, U)]
+        SortedIntervalPartitionJoin(region, bufferedLeft, rightIter)
+      }
+    }
+
+    // Execute the sort-merge join on each partition
+    // Note that we do NOT preserve the partitioning, as the ManualRegionPartitioner
+    // has no meaning for the return type of RDD[(T, U)].  In fact, how
+    // do you order a pair of ReferenceRegions?
+    sortedLeft.zipPartitions(sortedRight, preservesPartitioning = false)(sweep)
+  }
+}
+
+/**
+ * Partition a genome into a set of bins.
+ *
+ * Note that this class will not tolerate invalid input, so filter in advance if you use it.
+ *
+ * @param binSize The size of each bin in nucleotides
+ * @param seqLengths A map containing the length of each contig
+ */
+case class GenomeBins(binSize: Long, seqLengths: Map[String, Long]) extends Serializable {
+  private val names: Seq[String] = seqLengths.keys.toSeq.sortWith(_ < _)
+  private val lengths: Seq[Long] = names.map(seqLengths(_))
+  private val parts: Seq[Int] = lengths.map(v => round(ceil(v.toDouble / binSize)).toInt)
+  private val cumulParts: Seq[Int] = parts.scan(0)(_ + _)
+  private val contig2cumulParts: Map[String, Int] = Map(names.zip(cumulParts): _*)
+
+  /**
+   * The total number of bins induced by this partition.
+   */
+  def numBins: Int = parts.sum
+
+  /**
+   * Get the bin number corresponding to a query ReferenceRegion.
+   *
+   * @param region the query ReferenceRegion
+   * @param useStart whether to use the start or end point of region to pick the bin
+   */
+  def get(region: ReferenceRegion, useStart: Boolean = true): Int = {
+    val pos = if (useStart) region.start else (region.end - 1)
+    (contig2cumulParts(region.referenceName) + pos / binSize).toInt
+  }
+
+  /**
+   * Get the bin number corresponding to the start of the query ReferenceRegion.
+   */
+  def getStartBin(region: ReferenceRegion): Int = get(region, useStart = true)
+
+  /**
+   * Get the bin number corresponding to the end of the query ReferenceRegion.
+   */
+  def getEndBin(region: ReferenceRegion): Int = get(region, useStart = false)
+
+  /**
+   * Given a bin number, return its corresponding ReferenceRegion.
+   */
+  def invert(bin: Int): ReferenceRegion = {
+    val idx = cumulParts.indexWhere(_ > bin) - 1
+    val name = names(idx)
+    val relPartition = bin - contig2cumulParts(name)
+    val start = relPartition * binSize
+    val end = min((relPartition + 1) * binSize, seqLengths(name))
+    ReferenceRegion(name, start, end)
+  }
+}
+
+/**
+ * A Partitioner that simply passes through the precomputed partition number for the RegionJoin.
+ *
+ * This is a "hack" partitioner enables the replication of objects into different genome bins.
+ * The key should correspond to a pair (region: ReferenceRegion, bin: Int).
+ * The Spark partition number corresponds to the genome bin number, and was precomputed
+ * with a flatmap to allow for replication into multiple bins.
+ *
+ * @param partitions should correspond to the number of bins in the corresponding GenomeBins
+ */
+private case class ManualRegionPartitioner(partitions: Int) extends Partitioner {
+  override def numPartitions: Int = partitions
+  override def getPartition(key: Any): Int = key match {
+    case (r: ReferenceRegion, p: Int) => p
+    case _                            => throw new AssertionError("Unexpected key in ManualRegionPartitioner")
+  }
+}
+
+/**
+ * Implementation of a "chromosome sweep" sort-merge join.
+ *
+ * This implementation is based on the implementation used in bedtools:
+ * https://github.com/arq5x/bedtools2
+ *
+ * Given two iterators of ReferenceRegions in sorted order, sweep across the relevant
+ * region of the genome to emit pairs of overlapping regions.  Compared with the bedtools impl,
+ * and to ensure no duplication of joined records, a joined pair is emitted only if at least
+ * one of the two regions starts in the corresponding genome bin.  For this reason, we must
+ * take in the partition's genome coords
+ *
+ * @param binRegion The ReferenceRegion corresponding to this partition, to ensure no duplicate
+ *                  join results across the whole RDD
+ * @param leftIter The left iterator
+ * @param rightIter The right iterator
+ * @tparam T type of leftIter
+ * @tparam U type of rightIter
+ */
+private case class SortedIntervalPartitionJoin[T, U](binRegion: ReferenceRegion,
+                                                     leftIter: Iterator[((ReferenceRegion, Int), T)],
+                                                     rightIter: Iterator[((ReferenceRegion, Int), U)])
+    extends Iterator[(T, U)] with Serializable {
+  // inspired by bedtools2 chromsweep
+  private val left: BufferedIterator[((ReferenceRegion, Int), T)] = leftIter.buffered
+  private val right: BufferedIterator[((ReferenceRegion, Int), U)] = rightIter.buffered
+  // stores the current set of joined pairs
+  private var hits: List[(T, U)] = List.empty
+  // stores the rightIter values that might overlap the current value from the leftIter
+  private val cache: ListBuffer[(ReferenceRegion, U)] = ListBuffer.empty
+  private var prevLeftRegion: ReferenceRegion = _
+
+  private def advanceCache(until: Long): Unit = {
+    while (right.hasNext && right.head._1._1.start < until) {
+      val x = right.next()
+      cache += x._1._1 -> x._2
+    }
+  }
+
+  private def pruneCache(to: Long): Unit = {
+    cache.dropWhile(_._1.end <= to)
+  }
+
+  private def getHits(): Unit = {
+    // if there is nothing more in left, then I'm done
+    while (left.hasNext && hits.isEmpty) {
+      // there is more in left...
+      val ((nextLeftRegion, _), nextLeft) = left.next
+      // ...so check whether I need to advance the cache
+      // (where nextLeftRegion's end is further than prevLeftRegion's end)...
+      // (the null checks are for the first iteration)
+      if (prevLeftRegion == null || nextLeftRegion.end > prevLeftRegion.end) {
+        advanceCache(nextLeftRegion.end)
+      }
+      // ...and whether I need to prune the cache
+      if (prevLeftRegion == null) {
+        pruneCache(nextLeftRegion.start)
+      }
+      // at this point, we effectively do a cross-product and filter; this could probably
+      // be improved by making cache a fancier data structure than just a list
+      // we filter for things that overlap, where at least one side of the join has a start position
+      // in this partition
+      hits = cache
+        .filter(y => {
+          y._1.overlaps(nextLeftRegion) &&
+            (y._1.start >= binRegion.start || nextLeftRegion.start >= binRegion.start)
+        })
+        .map(y => (nextLeft, y._2))
+        .toList
+      prevLeftRegion = nextLeftRegion
+    }
+  }
+
+  def hasNext: Boolean = {
+    // if the list of current hits is empty, try to refill it by moving forward
+    if (hits.isEmpty) {
+      getHits()
+    }
+    // if hits is still empty, I must really be at the end
+    !hits.isEmpty
+  }
+
+  def next: (T, U) = {
+    val popped = hits.head
+    hits = hits.tail
+    popped
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/variation/ADAMVCFOutputFormat.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/variation/ADAMVCFOutputFormat.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/variation/ADAMVCFOutputFormat.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/variation/ADAMVCFOutputFormat.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,49 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.variation
+
+import htsjdk.variant.vcf.{ VCFHeaderLine, VCFHeader }
+import org.bdgenomics.adam.converters.VariantAnnotationConverter
+import org.seqdoop.hadoop_bam.{ VCFFormat, KeyIgnoringVCFOutputFormat }
+import scala.collection.JavaConversions._
+
+object ADAMVCFOutputFormat extends Serializable {
+  private var header: Option[VCFHeader] = None
+
+  def getHeader: VCFHeader = header match {
+    case Some(h) => h
+    case None    => setHeader(Seq())
+  }
+
+  def setHeader(samples: Seq[String]): VCFHeader = {
+    header = Some(new VCFHeader(
+      (VariantAnnotationConverter.infoHeaderLines ++ VariantAnnotationConverter.formatHeaderLines).toSet: Set[VCFHeaderLine],
+      samples))
+    header.get
+  }
+}
+
+/**
+ * Wrapper for Hadoop-BAM to work around requirement for no-args constructor. Depends on
+ * ADAMVCFOutputFormat object to maintain global state (such as samples)
+ *
+ * @tparam K
+ */
+class ADAMVCFOutputFormat[K] extends KeyIgnoringVCFOutputFormat[K](VCFFormat.VCF) with Serializable {
+  setHeader(ADAMVCFOutputFormat.getHeader)
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/variation/VariationRDDFunctions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/variation/VariationRDDFunctions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rdd/variation/VariationRDDFunctions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rdd/variation/VariationRDDFunctions.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,157 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rdd.variation
+
+import org.apache.hadoop.io.LongWritable
+import org.apache.spark.SparkContext._
+import org.apache.spark.{ Logging, SparkContext }
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.converters.VariantContextConverter
+import org.bdgenomics.adam.models.{
+  ReferencePosition,
+  ReferenceRegion,
+  SequenceDictionary,
+  SequenceRecord,
+  VariantContext
+}
+import org.bdgenomics.adam.rdd.ADAMSequenceDictionaryRDDAggregator
+import org.bdgenomics.adam.rich.RichVariant
+import org.bdgenomics.adam.rich.RichGenotype._
+import org.bdgenomics.formats.avro.{ Genotype, GenotypeType, DatabaseVariantAnnotation }
+import org.bdgenomics.utils.misc.HadoopUtil
+import org.seqdoop.hadoop_bam._
+
+class VariantContextRDDFunctions(rdd: RDD[VariantContext]) extends ADAMSequenceDictionaryRDDAggregator[VariantContext](rdd) with Logging {
+
+  /**
+   * For a single variant context, returns sequence record elements.
+   *
+   * @param elem Element from which to extract sequence records.
+   * @return A seq of sequence records.
+   */
+  def getSequenceRecordsFromElement(elem: VariantContext): scala.collection.Set[SequenceRecord] = {
+    elem.genotypes.map(gt => SequenceRecord.fromSpecificRecord(gt.getVariant)).toSet
+  }
+
+  /**
+   * Left outer join database variant annotations
+   *
+   */
+  def joinDatabaseVariantAnnotation(ann: RDD[DatabaseVariantAnnotation]): RDD[VariantContext] = {
+    rdd.keyBy(_.variant)
+      .leftOuterJoin(ann.keyBy(_.getVariant))
+      .values
+      .map { case (v: VariantContext, a) => VariantContext(v.variant, v.genotypes, a) }
+
+  }
+
+  def getCallsetSamples(): List[String] = {
+    rdd.flatMap(c => c.genotypes.map(_.getSampleId).toSeq.distinct)
+      .distinct
+      .map(_.toString)
+      .collect()
+      .toList
+  }
+
+  /**
+   * Converts an RDD of ADAM VariantContexts to HTSJDK VariantContexts
+   * and saves to disk as VCF.
+   *
+   * @param filePath The filepath to save to.
+   * @param dict An optional sequence dictionary. Default is none.
+   * @param sortOnSave Whether to sort before saving. Sort is run after coalescing.
+   *                   Default is false (no sort).
+   * @param coalesceTo Optionally coalesces the RDD down to _n_ partitions. Default is none.
+   */
+  def saveAsVcf(filePath: String,
+                dict: Option[SequenceDictionary] = None,
+                sortOnSave: Boolean = false,
+                coalesceTo: Option[Int] = None) = {
+    val vcfFormat = VCFFormat.inferFromFilePath(filePath)
+    assert(vcfFormat == VCFFormat.VCF, "BCF not yet supported") // TODO: Add BCF support
+
+    rdd.cache()
+    log.info("Writing %s file to %s".format(vcfFormat, filePath))
+
+    // Initialize global header object required by Hadoop VCF Writer
+    val header = getCallsetSamples()
+    val bcastHeader = rdd.context.broadcast(header)
+    val mp = rdd.mapPartitionsWithIndex((idx, iter) => {
+      log.warn("Setting header for partition " + idx)
+      synchronized {
+        // perform map partition call to ensure that the VCF header is set on all
+        // nodes in the cluster; see:
+        // https://github.com/bigdatagenomics/adam/issues/353
+        ADAMVCFOutputFormat.setHeader(bcastHeader.value)
+        log.warn("Set VCF header for partition " + idx)
+      }
+      Iterator[Int]()
+    }).count()
+
+    // force value check, ensure that computation happens
+    if (mp != 0) {
+      log.warn("Had more than 0 elements after map partitions call to set VCF header across cluster.")
+    }
+
+    // convert the variants to htsjdk vc
+    val converter = new VariantContextConverter(dict)
+    val gatkVCs: RDD[VariantContextWritable] = rdd.map(v => {
+      val vcw = new VariantContextWritable
+      vcw.set(converter.convert(v))
+      vcw
+    })
+
+    // coalesce the rdd if requested
+    val coalescedVCs = coalesceTo.fold(gatkVCs)(p => gatkVCs.repartition(p))
+
+    // sort if requested
+    val withKey = if (sortOnSave) {
+      coalescedVCs.keyBy(v => ReferencePosition(v.get.getChr(), v.get.getStart()))
+        .sortByKey()
+        .map(kv => (new LongWritable(kv._1.pos), kv._2))
+    } else {
+      coalescedVCs.keyBy(v => new LongWritable(v.get.getStart))
+    }
+
+    // save to disk
+    val conf = rdd.context.hadoopConfiguration
+    conf.set(VCFOutputFormat.OUTPUT_VCF_FORMAT_PROPERTY, vcfFormat.toString)
+    withKey.saveAsNewAPIHadoopFile(filePath,
+      classOf[LongWritable], classOf[VariantContextWritable], classOf[ADAMVCFOutputFormat[LongWritable]],
+      conf)
+
+    log.info("Write %d records".format(gatkVCs.count()))
+    rdd.unpersist()
+  }
+}
+
+class GenotypeRDDFunctions(rdd: RDD[Genotype]) extends Serializable with Logging {
+  def toVariantContext(): RDD[VariantContext] = {
+    rdd.keyBy({ g => RichVariant.variantToRichVariant(g.getVariant) })
+      .groupByKey
+      .map { case (v: RichVariant, g) => new VariantContext(ReferencePosition(v), v, g, None) }
+  }
+
+  def filterByOverlappingRegion(query: ReferenceRegion): RDD[Genotype] = {
+    def overlapsQuery(rec: Genotype): Boolean =
+      rec.getVariant.getContig.getContigName.toString == query.referenceName &&
+        rec.getVariant.getStart < query.end &&
+        rec.getVariant.getEnd > query.start
+    rdd.filter(overlapsQuery)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/DecadentRead.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/DecadentRead.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/DecadentRead.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/DecadentRead.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,165 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import org.apache.spark.Logging
+import org.apache.spark.rdd.RDD
+import org.bdgenomics.adam.models.ReferencePosition
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.util.{ MdTag, QualityScore }
+import org.bdgenomics.formats.avro.AlignmentRecord
+
+object DecadentRead {
+  type Residue = DecadentRead#Residue
+
+  // Constructors
+  def apply(record: AlignmentRecord): DecadentRead = DecadentRead(RichAlignmentRecord(record))
+
+  def apply(rich: RichAlignmentRecord): DecadentRead = {
+    try {
+      new DecadentRead(rich)
+    } catch {
+      case exc: Exception =>
+        val msg = "Error \"%s\" while constructing DecadentRead from Read(%s)".format(exc.getMessage, rich.record)
+        throw new IllegalArgumentException(msg, exc)
+    }
+  }
+
+  /**
+   * cloy (verb)
+   *   1. To fill to loathing; to surfeit.
+   *   2. To clog, to glut, or satisfy, as the appetite; to satiate.
+   *   3. To fill up or choke up; to stop up.
+   */
+  def cloy(rdd: RDD[AlignmentRecord]): RDD[DecadentRead] = rdd.map(DecadentRead.apply)
+
+  // The inevitable counterpart of the above.
+  implicit def decay(rdd: RDD[DecadentRead]): RDD[AlignmentRecord] = rdd.map(_.record)
+}
+
+class DecadentRead(val record: RichAlignmentRecord) extends Logging {
+  // Can't be a primary alignment unless it has been aligned
+  require(!record.getPrimaryAlignment || record.getReadMapped, "Unaligned read can't be a primary alignment")
+
+  // Should have quality scores for all residues
+  require(record.getQual == null ||
+    record.getSequence.length == record.qualityScores.length, "sequence and qualityScores must be same length")
+
+  // MapQ should be valid
+  require(record.getMapq == null || (record.getMapq >= 0 && record.getMapq <= 93), "MapQ must be in [0, 255]")
+
+  // Alignment must be valid
+  require(!record.getReadMapped || record.getStart >= 0, "Invalid alignment start index")
+
+  // Sanity check on referencePositions
+  require(record.referencePositions.length == record.getSequence.length)
+
+  /**
+   * In biochemistry and molecular biology, a "residue" refers to a specific
+   * monomer within a polymeric chain, such as DNA.
+   */
+  class Residue private[DecadentRead] (val offset: Int) {
+    def read = DecadentRead.this
+
+    /**
+     * Nucleotide at this offset.
+     *
+     * TODO: Return values of meaningful type, e.g. `DNABase`.
+     */
+    def base: Char = read.baseSequence(offset)
+
+    def quality = QualityScore(record.qualityScores(offset))
+
+    def isRegularBase: Boolean = base match {
+      case 'A' => true
+      case 'C' => true
+      case 'T' => true
+      case 'G' => true
+      case 'N' => false
+      case unk => throw new IllegalArgumentException("Encountered unexpected base '%s'".format(unk))
+    }
+
+    def isMismatch(includeInsertions: Boolean = true): Boolean =
+      assumingAligned(record.isMismatchAtReadOffset(offset).getOrElse(includeInsertions))
+
+    def isSNP: Boolean = isMismatch(false)
+
+    def isInsertion: Boolean =
+      assumingAligned(record.isMismatchAtReadOffset(offset).isEmpty)
+
+    def referencePositionOption: Option[ReferencePosition] =
+      assumingAligned(
+        record.readOffsetToReferencePosition(offset))
+
+    def referenceSequenceContext: Option[ReferenceSequenceContext] =
+      assumingAligned(record.readOffsetToReferenceSequenceContext(offset))
+
+    def referencePosition: ReferencePosition =
+      referencePositionOption.getOrElse(
+        throw new IllegalArgumentException("Residue has no reference location (may be an insertion)"))
+  }
+
+  lazy val readGroup: String = record.getRecordGroupName.toString
+
+  private lazy val baseSequence: String = record.getSequence.toString
+
+  lazy val residues: IndexedSeq[Residue] = Range(0, baseSequence.length).map(new Residue(_))
+
+  def name: String = record.getReadName
+
+  def isAligned: Boolean = record.getReadMapped
+
+  def alignmentQuality: Option[QualityScore] = assumingAligned {
+    if (record.getMapq == null || record.getMapq == 255) {
+      None
+    } else {
+      Some(QualityScore(record.getMapq))
+    }
+  }
+
+  def ensureAligned: Boolean =
+    isAligned || (throw new IllegalArgumentException("Read has not been aligned to a reference"))
+
+  def isPrimaryAlignment: Boolean = isAligned && record.getPrimaryAlignment
+
+  def isDuplicate: Boolean = record.getDuplicateRead
+
+  def isPaired: Boolean = record.getReadPaired
+
+  def isFirstOfPair: Boolean = isPaired && !record.getSecondOfPair
+
+  def isSecondOfPair: Boolean = isPaired && record.getSecondOfPair
+
+  def isNegativeRead: Boolean = record.getReadNegativeStrand
+
+  // Is this the most representative record for this read?
+  def isCanonicalRecord: Boolean = isPrimaryAlignment && !isDuplicate
+
+  def passedQualityChecks: Boolean = !record.getFailedVendorQualityChecks
+
+  def mismatchesOption: Option[MdTag] = record.mdTag
+
+  def mismatches: MdTag =
+    mismatchesOption.getOrElse(throw new IllegalArgumentException("Read has no MD tag"))
+
+  private def assumingAligned[T](func: => T): T = {
+    ensureAligned
+    func
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/ReferenceSequenceContext.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/ReferenceSequenceContext.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/ReferenceSequenceContext.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/ReferenceSequenceContext.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,27 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import org.bdgenomics.adam.models.ReferencePosition
+import htsjdk.samtools.CigarElement
+
+/**
+ * Represents information on the reference relative to a particular residue
+ */
+
+private[rich] case class ReferenceSequenceContext(pos: Option[ReferencePosition], referenceBase: Option[Char], cigarElement: CigarElement, cigarElementOffset: Int)
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichAlignmentRecord.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichAlignmentRecord.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichAlignmentRecord.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichAlignmentRecord.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,248 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import java.util.regex.Pattern
+import htsjdk.samtools.{ Cigar, CigarElement, CigarOperator, TextCigarCodec }
+import org.bdgenomics.adam.models.{MyReferencePosition, Attribute, ReferencePosition, ReferenceRegion}
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.util._
+import org.bdgenomics.formats.avro.{ AlignmentRecord, Strand }
+import scala.collection.immutable.NumericRange
+import scala.math.max
+
+object RichAlignmentRecord {
+  val ILLUMINA_READNAME_REGEX = "[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).*".r
+
+  val cigarPattern = Pattern.compile("([0-9]+)([MIDNSHPX=])")
+
+  /**
+   * Parses a CIGAR string, and returns the aligned length with respect to the
+   * reference genome (i.e. skipping clipping, padding, and insertion operators)
+   *
+   * @param cigar The CIGAR string whose reference length is to be measured
+   * @return A non-negative integer, the sum of the MDNX= operators in the CIGAR string.
+   */
+  def referenceLengthFromCigar(cigar: String): Int = {
+    val m = cigarPattern.matcher(cigar)
+    var i = 0
+    var len: Int = 0
+    while (i < cigar.length) {
+      if (m.find(i)) {
+        val op = m.group(2)
+        if ("MDNX=".indexOf(op) != -1) {
+          len += m.group(1).toInt
+        }
+      } else {
+        return len
+      }
+      i = m.end()
+    }
+    len
+  }
+
+  def apply(record: AlignmentRecord) = {
+    new RichAlignmentRecord(record)
+  }
+
+  implicit def recordToRichRecord(record: AlignmentRecord): RichAlignmentRecord = new RichAlignmentRecord(record)
+  implicit def richRecordToRecord(record: RichAlignmentRecord): AlignmentRecord = record.record
+}
+
+class IlluminaOptics(val tile: Long, val x: Long, val y: Long) {}
+
+class RichAlignmentRecord(val record: AlignmentRecord) {
+
+  lazy val referenceLength: Int = RichAlignmentRecord.referenceLengthFromCigar(record.getCigar)
+
+  // Returns the quality scores as a list of bytes
+  lazy val qualityScores: Array[Int] = {
+    record.getQual.toString.toCharArray.map(q => q - 33)
+  }
+
+  // Parse the tags ("key:type:value" triples)
+  lazy val tags: Seq[Attribute] = AttributeUtils.parseAttributes(record.getAttributes)
+
+  // Parses the readname to Illumina optics information
+  lazy val illuminaOptics: Option[IlluminaOptics] = {
+    try {
+      val RichAlignmentRecord.ILLUMINA_READNAME_REGEX(tile, x, y) = record.getReadName
+      Some(new IlluminaOptics(tile.toInt, x.toInt, y.toInt))
+    } catch {
+      case e: MatchError => None
+    }
+  }
+
+  lazy val samtoolsCigar: Cigar = {
+    TextCigarCodec.decode(record.getCigar.toString)
+  }
+
+  // Returns the MdTag if the read is mapped, None otherwise
+  lazy val mdTag: Option[MdTag] = {
+    if (record.getReadMapped && record.getMismatchingPositions != null) {
+      Some(MdTag(record.getMismatchingPositions, record.getStart))
+    } else {
+      None
+    }
+  }
+
+  private def isClipped(el: CigarElement) = {
+    el.getOperator == CigarOperator.SOFT_CLIP ||
+      el.getOperator == CigarOperator.HARD_CLIP
+  }
+
+  // Returns the position of the unclipped end if the read is mapped, None otherwise
+  lazy val unclippedEnd: Long = {
+    max(0L, samtoolsCigar.getCigarElements.reverse.takeWhile(isClipped).foldLeft(record.getEnd)({
+      (pos, cigarEl) => pos + cigarEl.getLength
+    }))
+  }
+
+  // Returns the position of the unclipped start if the read is mapped, None otherwise.
+  lazy val unclippedStart: Long = {
+    max(0L, samtoolsCigar.getCigarElements.takeWhile(isClipped).foldLeft(record.getStart)({
+      (pos, cigarEl) => pos - cigarEl.getLength
+    }))
+  }
+
+  // Return the 5 prime position.
+  def fivePrimePosition: Long = {
+    if (record.getReadNegativeStrand) unclippedEnd else unclippedStart
+  }
+
+  def fivePrimeReferencePosition: ReferencePosition = {
+    try {
+      val strand = if (record.getReadNegativeStrand) {
+        Strand.Reverse
+      } else {
+        Strand.Forward
+      }
+      ReferencePosition(record.getContig.getContigName, fivePrimePosition, strand)
+    } catch {
+      case e: Throwable => {
+        println("caught " + e + " when trying to get position for " + record)
+        throw e
+      }
+    }
+  }
+
+
+  // Does this read overlap with the given reference position?
+  def overlapsReferencePosition(pos: ReferencePosition): Boolean = {
+    if (record.getReadMapped) {
+      ReferenceRegion(record).overlaps(pos)
+    } else {
+      false
+    }
+  }
+
+  // Does this read mismatch the reference at the given reference position?
+  def isMismatchAtReferencePosition(pos: ReferencePosition): Option[Boolean] = {
+    if (mdTag.isEmpty || !overlapsReferencePosition(pos)) {
+      None
+    } else {
+      mdTag.map(!_.isMatch(pos))
+    }
+  }
+
+  // Does this read mismatch the reference at the given offset within the read?
+  def isMismatchAtReadOffset(offset: Int): Option[Boolean] = {
+    // careful about offsets that are within an insertion!
+    if (referencePositions.isEmpty) {
+      None
+    } else {
+      readOffsetToReferencePosition(offset).flatMap(pos => isMismatchAtReferencePosition(pos))
+    }
+  }
+
+  def getReferenceContext(readOffset: Int, referencePosition: Long, cigarElem: CigarElement, elemOffset: Int): ReferenceSequenceContext = {
+    val position = if (record.getReadMapped) {
+      Some(ReferencePosition(record.getContig.getContigName, referencePosition))
+    } else {
+      None
+    }
+
+    def getReferenceBase(cigarElement: CigarElement, refPos: Long, readPos: Int): Option[Char] = {
+      mdTag.flatMap(tag => {
+        cigarElement.getOperator match {
+          case CigarOperator.M =>
+            if (!tag.isMatch(refPos)) {
+              tag.mismatchedBase(refPos)
+            } else {
+              Some(record.getSequence()(readPos))
+            }
+          case CigarOperator.D =>
+            // if a delete, get from the delete pool
+            tag.deletedBase(refPos)
+          case _ => None
+        }
+      })
+    }
+
+    val referenceBase = getReferenceBase(cigarElem, referencePosition, readOffset)
+    ReferenceSequenceContext(position, referenceBase, cigarElem, elemOffset)
+  }
+
+  lazy val referencePositions: Seq[Option[ReferencePosition]] = referenceContexts.map(ref => ref.flatMap(_.pos))
+
+  lazy val referenceContexts: Seq[Option[ReferenceSequenceContext]] = {
+    if (record.getReadMapped) {
+      val resultTuple = samtoolsCigar.getCigarElements.foldLeft((unclippedStart, List[Option[ReferenceSequenceContext]]()))((runningPos, elem) => {
+        // runningPos is a tuple, the first element holds the starting position of the next CigarOperator
+        // and the second element is the list of positions up to this point
+        val op = elem.getOperator
+        val currentRefPos = runningPos._1
+        val resultAccum = runningPos._2
+        val advanceReference = op.consumesReferenceBases || op == CigarOperator.S
+        val newRefPos = currentRefPos + (if (advanceReference) elem.getLength else 0)
+        val resultParts: Seq[Option[ReferenceSequenceContext]] =
+          if (op.consumesReadBases) {
+            val range = NumericRange(currentRefPos, currentRefPos + elem.getLength, 1L)
+            range.zipWithIndex.map(kv =>
+              if (advanceReference)
+                Some(getReferenceContext(resultAccum.size + kv._2, kv._1, elem, kv._2))
+              else None)
+          } else {
+            Seq.empty
+          }
+        (newRefPos, resultAccum ++ resultParts)
+      })
+      val results = resultTuple._2
+      results.toIndexedSeq
+    } else {
+      qualityScores.map(t => None)
+    }
+  }
+
+  def readOffsetToReferencePosition(offset: Int): Option[ReferencePosition] = {
+    if (record.getReadMapped) {
+      referencePositions(offset)
+    } else {
+      None
+    }
+  }
+
+  def readOffsetToReferenceSequenceContext(offset: Int): Option[ReferenceSequenceContext] = {
+    if (record.getReadMapped) {
+      referenceContexts(offset)
+    } else {
+      None
+    }
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichCigar.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichCigar.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichCigar.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichCigar.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,127 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import htsjdk.samtools.{ Cigar, CigarOperator, CigarElement }
+import org.bdgenomics.adam.util.ImplicitJavaConversions._
+import scala.annotation.tailrec
+
+object RichCigar {
+
+  def apply(cigar: Cigar) = {
+    new RichCigar(cigar)
+  }
+
+  implicit def cigarToRichCigar(cigar: Cigar): RichCigar = new RichCigar(cigar)
+
+}
+
+class RichCigar(cigar: Cigar) {
+
+  lazy val numElements: Int = cigar.numCigarElements
+
+  // number of alignment blocks is defined as the number of segments in the sequence that are a cigar match
+  lazy val numAlignmentBlocks: Int = {
+    cigar.getCigarElements.map(element => {
+      element.getOperator match {
+        case CigarOperator.M => 1
+        case _               => 0
+      }
+    }).reduce(_ + _)
+  }
+
+  /**
+   * Moves a single element in the cigar left by one position.
+   *
+   * @param index Index of the element to move.
+   * @return New cigar with this element moved left.
+   */
+  def moveLeft(index: Int): Cigar = {
+    // var elements = List[CigarElement]()
+    // deepclone instead of empty list initialization
+    var elements = cigar.getCigarElements.map(e => new CigarElement(e.getLength, e.getOperator))
+
+    /**
+     * Moves an element of a cigar left.
+     *
+     * @param index Element to move left.
+     * @param cigarElements List of cigar elements to move.
+     * @return List of cigar elements with single element moved.
+     */
+    @tailrec def moveCigarLeft(head: List[CigarElement],
+                               index: Int,
+                               cigarElements: List[CigarElement]): List[CigarElement] = {
+      if (index == 1) {
+        val elementToTrim = cigarElements.head
+        val elementToMove: Option[CigarElement] = Some(cigarElements(1))
+        val elementToPad: Option[CigarElement] = if (cigarElements.length > 2) {
+          Some(cigarElements(2))
+        } else {
+          None
+        }
+        val elementsAfterPad = if (cigarElements.length > 4) {
+          cigarElements.drop(3)
+        } else {
+          List[CigarElement]()
+        }
+
+        // if we are at the position to move, then we take one from it and add to the next element
+        val elementMovedLeft: Option[CigarElement] = if (elementToTrim.getLength > 1) {
+          Some(new CigarElement(elementToTrim.getLength - 1, elementToTrim.getOperator))
+        } else {
+          None
+        }
+
+        // if there are no elements afterwards to pad, add a match operator with length 1 to the end
+        // if there are elements afterwards, pad the first one
+        val elementPadded = elementToPad match {
+          case Some(o: CigarElement) => Some(new CigarElement(o.getLength + 1, o.getOperator))
+          case _                     => Some(new CigarElement(1, CigarOperator.M))
+        }
+
+        // flatmap to remove empty options
+        val changedElements: List[CigarElement] = List(elementMovedLeft, elementToMove, elementPadded).flatMap((o: Option[CigarElement]) => o)
+
+        // cat lists together
+        head ::: changedElements ::: elementsAfterPad
+      } else if (index == 0 || cigarElements.length < 2) {
+        head ::: cigarElements
+      } else {
+        moveCigarLeft(head ::: List(cigarElements.head), index - 1, cigarElements.tail)
+      }
+    }
+
+    // create cigar from new list
+    new Cigar(moveCigarLeft(List[CigarElement](), index, elements))
+  }
+
+  def getLength(): Int = {
+    cigar.getCigarElements.map(_.getLength).reduce(_ + _)
+  }
+
+  /**
+   * Checks to see if Cigar is well formed. We assume that it is well formed if the cigar lenmgth matches
+   * the read length.
+   *
+   * @param readLength Length of the read sequence.
+   */
+  def isWellFormed(readLength: Int): Boolean = {
+    readLength == getLength
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichGenotype.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichGenotype.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichGenotype.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichGenotype.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,41 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import org.bdgenomics.formats.avro.{ GenotypeType, GenotypeAllele, Genotype }
+import scala.collection.JavaConversions._
+
+object RichGenotype {
+  implicit def genotypeToRichGenotype(g: Genotype) = new RichGenotype(g)
+  implicit def richGenotypeToGenotype(g: RichGenotype) = g.genotype
+}
+
+class RichGenotype(val genotype: Genotype) {
+  def ploidy: Int = genotype.getAlleles.size
+
+  def getType: GenotypeType = {
+    assert(ploidy <= 2, "getType only meaningful for genotypes with ploidy <= 2")
+    genotype.getAlleles.toList.distinct match {
+      case List(GenotypeAllele.Ref) => GenotypeType.HOM_REF
+      case List(GenotypeAllele.Alt) => GenotypeType.HOM_ALT
+      case List(GenotypeAllele.Ref, GenotypeAllele.Alt) |
+        List(GenotypeAllele.Alt, GenotypeAllele.Ref) => GenotypeType.HET
+      case _ => GenotypeType.NO_CALL
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichVariant.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichVariant.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/rich/RichVariant.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/rich/RichVariant.scala	2016-05-06 09:27:54.009365912 +0800
@@ -0,0 +1,46 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.rich
+
+import org.bdgenomics.formats.avro.Variant
+
+object RichVariant {
+  implicit def variantToRichVariant(variant: Variant): RichVariant = new RichVariant(variant)
+  implicit def richVariantToVariant(variant: RichVariant): Variant = variant.variant
+}
+
+class RichVariant(val variant: Variant) {
+  def isSingleNucleotideVariant() = {
+    variant.getReferenceAllele.length == 1 && variant.getAlternateAllele.length == 1
+  }
+
+  def isMultipleNucleotideVariant() = {
+    !isSingleNucleotideVariant && variant.getReferenceAllele.length == variant.getAlternateAllele.length
+  }
+
+  def isInsertion() = variant.getReferenceAllele.length < variant.getAlternateAllele.length
+
+  def isDeletion() = variant.getReferenceAllele.length > variant.getAlternateAllele.length
+
+  override def hashCode = variant.hashCode
+
+  override def equals(o: Any) = o match {
+    case that: RichVariant => variant.equals(that.variant)
+    case _                 => false
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/serialization/ADAMKryoRegistrator.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,88 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.serialization
+
+import com.esotericsoftware.kryo.{ Kryo, Serializer }
+import com.esotericsoftware.kryo.io.{ Input, Output }
+import it.unimi.dsi.fastutil.io.{ FastByteArrayInputStream, FastByteArrayOutputStream }
+import org.apache.avro.io.{ BinaryDecoder, DecoderFactory, BinaryEncoder, EncoderFactory }
+import org.apache.avro.specific.{ SpecificDatumWriter, SpecificDatumReader, SpecificRecord }
+import org.apache.spark.serializer.KryoRegistrator
+import org.bdgenomics.formats.avro._
+import org.bdgenomics.adam.models._
+import org.bdgenomics.adam.rdd.read.realignment._
+import scala.reflect.ClassTag
+
+case class InputStreamWithDecoder(size: Int) {
+  val buffer = new Array[Byte](size)
+  val stream = new FastByteArrayInputStream(buffer)
+  val decoder = DecoderFactory.get().directBinaryDecoder(stream, null.asInstanceOf[BinaryDecoder])
+}
+
+// NOTE: This class is not thread-safe; however, Spark guarantees that only a single thread will access it.
+class AvroSerializer[T <: SpecificRecord: ClassTag] extends Serializer[T] {
+  val reader = new SpecificDatumReader[T](scala.reflect.classTag[T].runtimeClass.asInstanceOf[Class[T]])
+  val writer = new SpecificDatumWriter[T](scala.reflect.classTag[T].runtimeClass.asInstanceOf[Class[T]])
+  var in = InputStreamWithDecoder(1024)
+  val outstream = new FastByteArrayOutputStream()
+  val encoder = EncoderFactory.get().directBinaryEncoder(outstream, null.asInstanceOf[BinaryEncoder])
+
+  setAcceptsNull(false)
+
+  def write(kryo: Kryo, kryoOut: Output, record: T) = {
+    outstream.reset()
+    writer.write(record, encoder)
+    kryoOut.writeInt(outstream.array.length, true)
+    kryoOut.write(outstream.array)
+  }
+
+  def read(kryo: Kryo, kryoIn: Input, klazz: Class[T]): T = this.synchronized {
+    val len = kryoIn.readInt(true)
+    if (len > in.size) {
+      in = InputStreamWithDecoder(len + 1024)
+    }
+    in.stream.reset()
+    // Read Kryo bytes into input buffer
+    kryoIn.readBytes(in.buffer, 0, len)
+    // Read the Avro object from the buffer
+    reader.read(null.asInstanceOf[T], in.decoder)
+  }
+}
+
+class ADAMKryoRegistrator extends KryoRegistrator {
+  override def registerClasses(kryo: Kryo) {
+    kryo.register(classOf[AlignmentRecord], new AvroSerializer[AlignmentRecord]())
+    kryo.register(classOf[Genotype], new AvroSerializer[Genotype]())
+    kryo.register(classOf[Variant], new AvroSerializer[Variant]())
+    kryo.register(classOf[DatabaseVariantAnnotation], new AvroSerializer[DatabaseVariantAnnotation]())
+    kryo.register(classOf[NucleotideContigFragment], new AvroSerializer[NucleotideContigFragment]())
+    kryo.register(classOf[Contig], new AvroSerializer[Contig]())
+    kryo.register(classOf[StructuralVariant], new AvroSerializer[StructuralVariant]())
+    kryo.register(classOf[VariantCallingAnnotations], new AvroSerializer[VariantCallingAnnotations]())
+    kryo.register(classOf[VariantEffect], new AvroSerializer[VariantEffect]())
+    kryo.register(classOf[DatabaseVariantAnnotation], new AvroSerializer[DatabaseVariantAnnotation]())
+    kryo.register(classOf[Dbxref], new AvroSerializer[Dbxref]())
+    kryo.register(classOf[Feature], new AvroSerializer[Feature]())
+    kryo.register(classOf[ReferencePosition], new ReferencePositionSerializer)
+    kryo.register(classOf[ReferencePositionPair], new ReferencePositionPairSerializer)
+    kryo.register(classOf[SingleReadBucket], new SingleReadBucketSerializer)
+    kryo.register(classOf[IndelRealignmentTarget])
+    kryo.register(classOf[TargetSet], new TargetSetSerializer)
+    kryo.register(classOf[ZippedTargetSet], new ZippedTargetSetSerializer)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/AttributeUtils.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/AttributeUtils.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/AttributeUtils.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/AttributeUtils.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,103 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import htsjdk.samtools.SAMRecord.SAMTagAndValue
+import org.bdgenomics.adam.models.{ TagType, Attribute }
+
+/**
+ * AttributeUtils is a utility object for parsing optional fields from a BAM file, or
+ * the attributes column from an ADAM file.
+ *
+ */
+object AttributeUtils {
+
+  val attrRegex = RegExp("([^:]{2}):([AifZHB]):(.*)")
+
+  def convertSAMTagAndValue(attr: SAMTagAndValue): Attribute = {
+    attr.value match {
+      case x: java.lang.Integer   => Attribute(attr.tag, TagType.Integer, attr.value.asInstanceOf[Int])
+      case x: java.lang.Character => Attribute(attr.tag, TagType.Character, attr.value.asInstanceOf[Char])
+      case x: java.lang.Float     => Attribute(attr.tag, TagType.Float, attr.value.asInstanceOf[Float])
+      case x: java.lang.String    => Attribute(attr.tag, TagType.String, attr.value.asInstanceOf[String])
+      case Array(_*)              => Attribute(attr.tag, TagType.ByteSequence, attr.value.asInstanceOf[Array[java.lang.Byte]])
+      // It appears from the code, that 'H' is encoded as a String as well? I'm not sure
+      // how to pull that out here.
+    }
+  }
+
+  /**
+   * Parses a tab-separated string of attributes (tag:type:value) into a Seq of Attribute values.
+   * @param tagStrings The String to be parsed
+   * @return The parsed Attributes
+   */
+  def parseAttributes(tagStrings: String): Seq[Attribute] =
+    tagStrings.split("\t").filter(_.length > 0).map(parseAttribute)
+
+  /**
+   * Extract the Attribute value from the corresponding String fragment stored in the
+   * Read.attributes field.
+   *
+   * @param encoded A three-part ':'-separated string, containing [attrTuple]:[type]:[value]
+   * @return An Attribute value which has the three, extracted and correctly-typed parts of the encoded argument.
+   * @throws IllegalArgumentException if the encoded string doesn't conform to the required regular
+   *         expression ([A-Z]{2}:[AifZHB]:[^\t^]+)
+   */
+  def parseAttribute(encoded: String): Attribute = {
+    attrRegex.matches(encoded) match {
+      case Some(m) => createAttribute(m.group(1), m.group(2), m.group(3))
+      case None =>
+        throw new IllegalArgumentException(
+          "attribute string \"%s\" doesn't match format attrTuple:type:value".format(encoded))
+    }
+  }
+
+  private def createAttribute(attrTuple: (String, String, String)): Attribute = {
+    val tagName = attrTuple._1
+    val tagTypeString = attrTuple._2
+    val valueStr = attrTuple._3
+
+    // partial match, but these letters should be (per the SAM file format spec)
+    // the only legal values of the tagTypeString anyway.
+    val tagType = tagTypeString match {
+      case "A" => TagType.Character
+      case "i" => TagType.Integer
+      case "f" => TagType.Float
+      case "Z" => TagType.String
+      case "H" => TagType.ByteSequence
+      case "B" => TagType.NumericSequence
+    }
+
+    Attribute(tagName, tagType, typedStringToValue(tagType, valueStr))
+  }
+
+  private def typedStringToValue(tagType: TagType.Value, valueStr: String): Any = {
+    tagType match {
+      case TagType.Character    => valueStr(0)
+      case TagType.Integer      => Integer.valueOf(valueStr)
+      case TagType.Float        => java.lang.Float.valueOf(valueStr)
+      case TagType.String       => valueStr
+      case TagType.ByteSequence => valueStr.map(c => java.lang.Byte.valueOf("" + c))
+      case TagType.NumericSequence => valueStr.split(",").map(c => {
+        if (c.contains(".")) java.lang.Float.valueOf(c)
+        else Integer.valueOf(c)
+      })
+    }
+  }
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/BuildInformation.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/BuildInformation.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/BuildInformation.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/BuildInformation.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,53 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import scala.Some
+import scala.collection.JavaConverters._
+import java.util.Properties
+
+/**
+ * A static object that gets the resource "git.properties" and renders the
+ * build-information in different formats. The parsing is "lazy", it parses
+ * the properties only the first time a function is called.
+ */
+object BuildInformation {
+
+  private var properties: scala.Option[scala.collection.mutable.Map[String, String]] = None
+
+  /** Return a scala mutable Map (property, value).  */
+  def asMap(): scala.collection.mutable.Map[String, String] = {
+    properties match {
+      case Some(inThere) => inThere
+      case None => {
+        val javaProperties = new Properties;
+        javaProperties.load(getClass().getClassLoader().getResourceAsStream("git.properties"));
+        properties = Some(javaProperties.asScala);
+        javaProperties.asScala
+      }
+    }
+  }
+
+  /** Return a formatted human-readable string. */
+  def asString(): String = {
+    asMap().foldLeft("") {
+      case (prev_string, (key, value)) =>
+        prev_string + key + ": " + value + "\n"
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/Flattener.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/Flattener.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/Flattener.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/Flattener.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,136 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import org.apache.avro.Schema
+import org.apache.avro.Schema.Type._
+import org.apache.avro.generic.{ GenericData, IndexedRecord }
+
+import org.bdgenomics.adam.util.ImplicitJavaConversions._
+import org.codehaus.jackson.node.NullNode
+
+import scala.collection.JavaConverters._
+import scala.collection.mutable.ListBuffer
+
+object Flattener {
+
+  val SEPARATOR: String = "__";
+
+  def flattenSchema(schema: Schema): Schema = {
+    val flatSchema: Schema = Schema.createRecord(schema.getName + "_flat", schema.getDoc,
+      schema.getNamespace, schema.isError)
+    flatSchema.setFields(flatten(schema, "", new ListBuffer[Schema.Field]).asJava)
+    flatSchema
+  }
+
+  private def flatten(schema: Schema, prefix: String,
+                      accumulator: ListBuffer[Schema.Field],
+                      makeOptional: Boolean = false): ListBuffer[Schema.Field] = {
+    for (f: Schema.Field <- schema.getFields) {
+      f.schema.getType match {
+        case NULL | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BYTES | STRING |
+          FIXED | ENUM =>
+          accumulator += copy(f, prefix, makeOptional)
+        case RECORD =>
+          flatten(f.schema, prefix + f.name + SEPARATOR, accumulator, makeOptional)
+        case UNION =>
+          val nested: List[Schema] = f.schema.getTypes.filter(_.getType != Schema.Type.NULL)
+          if (nested.size == 1) {
+            val s: Schema = nested.head
+            s.getType match {
+              case NULL | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BYTES | STRING |
+                FIXED | ENUM =>
+                accumulator += copy(f, prefix, makeOptional)
+              case RECORD =>
+                val opt = makeOptional || f.defaultValue.equals(NullNode.getInstance)
+                flatten(s, prefix + f.name + SEPARATOR, accumulator, opt)
+              case UNION | ARRAY | MAP | _ => // drop field
+            }
+          }
+        case ARRAY | MAP | _ => // drop field
+      }
+    }
+    accumulator
+  }
+
+  private def copy(f: Schema.Field, prefix: String, makeOptional: Boolean): Schema.Field = {
+    val schema = if (makeOptional) optional(f.schema) else f.schema
+    val defaultValue = if (f.defaultValue == null && makeOptional) NullNode.getInstance
+    else f.defaultValue
+    val copy: Schema.Field = new Schema.Field(prefix + f.name, schema, f.doc, defaultValue)
+    import scala.collection.JavaConversions._
+    for (prop <- f.getJsonProps.entrySet) {
+      copy.addProp(prop.getKey, prop.getValue)
+    }
+    copy
+  }
+
+  private def optional(schema: Schema): Schema = {
+    if (schema.getType eq Schema.Type.NULL) {
+      return schema
+    }
+
+    if (schema.getType ne Schema.Type.UNION) {
+      return Schema.createUnion(
+        ListBuffer[Schema](Schema.create(Schema.Type.NULL), schema).asJava)
+    }
+
+    schema // TODO: what about unions that don't contain null?
+  }
+
+  def flattenRecord(flatSchema: Schema, record: IndexedRecord): IndexedRecord = {
+    val flatRecord: GenericData.Record = new GenericData.Record(flatSchema)
+    flatten(record.getSchema, record, flatRecord, 0)
+    flatRecord
+  }
+
+  private def flatten(schema: Schema, record: IndexedRecord, flatRecord: IndexedRecord,
+                      offset: Int): Int = {
+    if (record == null)
+      return offset + schema.getFields.size
+    var off: Int = offset
+    for (f: Schema.Field <- schema.getFields) {
+      f.schema.getType match {
+        case NULL | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BYTES | STRING |
+          FIXED | ENUM =>
+          flatRecord.put(off, record.get(f.pos))
+          off += 1
+        case RECORD =>
+          off = flatten(f.schema, record.get(f.pos).asInstanceOf[IndexedRecord],
+            flatRecord, off)
+        case UNION =>
+          val nested: List[Schema] = f.schema.getTypes.filter(_.getType != Schema.Type.NULL)
+          if (nested.size == 1) {
+            val s: Schema = nested.head
+            s.getType match {
+              case NULL | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BYTES | STRING |
+                FIXED | ENUM =>
+                flatRecord.put(off, record.get(f.pos))
+                off += 1
+              case RECORD =>
+                off = flatten(s, record.get(f.pos).asInstanceOf[IndexedRecord],
+                  flatRecord, off)
+              case UNION | ARRAY | MAP | _ => // drop field
+            }
+          }
+        case ARRAY | MAP | _ => // drop field
+      }
+    }
+    return off
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ImplicitJavaConversions.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ImplicitJavaConversions.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ImplicitJavaConversions.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ImplicitJavaConversions.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,38 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import scala.collection.JavaConversions._
+
+object ImplicitJavaConversions {
+
+  implicit def listToJavaList[A](list: List[A]): java.util.List[A] = seqAsJavaList(list)
+
+  implicit def javaListToList[A](list: java.util.List[A]): List[A] = {
+    asScalaBuffer(list).toList
+  }
+
+  implicit def intListToJavaIntegerList(list: List[Int]): java.util.List[java.lang.Integer] = {
+    seqAsJavaList(list.map(i => i: java.lang.Integer))
+  }
+
+  //  implicit def charSequenceToString(cs: CharSequence): String = cs.toString
+
+  //  implicit def charSequenceToList(cs: CharSequence): List[Char] = cs.toList
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/IntervalListReader.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/IntervalListReader.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/IntervalListReader.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/IntervalListReader.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,53 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import java.io.{ FileInputStream, File }
+import scala.collection._
+import org.bdgenomics.adam.models.{ SequenceDictionary, ReferenceRegion }
+import htsjdk.samtools.util.IntervalList
+import scala.collection.JavaConverters._
+import htsjdk.samtools.SAMTextHeaderCodec
+import htsjdk.samtools.util.BufferedLineReader
+
+/**
+ * Reads GATK-style interval list files
+ * e.g. example file taken from this page:
+ * http://www.broadinstitute.org/gatk/guide/article?id=1204
+ *
+ * @param file a File whose contents are a (line-based tab-separated) interval file in UTF-8 encoding
+ */
+class IntervalListReader(file: File) extends Traversable[(ReferenceRegion, String)] {
+  val encoding = "UTF-8"
+
+  /**
+   * The interval list file contains a SAM-style sequence dictionary as a header --
+   * this value holds that dictionary, reading it if necessary (if the file hasn't been
+   * opened before).
+   */
+  lazy val sequenceDictionary: SequenceDictionary = {
+    // Do we need to manually close the file stream?
+    val codec = new SAMTextHeaderCodec()
+    SequenceDictionary(codec.decode(new BufferedLineReader(new FileInputStream(file)), file.toString))
+  }
+
+  def foreach[U](f: ((ReferenceRegion, String)) => U) {
+    IntervalList.fromFile(file).asScala.foreach(
+      i => f((ReferenceRegion(i.getSequence, i.getStart, i.getEnd), i.getName)))
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/MapTools.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/MapTools.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/MapTools.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/MapTools.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,52 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+object MapTools {
+
+  /**
+   * Takes two "count maps" (i.e. Map values that contain numeric counts as the values of the keys),
+   * and does a point-wise add.  For example,
+   *
+   *   add(Map("A"->1, "B"->1), Map("B"->1, "C"->1))
+   *
+   * should be equal to
+   *
+   *   Map("A"->1, "B"->2, "C"->1)
+   *
+   * The operation should be commutative, although it contains a "filter" step on the second addend
+   * (in the current implementation), so if the two maps are of wildly different sizes, you might
+   * want to make the larger map the first argument.
+   *
+   * @param map1 Left addend
+   * @param map2 Right addend
+   * @param ops
+   * @tparam KeyType
+   * @tparam NumberType
+   * @return
+   */
+  def add[KeyType, NumberType](map1: Map[KeyType, NumberType],
+                               map2: Map[KeyType, NumberType])(implicit ops: Numeric[NumberType]): Map[KeyType, NumberType] = {
+
+    (map1.keys ++ map2.keys.filter(!map1.contains(_))).map {
+      (key: KeyType) =>
+        (key, ops.plus(map1.getOrElse(key, ops.zero), map2.getOrElse(key, ops.zero)))
+    }.toMap
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/MdTag.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/MdTag.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/MdTag.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/MdTag.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,560 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import htsjdk.samtools.{ Cigar, CigarOperator }
+import org.bdgenomics.adam.models.ReferencePosition
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import scala.collection.immutable
+import scala.collection.immutable.NumericRange
+import scala.util.matching.Regex
+
+object MdTagEvent extends Enumeration {
+  val Match, Mismatch, Delete = Value
+}
+
+object MdTag {
+
+  private val digitPattern = new Regex("\\d+")
+  // for description, see base enum in adam schema
+  private val basesPattern = new Regex("[AaGgCcTtNnUuKkMmRrSsWwBbVvHhDdXxYy]+")
+
+  /**
+   * Builds an MD tag object from the string representation of an MD tag and the
+   * start position of the read.
+   *
+   * @param mdTagInput Textual MD tag/mismatchingPositions string.
+   * @param referenceStart The read start position.
+   * @return Returns a populated MD tag.
+   */
+  def apply(mdTagInput: String, referenceStart: Long): MdTag = {
+    var matches = List[NumericRange[Long]]()
+    var mismatches = Map[Long, Char]()
+    var deletions = Map[Long, Char]()
+
+    if (mdTagInput != null && mdTagInput == "0") {
+      new MdTag(referenceStart, List(), Map(), Map())
+    } else if (mdTagInput != null && mdTagInput.length > 0) {
+      val mdTag = mdTagInput.toUpperCase
+      val end = mdTag.length
+
+      var offset = 0
+      var referencePos = referenceStart
+
+      def readMatches(errMsg: String): Unit = {
+        digitPattern.findPrefixOf(mdTag.substring(offset)) match {
+          case None => throw new IllegalArgumentException(errMsg)
+          case Some(s) =>
+            val length = s.toInt
+            if (length > 0) {
+              matches ::= NumericRange(referencePos, referencePos + length, 1L)
+            }
+            offset += s.length
+            referencePos += length
+        }
+      }
+
+      readMatches("MD tag must start with a digit")
+
+      while (offset < end) {
+        val mdTagType = {
+          if (mdTag.charAt(offset) == '^') {
+            offset += 1
+            MdTagEvent.Delete
+          } else {
+            MdTagEvent.Mismatch
+          }
+        }
+        basesPattern.findPrefixOf(mdTag.substring(offset)) match {
+          case None => throw new IllegalArgumentException("Failed to find deleted or mismatched bases after a match: %s".format(mdTagInput))
+          case Some(bases) =>
+            mdTagType match {
+              case MdTagEvent.Delete =>
+                bases.foreach {
+                  base =>
+                    deletions += (referencePos -> base)
+                    referencePos += 1
+                }
+              case MdTagEvent.Mismatch =>
+                bases.foreach {
+                  base =>
+                    mismatches += (referencePos -> base)
+                    referencePos += 1
+                }
+            }
+            offset += bases.length
+        }
+        readMatches("MD tag should have matching bases after mismatched or missing bases")
+      }
+    }
+
+    new MdTag(referenceStart, matches, mismatches, deletions)
+  }
+
+  /**
+   * From an updated read alignment, writes a new MD tag.
+   *
+   * @param read Record for current alignment.
+   * @param newCigar Realigned cigar string.
+   * @return Returns an MD tag for the new read alignment.
+   *
+   * @see moveAlignment
+   */
+  def apply(read: RichAlignmentRecord, newCigar: Cigar): MdTag = {
+    moveAlignment(read, newCigar)
+  }
+
+  /**
+   * From an updated read alignment, writes a new MD tag.
+   *
+   * @param read Read to write a new alignment for.
+   * @param newReference Reference sequence to write alignment against.
+   * @param newCigar The Cigar for the new read alignment.
+   * @param newAlignmentStart The position of the new read alignment.
+   * @return Returns an MD tag for the new read alignment.
+   *
+   * @see moveAlignment
+   */
+  def apply(read: RichAlignmentRecord, newCigar: Cigar, newReference: String, newAlignmentStart: Long): MdTag = {
+    moveAlignment(read, newCigar, newReference, newAlignmentStart)
+  }
+
+  /**
+   * Helper function for moving the alignment of a read.
+   *
+   * @param reference String corresponding to the reference sequence overlapping this read.
+   * @param sequence String corresponding to the sequence of read bases.
+   * @param newCigar Cigar for the new alignment of this read.
+   * @param readStart Start position of the new read alignment.
+   * @return MdTag corresponding to the new alignment.
+   */
+  private def moveAlignment(reference: String, sequence: String, newCigar: Cigar, readStart: Long): MdTag = {
+    var referencePos = 0
+    var readPos = 0
+
+    var matches: List[NumericRange[Long]] = List[NumericRange[Long]]()
+    var mismatches: Map[Long, Char] = Map[Long, Char]()
+    var deletions: Map[Long, Char] = Map[Long, Char]()
+
+    // loop over cigar elements and fill sets
+    newCigar.getCigarElements.foreach(cigarElement => {
+      cigarElement.getOperator match {
+        case CigarOperator.M => {
+          var rangeStart = 0L
+          var inMatch = false
+
+          // dirty dancing to recalculate match sets
+          for (i <- 0 until cigarElement.getLength) {
+            if (reference(referencePos) == sequence(readPos)) {
+              if (!inMatch) {
+                rangeStart = referencePos.toLong
+                inMatch = true
+              }
+            } else {
+              if (inMatch) {
+                // we are no longer inside of a match, so use until
+                matches = ((rangeStart + readStart) until (referencePos.toLong + readStart)) :: matches
+                inMatch = false
+              }
+
+              mismatches += ((referencePos + readStart) -> reference(referencePos))
+            }
+
+            readPos += 1
+            referencePos += 1
+          }
+
+          // we are currently in a match, so use to
+          if (inMatch) {
+            matches = ((rangeStart + readStart) until (referencePos.toLong + readStart)) :: matches
+          }
+        }
+        case CigarOperator.D => {
+          for (i <- 0 until cigarElement.getLength) {
+            deletions += ((referencePos + readStart) -> reference(referencePos))
+
+            referencePos += 1
+          }
+        }
+        case _ => {
+          if (cigarElement.getOperator.consumesReadBases) {
+            readPos += cigarElement.getLength
+          }
+          if (cigarElement.getOperator.consumesReferenceBases) {
+            throw new IllegalArgumentException("Cannot handle operator: " + cigarElement.getOperator)
+          }
+        }
+      }
+    })
+
+    new MdTag(readStart, matches, mismatches, deletions)
+  }
+
+  /**
+   * Given a single read and an updated Cigar, recalculates the MD tag.
+   *
+   * @note For this method, the read must be mapped and adjustments to the cigar must not have led to a change in the alignment start position.
+   * If the alignment position has been changed, then the moveAlignment function with a new reference must be used.
+   *
+   * @param read Record for current alignment.
+   * @param newCigar Realigned cigar string.
+   * @return Returns an MD tag for the new read alignment.
+   *
+   * @see apply
+   */
+  def moveAlignment(read: RichAlignmentRecord, newCigar: Cigar): MdTag = {
+    val reference = read.mdTag.get.getReference(read.record)
+
+    moveAlignment(reference, read.record.getSequence, newCigar, read.record.getStart)
+  }
+
+  /**
+   * Given a single read, an updated reference, and an updated Cigar, this method calculates a new MD tag.
+   *
+   * @note If the alignment start position has not changed (e.g., the alignment change is that an indel in the read was left normalized), then
+   * the two argument (RichADAMRecord, Cigar) moveAlignment function should be used.
+   *
+   * @param read Read to write a new alignment for.
+   * @param newCigar The Cigar for the new read alignment.
+   * @param newReference Reference sequence to write alignment against.
+   * @param newAlignmentStart The position of the new read alignment.
+   * @return Returns an MD tag for the new read alignment.
+   *
+   * @see apply
+   */
+  def moveAlignment(read: RichAlignmentRecord, newCigar: Cigar, newReference: String, newAlignmentStart: Long): MdTag = {
+    moveAlignment(newReference, read.record.getSequence, newCigar, newAlignmentStart)
+  }
+
+  /**
+   * Creates an MD tag object given a read, and the accompanying reference alignment.
+   *
+   * @param read Sequence of bases in the read.
+   * @param reference Reference sequence that the read is aligned to.
+   * @param cigar The CIGAR for the reference alignment.
+   * @param start The start position of the read alignment.
+   * @return Returns a populated MD tag.
+   */
+  def apply(read: String, reference: String, cigar: Cigar, start: Long): MdTag = {
+    var matchCount = 0
+    var delCount = 0
+    var string = ""
+    var readPos = 0
+    var refPos = 0
+
+    // loop over all cigar elements
+    cigar.getCigarElements.foreach(cigarElement => {
+      cigarElement.getOperator match {
+        case CigarOperator.M | CigarOperator.EQ | CigarOperator.X => {
+          for (i <- 0 until cigarElement.getLength) {
+            if (read(readPos) == reference(refPos)) {
+              matchCount += 1
+            } else {
+              string += matchCount.toString + reference(refPos)
+              matchCount = 0
+            }
+            readPos += 1
+            refPos += 1
+            delCount = 0
+          }
+        }
+        case CigarOperator.D => {
+          for (i <- 0 until cigarElement.getLength) {
+            if (delCount == 0) {
+              string += matchCount.toString + "^"
+            }
+            string += reference(refPos)
+
+            matchCount = 0
+            delCount += 1
+            refPos += 1
+          }
+        }
+        case _ => {
+          if (cigarElement.getOperator.consumesReadBases) {
+            readPos += cigarElement.getLength
+          }
+          if (cigarElement.getOperator.consumesReferenceBases) {
+            throw new IllegalArgumentException("Cannot handle operator: " + cigarElement.getOperator)
+          }
+        }
+      }
+    })
+
+    string += matchCount.toString
+
+    apply(string, start)
+  }
+}
+
+/**
+ * Represents the mismatches and deletions present in a read that has been
+ * aligned to a reference genome. The MD tag can be used to reconstruct
+ * the reference that an aligned read overlaps.
+ *
+ * @param start Start position of the alignment.
+ * @param matches A list of the ranges over which the read has a perfect
+ *                sequence match.
+ * @param mismatches A map of all the locations where a base mismatched.
+ * @param deletions A map of all locations where a base was deleted.
+ */
+class MdTag(
+    val start: Long,
+    val matches: immutable.List[NumericRange[Long]],
+    val mismatches: immutable.Map[Long, Char],
+    val deletions: immutable.Map[Long, Char]) {
+
+  /**
+   * Returns whether a base is a match against the reference.
+   *
+   * @param pos Reference based position to check.
+   * @return True if base matches reference. False means that the base may be either a mismatch or a deletion.
+   */
+  def isMatch(pos: Long): Boolean = {
+    matches.exists(_.contains(pos))
+  }
+
+  /**
+   * Returns whether a base is a match against the reference.
+   *
+   * @param pos ReferencePosition object describing where to check.
+   * @return True if base matches reference. False means that the base may be either a mismatch or a deletion.
+   */
+  def isMatch(pos: ReferencePosition): Boolean = {
+    matches.exists(_.contains(pos.pos))
+  }
+
+  /**
+   * Returns the mismatched base at a position.
+   *
+   * @param pos Reference based position.
+   * @return The base at this position in the reference.
+   */
+  def mismatchedBase(pos: Long): Option[Char] = {
+    mismatches.get(pos)
+  }
+
+  /**
+   * Returns the base that was deleted at a position.
+   *
+   * @param pos Reference based position.
+   * @return The base that was deleted at this position in the reference.
+   */
+  def deletedBase(pos: Long): Option[Char] = {
+    deletions.get(pos)
+  }
+
+  /**
+   * Returns whether this read has any mismatches against the reference.
+   *
+   * @return True if this read has mismatches. We do not return true if the read has no mismatches but has deletions.
+   */
+  def hasMismatches: Boolean = {
+    !mismatches.isEmpty
+  }
+
+  /**
+   * Returns the number of mismatches against the reference.
+   *
+   * @return Number of mismatches against the reference
+   */
+  def countOfMismatches: Int = {
+    mismatches.size
+  }
+
+  /**
+   * Returns the end position of the record described by this MD tag.
+   *
+   * @return The reference based end position of this tag.
+   */
+  def end(): Long = {
+    val ends = matches.map(_.end - 1) ::: mismatches.keys.toList ::: deletions.keys.toList
+    ends.reduce(_ max _)
+  }
+
+  /**
+   * Given a read, returns the reference.
+   *
+   * @param read A read for which one desires the reference sequence.
+   * @return A string corresponding to the reference overlapping this read.
+   */
+  def getReference(read: RichAlignmentRecord): String = {
+    getReference(read.getSequence, read.samtoolsCigar, read.getStart)
+  }
+
+  /**
+   * Given a read sequence, cigar, and a reference start position, returns the reference.
+   *
+   * @param readSequence The base sequence of the read.
+   * @param cigar The cigar for the read.
+   * @param referenceFrom The starting point of this read alignment vs. the reference.
+   * @return A string corresponding to the reference overlapping this read.
+   */
+  def getReference(readSequence: String, cigar: Cigar, referenceFrom: Long): String = {
+
+    var referencePos = start
+    var readPos = 0
+    var reference = ""
+
+    // loop over all cigar elements
+    cigar.getCigarElements.foreach(cigarElement => {
+      cigarElement.getOperator match {
+        case CigarOperator.M | CigarOperator.EQ | CigarOperator.X => {
+          // if we are a match, loop over bases in element
+          for (i <- 0 until cigarElement.getLength) {
+            // if a mismatch, get from the mismatch set, else pull from read
+            mismatches.get(referencePos) match {
+              case Some(base) => reference += base
+              case _          => reference += readSequence(readPos)
+            }
+
+            readPos += 1
+            referencePos += 1
+          }
+        }
+        case CigarOperator.D => {
+          // if a delete, get from the delete pool
+          for (i <- 0 until cigarElement.getLength) {
+            reference += {
+              deletions.get(referencePos) match {
+                case Some(base) => base
+                case _          => throw new IllegalStateException("Could not find deleted base at cigar offset " + i)
+              }
+            }
+
+            referencePos += 1
+          }
+        }
+        case _ => {
+          // ignore inserts
+          if (cigarElement.getOperator.consumesReadBases) {
+            readPos += cigarElement.getLength
+          }
+          if (cigarElement.getOperator.consumesReferenceBases) {
+            throw new IllegalArgumentException("Cannot handle operator: " + cigarElement.getOperator)
+          }
+        }
+      }
+    })
+
+    reference
+  }
+
+  /**
+   * Converts an MdTag object to a properly formatted MD string.
+   *
+   * @return MD string corresponding to [0-9]+(([A-Z]|\&#94;[A-Z]+)[0-9]+)
+   * @see http://zenfractal.com/2013/06/19/playing-with-matches/
+   */
+  override def toString(): String = {
+    if (matches.isEmpty && mismatches.isEmpty && deletions.isEmpty) {
+      "0"
+    } else {
+      var mdString = ""
+      var lastWasMatch = false
+      var lastWasDeletion = false
+      var matchRun = 0
+
+      // loop over positions in tag - FSM for building string
+      (start to end()).foreach(i => {
+        if (isMatch(i)) {
+          if (lastWasMatch) {
+            // if in run of matches, increment count
+            matchRun += 1
+          } else {
+            // if first match, reset match count and set flag
+            matchRun = 1
+            lastWasMatch = true
+          }
+
+          // clear state
+          lastWasDeletion = false
+        } else if (deletions.contains(i)) {
+          if (!lastWasDeletion) {
+            // write match count before deletion
+            if (lastWasMatch) {
+              mdString += matchRun.toString
+            } else {
+              mdString += "0"
+            }
+            // add deletion caret
+            mdString += "^"
+
+            // set state
+            lastWasMatch = false
+            lastWasDeletion = true
+          }
+
+          // add deleted base
+          mdString += deletions(i)
+        } else {
+          // write match count before mismatch
+          if (lastWasMatch) {
+            mdString += matchRun.toString
+          } else {
+            mdString += "0"
+          }
+
+          mdString += mismatches(i)
+
+          // clear state
+          lastWasMatch = false
+          lastWasDeletion = false
+        }
+      })
+
+      // if we have more matches, write count
+      if (lastWasMatch) {
+        mdString += matchRun.toString
+      } else {
+        mdString += "0"
+      }
+
+      mdString
+    }
+  }
+
+  /**
+   * We implement equality checking by seeing whether two MD tags are at the
+   * same position and have the same value.
+   *
+   * @param other An object to compare to.
+   * @return True if the object is an MD tag at the same position and with the
+   *         same string value. Else, false.
+   */
+  override def equals(other: Any): Boolean = other match {
+    case that: MdTag => toString == that.toString && start == that.start
+    case _           => false
+  }
+
+  /**
+   * We can check equality against MdTags.
+   *
+   * @param other Object to see if we can compare against.
+   * @return Returns True if the object is an MdTag.
+   */
+  def canEqual(other: Any): Boolean = other.isInstanceOf[MdTag]
+
+  /**
+   * @return We implement hashing by hashing the string representation of the
+   *         MD tag.
+   */
+  override def hashCode: Int = toString().hashCode
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/NormalizationUtils.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/NormalizationUtils.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/NormalizationUtils.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/NormalizationUtils.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,153 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import scala.annotation.tailrec
+import htsjdk.samtools.{ Cigar, CigarOperator }
+import org.bdgenomics.formats.avro.AlignmentRecord
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.bdgenomics.adam.rich.RichAlignmentRecord
+import org.bdgenomics.adam.rich.RichCigar._
+
+object NormalizationUtils {
+
+  /**
+   * Given a cigar, returns the cigar with the position of the cigar shifted left.
+   *
+   * @param read Read whose Cigar should be left align.
+   * @return Cigar fully moved left.
+   */
+  def leftAlignIndel(read: AlignmentRecord): Cigar = {
+    var indelPos = -1
+    var pos = 0
+    var indelLength = 0
+    var readPos = 0
+    var referencePos = 0
+    var isInsert = false
+    val richRead = RichAlignmentRecord(read)
+    val cigar = richRead.samtoolsCigar
+
+    // find indel in cigar
+    cigar.getCigarElements.map(elem => {
+      elem.getOperator match {
+        case (CigarOperator.I) =>
+          if (indelPos == -1) {
+            indelPos = pos
+            indelLength = elem.getLength
+          } else {
+            // if we see a second indel, return the cigar
+            return cigar
+          }
+          pos += 1
+          isInsert = true
+        case (CigarOperator.D) =>
+          if (indelPos == -1) {
+            indelPos = pos
+            indelLength = elem.getLength
+          } else {
+            // if we see a second indel, return the cigar
+            return cigar
+          }
+          pos += 1
+        case _ =>
+          pos += 1
+          if (indelPos == -1) {
+            if (elem.getOperator.consumesReadBases()) {
+              readPos += elem.getLength
+            }
+            if (elem.getOperator.consumesReferenceBases()) {
+              referencePos += elem.getLength
+            }
+          }
+      }
+    })
+
+    // if there is an indel, shift it, else return
+    if (indelPos != -1) {
+
+      val readSeq: String = read.getSequence
+
+      // if an insert, get variant and preceeding bases from read
+      // if delete, pick variant, from reference, preceeding bases from read
+      val variant = if (isInsert) {
+        readSeq.drop(readPos).take(indelLength)
+      } else {
+        val refSeq = richRead.mdTag.get.getReference(read)
+        refSeq.drop(referencePos).take(indelLength)
+      }
+
+      // preceeding sequence must always come from read
+      // if preceeding sequence does not come from read, we may left shift through a SNP
+      val preceeding = readSeq.take(readPos)
+
+      // identify the number of bases to shift by
+      val shiftLength = numberOfPositionsToShiftIndel(variant, preceeding)
+
+      shiftIndel(cigar, indelPos, shiftLength)
+    } else {
+      cigar
+    }
+  }
+
+  /**
+   * Returns the maximum number of bases that an indel can be shifted left during left normalization.
+   * Requires that the indel has been trimmed. For an insertion, this should be called on read data
+   *
+   * @param variant Bases of indel variant sequence.
+   * @param preceeding Bases of sequence to left of variant.
+   * @return The number of bases to shift an indel for it to be left normalized.
+   */
+  def numberOfPositionsToShiftIndel(variant: String, preceeding: String): Int = {
+
+    // tail recursive function to determine shift
+    @tailrec def numberOfPositionsToShiftIndelAccumulate(variant: String, preceeding: String, accumulator: Int): Int = {
+      if (preceeding.length == 0 || preceeding.last != variant.last) {
+        // the indel cannot be moved further left if we do not have bases in front of our indel, or if we cannot barrel rotate the indel
+        accumulator
+      } else {
+        // barrel rotate variant
+        val newVariant = variant.last + variant.dropRight(1)
+        // trim preceeding sequence
+        val newPreceeding = preceeding.dropRight(1)
+        numberOfPositionsToShiftIndelAccumulate(newVariant, newPreceeding, accumulator + 1)
+      }
+    }
+
+    numberOfPositionsToShiftIndelAccumulate(variant, preceeding, 0)
+  }
+
+  /**
+   * Shifts an indel left by n. Is tail call recursive.
+   *
+   * @param cigar Cigar to shift.
+   * @param position Position of element to move.
+   * @param shifts Number of bases to shift element.
+   * @return Cigar that has been shifted as far left as possible.
+   */
+  @tailrec def shiftIndel(cigar: Cigar, position: Int, shifts: Int): Cigar = {
+    // generate new cigar with indel shifted by one
+    val newCigar = new Cigar(cigar.getCigarElements).moveLeft(position)
+
+    // if there are no more shifts to do, or if shifting breaks the cigar, return old cigar
+    if (shifts == 0 || !newCigar.isWellFormed(cigar.getLength())) {
+      cigar
+    } else {
+      shiftIndel(newCigar, position, shifts - 1)
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ParquetFileTraversable.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ParquetFileTraversable.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ParquetFileTraversable.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ParquetFileTraversable.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,68 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import org.apache.hadoop.fs.{ FileSystem, Path }
+import parquet.avro.AvroParquetReader
+import org.apache.avro.generic.IndexedRecord
+import org.apache.spark.SparkContext
+import org.bdgenomics.utils.misc.HadoopUtil
+
+class ParquetFileTraversable[T <: IndexedRecord](sc: SparkContext, file: Path) extends Traversable[T] {
+  def this(sc: SparkContext, file: String) = this(sc, new Path(file))
+
+  private val fs = FileSystem.get(sc.hadoopConfiguration)
+
+  val paths: List[Path] = {
+    if (!fs.exists(file)) {
+      throw new IllegalArgumentException("The path %s does not exist".format(file))
+    }
+    val status = fs.getFileStatus(file)
+    var paths = List[Path]()
+    if (HadoopUtil.isDirectory(status)) {
+      val files = fs.listStatus(file)
+      files.foreach {
+        file =>
+          if (file.getPath.getName.contains("part")) {
+            paths ::= file.getPath
+          }
+      }
+    } else if (fs.isFile(file)) {
+      paths ::= file
+    } else {
+      throw new IllegalArgumentException("The path '%s' is neither file nor directory".format(file))
+    }
+    paths
+  }
+
+  override def foreach[U](f: (T) => U) {
+    for (path <- paths) {
+      val parquetReader = new AvroParquetReader[T](path)
+      var record = null.asInstanceOf[T]
+      do {
+        record = parquetReader.read()
+        if (record != null.asInstanceOf[T]) {
+          f(record)
+        }
+      } while (record != null.asInstanceOf[T])
+      parquetReader.close()
+    }
+  }
+
+}
+
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ParquetLogger.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ParquetLogger.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ParquetLogger.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ParquetLogger.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,29 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import java.util.logging.{ Level, Logger }
+
+object ParquetLogger {
+
+  val hadoopLoggerLevel = (level: Level) => {
+    val parquetHadoopLogger = Logger.getLogger("parquet.hadoop")
+    parquetHadoopLogger.setLevel(level)
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/PhredUtils.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/PhredUtils.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/PhredUtils.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/PhredUtils.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,46 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import scala.math.{ pow, log10 }
+
+object PhredUtils {
+
+  lazy val phredToErrorProbabilityCache: Array[Double] = {
+    (0 until 256).map { p => pow(10.0, -p / 10.0) }.toArray
+  }
+
+  lazy val phredToSuccessProbabilityCache: Array[Double] = {
+    phredToErrorProbabilityCache.map { p => 1.0 - p }
+  }
+
+  def phredToSuccessProbability(phred: Int): Double = phredToSuccessProbabilityCache(phred)
+
+  def phredToErrorProbability(phred: Int): Double = phredToErrorProbabilityCache(phred)
+
+  private def probabilityToPhred(p: Double): Int = math.round(-10.0 * log10(p)).toInt
+
+  def successProbabilityToPhred(p: Double): Int = probabilityToPhred(1.0 - p)
+
+  def errorProbabilityToPhred(p: Double): Int = probabilityToPhred(p)
+
+  def main(args: Array[String]) = {
+    phredToErrorProbabilityCache.zipWithIndex.foreach(p => println("%3d = %f".format(p._2, p._1)))
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/QualityScore.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/QualityScore.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/QualityScore.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/QualityScore.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,52 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+class QualityScore(val phred: Int) extends Ordered[QualityScore] with Serializable {
+  // Valid range of phred + 33 is described by the regex "[!-~]".
+  require(phred + 33 >= '!'.toInt && phred + 33 <= '~'.toInt, "Phred %s out of range".format(phred))
+
+  def successProbability = PhredUtils.phredToSuccessProbability(phred)
+
+  def errorProbability = PhredUtils.phredToErrorProbability(phred)
+
+  def toChar: Char = (phred + 33).toChar
+
+  override def compare(that: QualityScore) = this.phred compare that.phred
+
+  override def toString = "Q%02d".format(phred)
+
+  override def equals(other: Any): Boolean = other match {
+    case that: QualityScore => this.phred == that.phred
+    case _                  => false
+  }
+
+  override def hashCode: Int = phred
+}
+
+object QualityScore {
+  val zero = new QualityScore(0)
+
+  def apply(phred: Int) = new QualityScore(phred)
+
+  def toString(quals: Seq[QualityScore]): String =
+    String.valueOf(quals.map(_.toChar).toArray)
+
+  def fromErrorProbability(p: Double) =
+    new QualityScore(PhredUtils.errorProbabilityToPhred(p))
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ReferenceFile.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ReferenceFile.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/ReferenceFile.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/ReferenceFile.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,34 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.bdgenomics.adam.util
+
+import org.bdgenomics.adam.models.ReferenceRegion
+
+/**
+ * File that contains a reference assembly that can be broadcasted
+ */
+trait ReferenceFile extends Serializable {
+  /**
+   * Extract reference sequence from the file.
+   *
+   * @param region The desired ReferenceRegion to extract.
+   * @return The reference sequence at the desired locus.
+   */
+  def extract(region: ReferenceRegion): String
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/RegExp.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/RegExp.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/RegExp.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/RegExp.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,53 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import java.util.regex._
+
+object RegExp {
+  def apply(pattern: String): RegExp = new RegExp(pattern)
+}
+
+/**
+ * Wraps the java Pattern class, to allow for easier regular expression matching
+ * (including making the matches/finds methods return Option[Matcher], so that we can
+ * flatMap a set of strings with these methods).
+ *
+ * @param patternString The Pattern-compatiable regular expression to be compiled and used for matching.
+ */
+class RegExp(val patternString: String) {
+  val pattern = Pattern.compile(patternString)
+
+  def matches(tgt: String): Option[Matcher] = {
+    val m = pattern.matcher(tgt)
+    if (m.matches()) {
+      Some(m)
+    } else {
+      None
+    }
+  }
+
+  def find(tgt: String, idx: Int = 0): Option[Matcher] = {
+    val m = pattern.matcher(tgt)
+    if (m.find(idx)) {
+      Some(m)
+    } else {
+      None
+    }
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/TwoBitFile.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,152 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.bdgenomics.adam.util
+
+import java.nio.{ ByteOrder, ByteBuffer }
+import org.bdgenomics.utils.io.ByteAccess
+import org.bdgenomics.adam.models.ReferenceRegion
+
+object TwoBitFile {
+  val MAGIC_NUMBER: Int = 0x1A412743
+  val BASES_PER_BYTE: Int = 4
+  val BYTE_SIZE: Int = 8
+  val MASK: Byte = 3 // 00000011
+
+  // file-level byte offsets
+  val VERSION_OFFSET: Int = 4
+  val SEQUENCE_COUNT_OFFSET: Int = 8
+  val HEADER_RESERVED_OFFSET: Int = 12
+  val FILE_INDEX_OFFSET: Int = 16
+
+  // index record-related sizes (bytes)
+  val NAME_SIZE_SIZE: Int = 1
+  val OFFSET_SIZE: Int = 4
+
+  // sequence record-related sizes (bytes)
+  val INT_SIZE: Int = 4
+  val DNA_SIZE_SIZE: Int = INT_SIZE
+  val BLOCK_COUNT_SIZE: Int = 4
+  // 4-byte int for Starts array and 4-byte int for Sizes array
+  val PER_BLOCK_SIZE: Int = 8
+  val SEQ_RECORD_RESERVED_SIZE: Int = 4
+}
+
+/**
+ * Represents a set of reference sequences backed by a .2bit file.
+ *
+ * See http://genome.ucsc.edu/FAQ/FAQformat.html#format7 for the spec.
+ *
+ * @param byteAccess ByteAccess pointing to a .2bit file.
+ */
+class TwoBitFile(byteAccess: ByteAccess) extends ReferenceFile {
+  // load file into memory
+  val bytes = ByteBuffer.wrap(byteAccess.readFully(0, byteAccess.length().toInt))
+  val numSeq = readHeader()
+  // hold current byte position of start of current index record
+  var indexRecordStart = TwoBitFile.FILE_INDEX_OFFSET
+  private val seqRecordStarts = (0 until numSeq).map(i => {
+    val tup = readIndexEntry(indexRecordStart)
+    indexRecordStart += TwoBitFile.NAME_SIZE_SIZE + tup._1.length + TwoBitFile.OFFSET_SIZE
+    tup
+  }).toMap
+  val seqRecords = seqRecordStarts.map(tup => tup._1 -> TwoBitRecord(bytes, tup._1, tup._2)).toMap
+
+  private def readHeader(): Int = {
+    // figure out proper byte order
+    bytes.order(ByteOrder.LITTLE_ENDIAN)
+    if (bytes.getInt(0) != TwoBitFile.MAGIC_NUMBER) {
+      bytes.order(ByteOrder.BIG_ENDIAN)
+    }
+    if (bytes.getInt(0) != TwoBitFile.MAGIC_NUMBER) {
+      throw new IllegalStateException()
+    }
+    // process header
+    assert(bytes.getInt(TwoBitFile.VERSION_OFFSET) == 0, "Version must be zero")
+    assert(bytes.getInt(TwoBitFile.HEADER_RESERVED_OFFSET) == 0, "Reserved field must be zero")
+    assert(bytes.hasArray)
+    bytes.getInt(TwoBitFile.SEQUENCE_COUNT_OFFSET)
+  }
+
+  private def readIndexEntry(indexRecordStart: Int): (String, Int) = {
+    val nameSize = bytes.get(indexRecordStart).toInt
+    val name = new String(bytes.array, indexRecordStart + TwoBitFile.NAME_SIZE_SIZE, nameSize, "UTF-8")
+    val contigOffset = bytes.getInt(indexRecordStart + TwoBitFile.NAME_SIZE_SIZE + nameSize)
+    name -> contigOffset
+  }
+
+  /**
+   * Extract reference sequence from the .2bit data.
+   *
+   * @param region The desired ReferenceRegion to extract.
+   * @return The reference sequence at the desired locus.
+   */
+  def extract(region: ReferenceRegion): String = {
+    val record = seqRecords(region.referenceName)
+    val contigLength = record.dnaSize
+    assert(region.start >= 0)
+    assert(region.end <= contigLength.toLong)
+    val offset = record.dnaOffset
+    val sb = StringBuilder.newBuilder
+    (0 until region.width.toInt).foreach(i => {
+      // TODO: this redundantly reads the byte at a given offset
+      // pull out the byte that contains the current base
+      val byte = bytes.get(offset + (region.start.toInt + i) / TwoBitFile.BASES_PER_BYTE)
+      // which slot in the byte does our base occupy?
+      // 1: 11000000; 2: 00110000; 3: 00001100; 4: 00000011
+      val slot = (region.start + i) % TwoBitFile.BASES_PER_BYTE + 1
+      val shift = TwoBitFile.BYTE_SIZE - 2 * slot
+      // move the 2-bit base to the least significant position
+      // and zero out the more significant bits
+      val nt = (byte >> shift) & TwoBitFile.MASK match {
+        case 0 => 'T'
+        case 1 => 'C'
+        case 2 => 'A'
+        case 3 => 'G'
+      }
+      sb += nt
+    })
+    sb.toString()
+  }
+}
+
+object TwoBitRecord {
+  def apply(twoBitBytes: ByteBuffer, name: String, seqRecordStart: Int): TwoBitRecord = {
+    val dnaSize = twoBitBytes.getInt(seqRecordStart)
+    val nBlockCount = twoBitBytes.getInt(seqRecordStart + TwoBitFile.DNA_SIZE_SIZE)
+    val nBlockArraysOffset = seqRecordStart + TwoBitFile.DNA_SIZE_SIZE + TwoBitFile.BLOCK_COUNT_SIZE
+    val nBlocks = (0 until nBlockCount).map(i => {
+      // reading into an array of ints
+      val nBlockStart = twoBitBytes.getInt(nBlockArraysOffset + i * TwoBitFile.INT_SIZE)
+      val nBlockSize = twoBitBytes.getInt(nBlockArraysOffset + (nBlockCount * TwoBitFile.INT_SIZE) + i * TwoBitFile.INT_SIZE)
+      ReferenceRegion(name, nBlockStart, nBlockStart + nBlockSize)
+    })
+    val maskBlockCount = twoBitBytes.getInt(nBlockArraysOffset + (nBlockCount * TwoBitFile.PER_BLOCK_SIZE))
+    val maskBlockArraysOffset = nBlockArraysOffset + (nBlockCount * TwoBitFile.PER_BLOCK_SIZE) + TwoBitFile.BLOCK_COUNT_SIZE
+    val maskBlocks = (0 until maskBlockCount).map(i => {
+      // reading into an array of ints
+      val maskBlockStart = twoBitBytes.getInt(maskBlockArraysOffset + i * TwoBitFile.INT_SIZE)
+      val maskBlockSize = twoBitBytes.getInt(maskBlockArraysOffset + (maskBlockCount * TwoBitFile.INT_SIZE) + i * TwoBitFile.INT_SIZE)
+      ReferenceRegion(name, maskBlockStart, maskBlockStart + maskBlockSize)
+    })
+    val dnaOffset = maskBlockArraysOffset + (maskBlockCount * TwoBitFile.PER_BLOCK_SIZE) + TwoBitFile.SEQ_RECORD_RESERVED_SIZE
+    TwoBitRecord(dnaSize, nBlocks, maskBlocks, dnaOffset)
+  }
+}
+
+case class TwoBitRecord(dnaSize: Int, nBlocks: Seq[ReferenceRegion], maskBlocks: Seq[ReferenceRegion], dnaOffset: Int)
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/Util.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/Util.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/Util.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/Util.scala	2016-05-06 09:27:54.011365912 +0800
@@ -0,0 +1,30 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import org.bdgenomics.formats.avro.Contig
+
+object Util {
+  def isSameContig(left: Contig, right: Contig): Boolean = {
+    val leftName = Option(left).map(_.getContigName)
+    val leftMD5 = Option(left).map(_.getContigMD5)
+    val rightName = Option(right).map(_.getContigName)
+    val rightMD5 = Option(right).map(_.getContigMD5)
+    leftName == rightName && (leftMD5.isEmpty || rightMD5.isEmpty || leftMD5 == rightMD5)
+  }
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/VcfHeaderUtils.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/VcfHeaderUtils.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/VcfHeaderUtils.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/VcfHeaderUtils.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,130 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+import org.bdgenomics.adam.models.{ VariantContext, SequenceDictionary }
+import org.bdgenomics.adam.rdd.ADAMContext._
+import org.apache.spark.rdd.RDD
+import htsjdk.variant.vcf.{
+  VCFHeader,
+  VCFHeaderLine,
+  VCFInfoHeaderLine,
+  VCFContigHeaderLine,
+  VCFConstants,
+  VCFStandardHeaderLines,
+  VCFHeaderLineCount,
+  VCFHeaderLineType
+}
+
+/**
+ * Convenience object for building a VCF header from sequence data.
+ */
+object VcfHeaderUtils {
+
+  /**
+   * Builds a VCF header.
+   *
+   * @param seqDict Sequence dictionary describing the contigs in this callset.
+   * @param samples List of samples in this callset.
+   * @return A complete VCF header.
+   */
+  def makeHeader(seqDict: SequenceDictionary, samples: List[String]): VCFHeader = {
+    val builder = new VcfHeaderBuilder(samples)
+
+    builder.addContigLines(seqDict)
+
+    builder.build()
+  }
+
+  /**
+   * Builds a VCF header.
+   *
+   * @param rdd An RDD of ADAM variant contexts.
+   * @return A complete VCF header.
+   */
+  def makeHeader(rdd: RDD[VariantContext]): VCFHeader = {
+    val sequenceDict = rdd.adamGetSequenceDictionary()
+    val samples = rdd.getCallsetSamples()
+
+    makeHeader(sequenceDict, samples)
+  }
+
+}
+
+private[util] class VcfHeaderBuilder(samples: List[String]) {
+
+  var contigLines = List.empty[VCFContigHeaderLine]
+
+  val formatLines: java.util.Set[VCFHeaderLine] = new java.util.HashSet[VCFHeaderLine]()
+  val infoLines: java.util.Set[VCFHeaderLine] = new java.util.HashSet[VCFHeaderLine]()
+  val otherLines: Set[VCFHeaderLine] = Set(new VCFInfoHeaderLine(VCFConstants.RMS_BASE_QUALITY_KEY,
+    1,
+    VCFHeaderLineType.Float,
+    "RMS Base Quality"),
+    new VCFInfoHeaderLine(VCFConstants.SAMPLE_NUMBER_KEY,
+      VCFHeaderLineCount.INTEGER,
+      VCFHeaderLineType.Integer,
+      "RMS Mapping Quality"))
+
+  /**
+   * Creates VCF contig lines from a sequence dictionary.
+   *
+   * @param dict Sequence dictionary containing contig info.
+   */
+  def addContigLines(dict: SequenceDictionary) {
+    val contigs: List[VCFContigHeaderLine] = dict.records.zipWithIndex.map {
+      case (record, index) => new VCFContigHeaderLine(Map("ID" -> record.name), index + 1)
+    }.toList
+    contigLines = contigs ::: contigLines
+  }
+
+  /**
+   * Adds standard VCF header lines to header.
+   */
+  private def addStandardLines() {
+    val formatKeys = List(VCFConstants.GENOTYPE_KEY,
+      VCFConstants.GENOTYPE_QUALITY_KEY,
+      VCFConstants.GENOTYPE_PL_KEY)
+    val infoKeys = List(VCFConstants.ALLELE_FREQUENCY_KEY,
+      VCFConstants.ALLELE_COUNT_KEY,
+      VCFConstants.ALLELE_NUMBER_KEY,
+      VCFConstants.STRAND_BIAS_KEY,
+      VCFConstants.RMS_MAPPING_QUALITY_KEY,
+      VCFConstants.MAPPING_QUALITY_ZERO_KEY,
+      VCFConstants.DEPTH_KEY)
+
+    VCFStandardHeaderLines.addStandardFormatLines(formatLines, false, formatKeys)
+    VCFStandardHeaderLines.addStandardInfoLines(infoLines, false, infoKeys)
+  }
+
+  /**
+   * Given current information, builds header.
+   *
+   * @return Complete VCF header with sample information.
+   */
+  def build(): VCFHeader = {
+    addStandardLines()
+
+    val stdFmtLines: Set[VCFHeaderLine] = formatLines
+    val stdInfLines: Set[VCFHeaderLine] = infoLines
+    val lines: Set[VCFHeaderLine] = contigLines.toSet ++ stdFmtLines ++ stdInfLines ++ otherLines
+
+    new VCFHeader(lines, samples)
+  }
+
+}
diff -uNr Spark-GATK/src/main/scala/org/bdgenomics/adam/util/VcfStringUtils.scala GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/VcfStringUtils.scala
--- Spark-GATK/src/main/scala/org/bdgenomics/adam/util/VcfStringUtils.scala	1970-01-01 08:00:00.000000000 +0800
+++ GATK-Spark-Clean/src/main/scala/org/bdgenomics/adam/util/VcfStringUtils.scala	2016-05-06 09:27:54.010365912 +0800
@@ -0,0 +1,77 @@
+/**
+ * Licensed to Big Data Genomics (BDG) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The BDG licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.bdgenomics.adam.util
+
+object VcfStringUtils {
+
+  def clean(s: String): String = {
+    val s0 = if (s.startsWith("[")) {
+      s.drop(1)
+    } else {
+      s
+    }
+    if (s0.endsWith("]")) {
+      s0.dropRight(1)
+    } else {
+      s0
+    }
+  }
+
+  def vcfListToInts(l: String): List[Int] = {
+    val valueList = l.split(",").toList
+
+    // TODO: @tailrec 
+    def convertListToInts(l: List[String]): List[Int] = {
+      if (l.length == 0) {
+        List[Int]()
+      } else {
+        clean(l.head).toInt :: convertListToInts(l.tail)
+      }
+    }
+
+    convertListToInts(valueList)
+  }
+
+  def vcfListToDoubles(l: String): List[Double] = {
+    val valueList = l.split(",").toList
+
+    // TODO: @tailrec 
+    def convertListToDoubles(l: List[String]): List[Double] = {
+      if (l.length == 0) {
+        List[Double]()
+      } else {
+        clean(l.head).toDouble :: convertListToDoubles(l.tail)
+      }
+    }
+
+    convertListToDoubles(valueList)
+  }
+
+  def listToString(l: List[Any]): String = listToString(l.map(_.toString))
+
+  // TODO: @tailrec final 
+  private def stringListToString(l: List[String]): String = {
+    if (l.length == 0) {
+      ""
+    } else {
+      l.head + "," + listToString(l.tail)
+    }
+  }
+
+  def stringToList(s: String): List[String] = s.split(",").toList
+}
diff -uNr Spark-GATK/src/main/scala/org/ncic/gatkspark/HaplotypeCaller.scala GATK-Spark-Clean/src/main/scala/org/ncic/gatkspark/HaplotypeCaller.scala
--- Spark-GATK/src/main/scala/org/ncic/gatkspark/HaplotypeCaller.scala	2016-04-28 19:54:38.023513752 +0800
+++ GATK-Spark-Clean/src/main/scala/org/ncic/gatkspark/HaplotypeCaller.scala	2016-05-06 09:27:54.012365912 +0800
@@ -29,17 +29,13 @@
 import htsjdk.variant.variantcontext.VariantContext
 import ncic.haplotypecaller.filter.FilterUtils
 import org.apache.spark.SparkContext._
-import org.apache.hadoop.mapreduce.Job
 import org.apache.spark.{ SparkContext, Logging }
-import org.apache.spark.rdd.RDD
 import org.bdgenomics.adam.algorithms.consensus._
 import org.bdgenomics.adam.converters.AlignmentRecordConverter
 import org.bdgenomics.adam.instrumentation.Timers._
 import org.bdgenomics.adam.models.{SAMFileHeaderWritable, ReferenceRegion, SnpTable}
 import org.bdgenomics.adam.rdd.ADAMContext._
-import org.bdgenomics.adam.rdd.read.GATKSparkMarkDuplicates
-import org.bdgenomics.adam.rdd.ADAMSaveAnyArgs
-import org.bdgenomics.adam.rich.{RichNucleotideContigFragment, RichVariant}
+import org.bdgenomics.adam.rich.RichVariant
 import org.bdgenomics.formats.avro.{NucleotideContigFragment, AlignmentRecord}
 import org.bdgenomics.utils.cli._
 import org.kohsuke.args4j.{ Argument, Option => Args4jOption }
@@ -107,10 +103,6 @@
 class HaplotypeCaller(protected val args: HaplotypeCallerArgs) extends BDGSparkCommand[HaplotypeCallerArgs] with Logging {
   val companion = HaplotypeCaller
 
-  def apply(fragments: RDD[NucleotideContigFragment], records: RDD[AlignmentRecord], args: HaplotypeCallerArgs): RDD[ReferenceRegion] = {
-    fragments.toActiveRegion(records, args.fragmentLength)
-  }
-
   def timeFormat():String={
     val date = new Date();
     date.toString
